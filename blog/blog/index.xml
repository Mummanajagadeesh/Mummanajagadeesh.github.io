&lt;?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Jagadeesh Mummana | Portfolio</title><link>https://mummanajagadeesh.github.io/</link><description>This is meta description</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://mummanajagadeesh.github.io/blog/blog/" rel="self" type="application/rss+xml"/><item><title>Getting Started with Hugo: A Step-by-Step Guide</title><link>https://mummanajagadeesh.github.io/blog/getting-started-with-hugo/</link><pubDate>Sat, 01 Mar 2025 05:30:00 +0000</pubDate><guid>https://mummanajagadeesh.github.io/blog/getting-started-with-hugo/</guid><description>&lt;![CDATA[<p>Hugo is a fast, flexible, and open-source static site generator that allows you to build websites with ease. Originally popular for blogging, Hugo’s versatility makes it ideal for creating a wide range of sites — from personal portfolios and academic project showcases to documentation hubs and even e-commerce sites. Whether you’re building a professional portfolio, a research site to share your academic work, or a personal blog, Hugo has you covered.</p>]]></description><content:encoded>&lt;![CDATA[<p>Hugo is a fast, flexible, and open-source static site generator that allows you to build websites with ease. Originally popular for blogging, Hugo’s versatility makes it ideal for creating a wide range of sites — from personal portfolios and academic project showcases to documentation hubs and even e-commerce sites. Whether you’re building a professional portfolio, a research site to share your academic work, or a personal blog, Hugo has you covered.</p><p>This guide will take you through the entire process of building a website using Hugo, from installation to deployment, with practical tips to make your site look professional and unique.</p><h5 id="1-install-hugo">1.<strong>Install Hugo</strong></h5><h6 id="macos-using-homebrew"><strong>macOS (using Homebrew)</strong></h6><p>If you&rsquo;re on macOS and have Homebrew installed, this is the easiest way to install Hugo:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>brew install hugo</span></span></code></pre></div><h6 id="windows-using-chocolatey"><strong>Windows (using Chocolatey)</strong></h6><p>For Windows, use the Chocolatey package manager:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>choco install hugo -confirm</span></span></code></pre></div><h6 id="linux-debianubuntu"><strong>Linux (Debian/Ubuntu)</strong></h6><p>If you&rsquo;re on Linux, use the following commands to install Hugo:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo apt-get update</span></span><span style="display:flex;"><span>sudo apt-get install hugo</span></span></code></pre></div><p>Alternatively, you can download a precompiled binary for your platform from the<a href="https://github.com/gohugoio/hugo/releases" target="_blank">Hugo releases page</a> and extract it manually.</p><h6 id="verifying-installation"><strong>Verifying Installation</strong></h6><p>Once installed, verify the installation by running:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>hugo version</span></span></code></pre></div><p>This will show the version of Hugo you have installed, confirming that it is ready to go.</p><hr><h5 id="2-create-a-new-hugo-site">2.<strong>Create a New Hugo Site</strong></h5><p>After Hugo is installed, you can create a new site with a simple command. Open a terminal (or command prompt on Windows) and run the following:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>hugo new site &lt;your-site-name></span></span></code></pre></div><p>This creates a new directory (<code>&lt;your-site-name></code>) with the basic structure of a Hugo site. You’ll see directories like<code>content/</code>,<code>layouts/</code>, and<code>themes/</code>.</p><h5 id="3-choose-and-install-a-theme">3.<strong>Choose and Install a Theme</strong></h5><p>Hugo uses themes to determine how your website looks. To browse available themes, head to the<a href="https://themes.gohugo.io/" target="_blank">Hugo Themes website</a>. There are hundreds of free and open-source themes to choose from.</p><p>Once you&rsquo;ve chosen a theme, you can add it to your site by following these steps:</p><h6 id="using-git-submodule-recommended"><strong>Using Git Submodule (Recommended)</strong></h6><ol><li><p>Inside your Hugo site directory, initialize a Git repository (if it isn’t already initialized):</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git init</span></span></code></pre></div></li><li><p>Add the theme as a submodule:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git submodule add &lt;theme-repository-url> themes/&lt;theme-name></span></span><span style="display:flex;"><span>git submodule update --init --recursive</span></span></code></pre></div></li><li><p>Configure your site to use the new theme. Open<code>config.toml</code> (or<code>config.yaml</code> or<code>config.json</code>, depending on your configuration format) and set the theme:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-toml" data-lang="toml"><span style="display:flex;"><span><span style="color:#a6e22e">theme</span> =<span style="color:#e6db74">"&lt;theme-name>"</span></span></span></code></pre></div></li></ol><p>Alternatively, you can download the theme manually, but using Git submodules is more efficient for managing updates.</p><hr><h5 id="4-understanding-hugos-directory-structure">4.<strong>Understanding Hugo’s Directory Structure</strong></h5><p>Hugo uses a specific directory structure to organize your website’s content, assets, and configuration. Here&rsquo;s a breakdown of the most important directories and files:</p><ul><li><strong><code>content/</code></strong>: Where your content lives. This is where you will add markdown files for posts, pages, and other content types.</li><li><strong><code>themes/</code></strong>: Contains the themes you use in your site. Each theme will have a<code>layouts/</code> directory, which contains the theme’s templates.</li><li><strong><code>static/</code></strong>: This directory holds static assets like images, CSS, JavaScript files, etc. Files here are copied directly to the root of the<code>public/</code> directory when Hugo generates the site.</li><li><strong><code>layouts/</code></strong>: This folder is used for your custom templates. You can override theme templates or create your own templates for specific types of content.</li><li><strong><code>config.toml</code> (or<code>config.yaml</code>,<code>config.json</code>)</strong>: This is your site’s configuration file, where you set global parameters like the site’s title, base URL, language, theme, and more.</li></ul><hr><h5 id="5-create-and-organize-content">5.<strong>Create and Organize Content</strong></h5><p>Now it’s time to add content to your website. You can create content types like blog posts, pages, or custom content.</p><h6 id="creating-a-new-page-or-post"><strong>Creating a New Page or Post</strong></h6><p>To create a new page or post, run the following command:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>hugo new &lt;content-type>/&lt;page-name>.md</span></span></code></pre></div><p>For example, to create a blog post:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>hugo new posts/my-first-post.md</span></span></code></pre></div><p>This will create a markdown file in<code>content/posts/my-first-post.md</code>.</p><h6 id="markdown-syntax-for-content"><strong>Markdown Syntax for Content</strong></h6><p>In the generated<code>.md</code> file, you&rsquo;ll see frontmatter and markdown content:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-markdown" data-lang="markdown"><span style="display:flex;"><span>---</span></span><span style="display:flex;"><span>title: "My First Post"</span></span><span style="display:flex;"><span>date: 2025-04-02</span></span><span style="display:flex;"><span>draft: true</span></span><span style="display:flex;"><span>---</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span># Welcome to my blog!</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span>This is a simple markdown file to demonstrate Hugo.</span></span></code></pre></div><ul><li>The<strong>frontmatter</strong> (between the<code>---</code> lines) contains metadata for your content. You can set fields like<code>title</code>,<code>date</code>,<code>draft</code>, and custom parameters like<code>tags</code> or<code>author</code>.</li><li>The<strong>content</strong> section uses standard markdown syntax. You can write paragraphs, lists, headings, links, images, and much more.</li></ul><h6 id="publishing-content"><strong>Publishing Content</strong></h6><p>Once you’re ready to publish, you can set<code>draft: false</code> in the frontmatter and run<code>hugo server</code> to preview the site.</p><hr><h5 id="6-start-the-development-server">6.<strong>Start the Development Server</strong></h5><p>To see your site in action, you can run a local development server:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>hugo server</span></span></code></pre></div><p>By default, this will run a server on<a href="http://localhost:1313" target="_blank">http://localhost:1313</a>. As you modify content or templates, Hugo will automatically regenerate the site and refresh the page.</p><hr><h5 id="7-building-the-site-for-production">7.<strong>Building the Site for Production</strong></h5><p>When you&rsquo;re satisfied with your site, you can build it for production. Run the following command:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>hugo</span></span></code></pre></div><p>This will generate the static website in the<code>public/</code> directory. The<code>public/</code> directory will contain all the HTML files, assets, and other content required to host your site.</p><hr><h5 id="8-deploying-the-site">8.<strong>Deploying the Site</strong></h5><p>Once your site is built, you can deploy it to a variety of hosting platforms. Hugo sites are static, so they can be deployed on platforms like:</p><ul><li><strong>GitHub Pages</strong>: You can push the contents of the<code>public/</code> directory to a GitHub repository and serve it using GitHub Pages.</li><li><strong>Netlify</strong>: A popular static site hosting platform. Just link your GitHub repository to Netlify, and it will automatically build and deploy your site.</li><li><strong>Vercel</strong>: Another static site hosting platform similar to Netlify.</li><li><strong>Your Own Server</strong>: If you have a hosting provider or VPS, you can upload the files in the<code>public/</code> directory to your web server.</li></ul><p>Each hosting platform will have specific instructions for deploying Hugo sites, but most of them integrate easily with Git-based workflows.</p><hr><h5 id="9-customizing-your-site">9.<strong>Customizing Your Site</strong></h5><h6 id="creating-custom-layouts"><strong>Creating Custom Layouts</strong></h6><p>To modify the look and feel of your site, you can create custom templates in the<code>layouts/</code> directory. You can override default templates from the theme by placing your custom templates here.</p><p>For example, to modify the homepage layout, you can create<code>layouts/index.html</code> or<code>layouts/_default/baseof.html</code> to adjust the base layout structure.</p><h6 id="adding-shortcodes"><strong>Adding Shortcodes</strong></h6><p>Hugo supports shortcodes, which are snippets of reusable content. You can use shortcodes to easily insert dynamic elements like galleries, videos, or calls to action. Here’s an example of using a shortcode to embed a YouTube video:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-markdown" data-lang="markdown"><span style="display:flex;"><span><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameborder="0" allowfullscreen=/></div></div></span></span></code></pre></div><p>Shortcodes can be defined in the<code>layouts/shortcodes/</code> directory.</p><hr><h5 id="10-additional-tips">10.<strong>Additional Tips</strong></h5><ul><li><p><strong>Taxonomies</strong>: You can organize content using taxonomies like categories or tags. Add this configuration in<code>config.toml</code>:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-toml" data-lang="toml"><span style="display:flex;"><span>[<span style="color:#a6e22e">taxonomies</span>]</span></span><span style="display:flex;"><span><span style="color:#a6e22e">category</span> =<span style="color:#e6db74">"categories"</span></span></span><span style="display:flex;"><span><span style="color:#a6e22e">tag</span> =<span style="color:#e6db74">"tags"</span></span></span></code></pre></div></li><li><p><strong>Multilingual Sites</strong>: Hugo supports multilingual sites. You can define different content for different languages in the<code>content/</code> folder, such as<code>content/en/</code> and<code>content/es/</code>.</p></li><li><p><strong>Hugo Modules</strong>: Hugo also supports modules, which allow you to manage external dependencies in your site, such as themes or libraries.</p></li><li><p><strong>GitHub Actions</strong>: You can automate your Hugo build and deployment process using GitHub Actions for continuous deployment.</p></li></ul><hr><h5 id="conclusion">Conclusion</h5><p>Hugo is a powerful tool for building fast, static websites. With this guide, you should now be able to create, customize, and deploy your own Hugo-powered site. Whether you&rsquo;re building a blog, portfolio, or documentation site, Hugo&rsquo;s flexibility and speed make it a fantastic choice for modern static websites.</p><p>Happy building!</p><blockquote><p>Quick Note:
The site you’re reading this from is also built using Hugo — but with a ton of tweaks to make it uniquely mine! I started with the Geeky-Hugo theme as the base and added a bunch of customizations, including:</p></blockquote><ul><li><p>Custom Layouts: Modified to fit my style and content structure.</p></li><li><p>Shortcodes: Added some handy ones for embedding interactive elements.</p></li><li><p>Custom CSS: To give it a personal touch and make it look just right.</p></li><li><p>LaTeX Support: For displaying complex mathematical equations seamlessly.</p></li><li><p>Extended Pages: Not just limited to blogs — I’ve got project showcases, technical documentation, and more.</p></li></ul><p>It’s proof that Hugo isn’t just about simple blogs — with some effort, you can turn it into a full-fledged portfolio or academic site!</p>
]]></content:encoded></item><item><title>Setting Up Icarus Verilog on Google Colab</title><link>https://mummanajagadeesh.github.io/blog/setting-up-icarus-verilog-on-google-colab/</link><pubDate>Mon, 24 Feb 2025 05:30:00 +0000</pubDate><guid>https://mummanajagadeesh.github.io/blog/setting-up-icarus-verilog-on-google-colab/</guid><description>&lt;![CDATA[<p>Google Colab is a cloud-based platform that allows you to run code in a Jupyter Notebook environment. While it&rsquo;s primarily designed for Python, it can also be adapted to run Verilog simulations using Icarus Verilog. This guide walks you through setting up Icarus Verilog on Colab, writing and compiling Verilog code, running simulations, and generating waveform files for debugging—all in the cloud.</p>]]></description><content:encoded>&lt;![CDATA[<p>Google Colab is a cloud-based platform that allows you to run code in a Jupyter Notebook environment. While it&rsquo;s primarily designed for Python, it can also be adapted to run Verilog simulations using Icarus Verilog. This guide walks you through setting up Icarus Verilog on Colab, writing and compiling Verilog code, running simulations, and generating waveform files for debugging—all in the cloud.</p><p>You might wonder why we’d even consider simulating Verilog code on Colab when there are many industry-grade tools available that offer synthesis, timing analysis, and complete hardware design capabilities. The answer lies in accessibility. Icarus Verilog is an open-source, lightweight alternative that remains incredibly relevant—especially for students, educators, and hobbyists. It’s perfect for academic projects, quick prototyping, and learning digital design fundamentals without the overhead of licensed tools or heavy installations.</p><p>One major advantage of using Colab is its seamless integration with the web—allowing you to import datasets or files directly from URLs or cloud storage. This becomes particularly useful for projects where Verilog testbenches need structured data, such as input vectors, weights, or test cases. With Python handling the preprocessing and formatting, you can easily generate files that your Verilog code can read, enabling a smooth and flexible software-hardware co-design workflow.</p><p>My own journey into this setup began with a somewhat unconventional idea: training a neural network in Verilog. It was a fun and technically challenging experiment that led me to build this workflow on Colab. If you&rsquo;re curious to see how that turned out, feel free to check out my project<a href="https://mummanajagadeesh.github.io/projects/improve/never/" target="_blank">here</a>.</p><h4 id="why-use-icarus-verilog-on-google-colab">Why Use Icarus Verilog on Google Colab?</h4><p>Icarus Verilog is an open-source Verilog simulation and synthesis tool that supports a wide range of Verilog constructs. Running it on Google Colab offers several advantages:</p><ul><li>No need to install software on your local machine.</li><li>Easy collaboration and sharing through Colab notebooks.</li><li>Accessible from any device with an internet connection.</li><li>Free computational resources provided by Google.</li></ul><p>Now, let&rsquo;s get started with setting up Icarus Verilog on Google Colab.</p><h4 id="installing-icarus-verilog">Installing Icarus Verilog</h4><p>Before you can run Verilog simulations, you need to install Icarus Verilog on your Colab environment. To do this, execute the following commands:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">!</span>sudo apt<span style="color:#f92672">-</span>get update</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">!</span>sudo apt<span style="color:#f92672">-</span>get install<span style="color:#f92672">-</span>y iverilog</span></span></code></pre></div><p>Once installed, verify the installation by checking the version:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">!</span>iverilog<span style="color:#f92672">-</span>v</span></span></code></pre></div><p>If the installation is successful, you will see the version information displayed in the output.</p><h4 id="writing-and-running-a-simple-verilog-program">Writing and Running a Simple Verilog Program</h4><p>To test if Icarus Verilog is working correctly, let&rsquo;s write a simple Verilog program that prints a message. In Colab, you can use the<code>%%writefile</code> magic command to create and save Verilog files:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">%%</span>writefile test<span style="color:#f92672">.</span>v</span></span><span style="display:flex;"><span>module hello;</span></span><span style="display:flex;"><span> initial begin</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">$</span>display(<span style="color:#e6db74">"Hello, Icarus Verilog on Colab!"</span>);</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">$</span>finish;</span></span><span style="display:flex;"><span> end</span></span><span style="display:flex;"><span>endmodule</span></span></code></pre></div><h5 id="compiling-and-running-the-verilog-code">Compiling and Running the Verilog Code</h5><p>Once the Verilog file is created, compile it using<code>iverilog</code>:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">!</span>iverilog<span style="color:#f92672">-</span>o test<span style="color:#f92672">.</span>out test<span style="color:#f92672">.</span>v</span></span></code></pre></div><p>Now, run the compiled Verilog file using<code>vvp</code>:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">!</span>vvp test<span style="color:#f92672">.</span>out</span></span></code></pre></div><p>If everything is working correctly, you should see the message &ldquo;Hello, Icarus Verilog on Colab!&rdquo; printed in the output.</p><h4 id="generating-and-viewing-waveform-files">Generating and Viewing Waveform Files</h4><p>In addition to printing messages, you can also generate waveform files to analyze signal behavior. This is particularly useful for debugging digital designs.</p><p>To generate a VCD (Value Change Dump) file, modify your Verilog code to include the necessary<code>$dumpfile</code> and<code>$dumpvars</code> commands:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">%%</span>writefile wave<span style="color:#f92672">.</span>v</span></span><span style="display:flex;"><span>module wave;</span></span><span style="display:flex;"><span> initial begin</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">$</span>dumpfile(<span style="color:#e6db74">"wave.vcd"</span>);</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">$</span>dumpvars(<span style="color:#ae81ff">0</span>, wave);</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">$</span>display(<span style="color:#e6db74">"Generating wave.vcd..."</span>);</span></span><span style="display:flex;"><span><span style="color:#75715e">#10;</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">$</span>finish;</span></span><span style="display:flex;"><span> end</span></span><span style="display:flex;"><span>endmodule</span></span></code></pre></div><h5 id="compiling-and-simulating">Compiling and Simulating</h5><p>Compile and simulate the modified Verilog file using the following commands:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">!</span>iverilog<span style="color:#f92672">-</span>o wave<span style="color:#f92672">.</span>out wave<span style="color:#f92672">.</span>v</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">!</span>vvp wave<span style="color:#f92672">.</span>out</span></span></code></pre></div><p>You should see a message confirming that<code>wave.vcd</code> has been generated.</p><h5 id="downloading-and-viewing-the-waveform-file">Downloading and Viewing the Waveform File</h5><p>Once the<code>wave.vcd</code> file is created, you can download it to your local machine for analysis using GTKWave, a popular waveform viewer. Use the following command to download the file:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> google.colab<span style="color:#f92672">import</span> files</span></span><span style="display:flex;"><span>files<span style="color:#f92672">.</span>download(<span style="color:#e6db74">"wave.vcd"</span>)</span></span></code></pre></div><p>After downloading, open the file in GTKWave and inspect the signal transitions.</p><h4 id="download-the-notebook">Download the Notebook</h4><p>You can download the Jupyter Notebook containing all the above steps from the following link:</p><p><a href="https://gist.github.com/Mummanajagadeesh/f62286b296e2e98182e8291ee2f5b6aa#file-icarus-verilog-setup-google-colab-ipynb" target="_blank">Download the Jupyter Notebook</a></p><h4 id="conclusion">Conclusion</h4><p>Using Google Colab for Icarus Verilog simulations provides a simple and convenient way to write, compile, and debug Verilog code without requiring any local installations. Whether you&rsquo;re a beginner learning Verilog or an experienced engineer testing small designs, this setup allows you to quickly prototype and verify your digital circuits.</p><p>By following the steps outlined in this guide, you can:</p><ul><li>Install Icarus Verilog in Google Colab.</li><li>Write and execute Verilog programs.</li><li>Generate and analyze waveform files.</li></ul><blockquote><p>What&rsquo;s more—Colab isn&rsquo;t just limited to Verilog. With a bit of setup, you can manage a range of open-source tools like Yosys, Graywolf, and many components from the OpenROAD flow, with GUIs disabled. This opens the door to enabling a full end-to-end digital design workflow—all within the cloud.</p></blockquote><p>This setup is especially useful for students, researchers, and hobbyists looking for a hassle-free environment to explore digital design, without worrying about system configurations or heavy installations.</p><p>Happy coding!</p>
]]></content:encoded></item><item><title>The Mathematics Behind the Rubik&amp;#39;s Cube #PID1.3</title><link>https://mummanajagadeesh.github.io/blog/mathematics-behind-rubiks-cube/</link><pubDate>Fri, 14 Feb 2025 05:30:00 +0000</pubDate><guid>https://mummanajagadeesh.github.io/blog/mathematics-behind-rubiks-cube/</guid><description>&lt;![CDATA[<p>The Rubik’s Cube is not just a puzzle; it’s a deep mathematical object grounded in group theory, combinatorics, and geometry. Understanding the math behind it allows us to grasp why it has 43 quintillion possible states, how we categorize moves, and why some solutions are more efficient than others.</p>]]></description><content:encoded>&lt;![CDATA[<p>The Rubik’s Cube is not just a puzzle; it’s a deep mathematical object grounded in group theory, combinatorics, and geometry. Understanding the math behind it allows us to grasp why it has 43 quintillion possible states, how we categorize moves, and why some solutions are more efficient than others.</p><h4 id="group-theory-and-the-rubiks-cube"><strong>Group Theory and the Rubik’s Cube</strong></h4><p>Group theory is a branch of mathematics that studies sets with operations that follow specific rules. The Rubik’s Cube can be seen as a mathematical group where:</p><ul><li>Each state of the cube is an element of the group.</li><li>Each valid move (rotating a face) is a group operation.</li><li>The identity element is the solved state of the cube.</li><li>Moves have inverses (e.g., turning the right face clockwise can be undone by turning it counterclockwise).</li></ul><p>The Rubik’s Cube belongs to a finite group since it has a limited number of positions. The set of all possible cube configurations, with the operation of applying a sequence of moves, forms a non-abelian group (meaning that order matters—doing move A then B is not the same as doing move B then A).</p><p>Here’s the updated version incorporating center orientation in the usual order for 3×3 scales:</p><h4 id="order-of-an-element-in-the-rubiks-cube-group"><strong>Order of an Element in the Rubik’s Cube Group</strong></h4><p>In group theory, the order of an element is the number of times it must be applied to return to the identity (solved state). In the Rubik’s Cube, certain moves or sequences have different orders:</p><ul><li>A single quarter-turn of a face has order 4 (doing it four times returns the cube to the original state).</li><li>A 180-degree turn has order 2.</li><li>Certain complex sequences have higher orders, meaning they take more repetitions to cycle back to the starting position.</li><li>On a standard 3×3 Rubik’s Cube, center pieces do not change position, but their orientation can matter in some cases, such as in supercube variants where sticker orientation is tracked. In such cases, center rotations may introduce elements of order 2 or 4, depending on the move sequence.</li></ul><p>Understanding the order of moves, including center orientation, helps in designing efficient solving algorithms.</p><h4 id="counting-the-43-quintillion-permutations"><strong>Counting the 43 Quintillion Permutations</strong></h4><p>To compute the number of possible Rubik’s Cube states, we analyze the degrees of freedom:</p><ul><li>There are 8 corner pieces, each of which can be arranged in (8!) ways.</li><li>Each corner has three orientations, giving (3^7) possibilities (the last one is determined by the others).</li><li>There are 12 edge pieces, which can be arranged in (12!) ways.</li><li>Each edge has two orientations, giving (2^{11}) possibilities (the last one is determined by the others).</li><li>However, only even permutations of corners and edges are possible, so we divide by 2.</li></ul><p>Thus, the total number of possible Rubik’s Cube states is:</p><p>$$
\frac{8! \times 3^7 \times 12! \times 2^{11}}{2} = 43,252,003,274,489,856,000
$$</p><p>which is approximately<strong>43 quintillion</strong>.</p><p>For the<strong>2×2×2 Rubik’s Cube</strong>, we use a similar method but without considering edges:</p><ul><li>The 8 corner pieces can be arranged in (8!) ways.</li><li>Each has 3 orientations, giving (3^7) (since the last one is determined).</li><li>Only even permutations are possible, so we divide by 2.</li></ul><p>Thus, the number of possible 2×2×2 states is:</p><p>$$
\frac{8! \times 3^7}{2} = 3,674,160
$$</p><p>which is significantly smaller than the 3×3×3 but still quite large.</p><h4 id="gods-number-and-move-metrics"><strong>God’s Number and Move Metrics</strong></h4><p>God’s Number is the maximum number of moves required to solve the worst-case scenario of a Rubik’s Cube optimally. In 2010, researchers proved that God’s Number for a standard 3×3×3 Rubik’s Cube is<strong>20 moves</strong> in the<strong>quarter-turn metric</strong> (where each 90-degree face turn counts as one move).</p><h5 id="move-metrics">Move Metrics</h5><ul><li><strong>Quarter-Turn Metric (QTM)</strong>: Every 90-degree turn is counted as one move. This is the standard used in the 20-move God’s Number proof.</li><li><strong>Half-Turn Metric (HTM)</strong>: Both 90-degree and 180-degree turns count as one move. In this metric, God’s Number is<strong>18 moves</strong>.</li><li><strong>Face-Turn Metric (FTM)</strong>: Any rotation of a face, whether 90, 180, or 270 degrees, is counted as one move.</li></ul><p>Different solving methods optimize for different metrics. For example, speedcubers prioritize<strong>fewer moves in practice</strong> rather than the theoretical minimum number of moves.</p><h4 id="euclidean-and-quaternion-mathematics-in-the-rubiks-cube"><strong>Euclidean and Quaternion Mathematics in the Rubik’s Cube</strong></h4><h5 id="euclidean-geometry-and-the-rubiks-cube"><strong>Euclidean Geometry and the Rubik’s Cube</strong></h5><p>The Rubik’s Cube exists in three-dimensional Euclidean space, meaning its transformations can be represented using classical geometric tools such as matrices and vector operations.</p><ul><li><p><strong>Rotation Matrices:</strong> Each face rotation can be described using a 3×3 rotation matrix. A 90-degree clockwise rotation about the x, y, or z-axis can be represented as:</p><p>$$
R_x(90^\circ) = \begin{bmatrix} 1 &amp; 0 &amp; 0 \ 0 &amp; 0 &amp; -1 \ 0 &amp; 1 &amp; 0 \end{bmatrix},
$$</p><p>$$
R_y(90^\circ) = \begin{bmatrix} 0 &amp; 0 &amp; 1 \ 0 &amp; 1 &amp; 0 \ -1 &amp; 0 &amp; 0 \end{bmatrix},
$$</p><p>$$
R_z(90^\circ) = \begin{bmatrix} 0 &amp; -1 &amp; 0 \ 1 &amp; 0 &amp; 0 \ 0 &amp; 0 &amp; 1 \end{bmatrix}.
$$</p></li><li><p><strong>Vector Representation:</strong> Each cubie (small cube piece) has a position vector ( v ), and applying a rotation matrix transforms it to a new position:
$$
v&rsquo; = R v.
$$
Using these transformations, all possible moves on the cube can be described mathematically.</p></li></ul><h5 id="quaternion-representation-and-the-rubiks-cube"><strong>Quaternion Representation and the Rubik’s Cube</strong></h5><p>Quaternions offer an alternative way to describe rotations in 3D space. A quaternion is defined as:
$$
q = a + bi + cj + dk,
$$
where ( a, b, c, d ) are real numbers, and ( i, j, k ) are imaginary unit vectors satisfying specific multiplication rules.</p><ul><li><p><strong>Rotation Using Quaternions:</strong> Any 3D rotation can be represented as:
$$
q = \cos\left(\frac{\theta}{2}\right) + \sin\left(\frac{\theta}{2}\right)(xi + yj + zk),
$$
where ( \theta ) is the rotation angle, and ( (x, y, z) ) is the rotation axis.</p></li><li><p><strong>Applying a Rotation:</strong> Given a point represented by a quaternion ( p ), the rotated point ( p&rsquo; ) is obtained as:
$$
p&rsquo; = q p q^{-1}.
$$</p></li></ul><p>Using quaternions avoids issues like gimbal lock and allows smooth, efficient calculations, making them useful in robotic cube solvers and computer simulations.</p><h5 id="comparison-of-euclidean-and-quaternion-methods"><strong>Comparison of Euclidean and Quaternion Methods</strong></h5><table><thead><tr><th>Method</th><th>Advantages</th><th>Use Case in Rubik’s Cube</th></tr></thead><tbody><tr><td><strong>Rotation Matrices</strong></td><td>Simple, easy to compute</td><td>Manual cube manipulation, algebraic solving</td></tr><tr><td><strong>Quaternions</strong></td><td>No gimbal lock, computationally efficient</td><td>Robotics, computer simulations</td></tr></tbody></table><p>While human solvers primarily use group-theoretic approaches, understanding Euclidean and quaternion mathematics is valuable for computational methods and AI-driven solutions.</p><h4 id="advanced-mathematics-behind-the-rubiks-cube">Advanced Mathematics Behind the Rubik’s Cube</h4><h4 id="graph-theory-and-the-rubiks-cube"><strong>Graph Theory and the Rubik’s Cube</strong></h4><p>The entire state space of the Rubik’s Cube can be visualized as a<strong>graph</strong>, where:</p><ul><li>Each<strong>node</strong> represents a unique cube configuration.</li><li>Each<strong>edge</strong> represents a valid move between two configurations.</li></ul><p>This allows us to analyze cube solving as a<strong>shortest path problem</strong> (like in Dijkstra’s algorithm). The challenge is that this graph is<strong>huge</strong>, containing about<strong>43 quintillion nodes</strong>! Researchers have used<strong>Breadth-First Search (BFS)</strong> to explore how quickly the cube can be solved from any state, leading to the proof of<strong>God’s Number</strong> (20 in QTM).</p><h4 id="markov-chains-and-random-scrambles"><strong>Markov Chains and Random Scrambles</strong></h4><p>If you randomly twist a Rubik’s Cube, how many moves does it take before it is &ldquo;fully scrambled&rdquo;? This is a classic<strong>Markov Chain</strong> problem, where each move represents a<strong>random transition</strong> between states. Studies suggest that after about<strong>19-20 random moves</strong>, the cube is statistically close to a uniformly random state. This insight is used in competitive cubing to ensure fairness in official scramble generation.</p><h4 id="group-structure-conjugacy-classes-and-commutators"><strong>Group Structure: Conjugacy Classes and Commutators</strong></h4><p>The Rubik’s Cube group has special elements called<strong>commutators</strong> and<strong>conjugates</strong>, which are fundamental to advanced solving techniques:</p><ul><li><strong>Commutator</strong>: ([A, B] = A B A^{-1} B^{-1}) – used in many algorithms to isolate cube pieces.</li><li><strong>Conjugate</strong>: (X A X^{-1}) – applies a transformation in a different context.</li></ul><p>These concepts allow cube solvers to move a small set of pieces without disrupting the rest, forming the basis for algorithms like<strong>CFOP, Roux, and ZZ methods</strong>.</p><h4 id="why-is-solving-the-cube-hard-computational-complexity"><strong>Why Is Solving the Cube Hard? Computational Complexity</strong></h4><p>Solving an<strong>arbitrary</strong> cube position optimally (in the least moves) is an<strong>NP-hard problem</strong>. That means there is no known efficient algorithm that can solve every case optimally in polynomial time. This is why human solvers use heuristic-based approaches like<strong>CFOP, Petrus, and Roux</strong>, rather than brute force computation.</p><h4 id="whats-next-computers-and-efficient-cube-solving"><strong>What’s Next? Computers and Efficient Cube Solving</strong></h4><p>In our next post, we will explore how<strong>computers</strong> approach solving the Rubik’s Cube, including AI techniques, heuristics, and optimal solvers like<strong>Kociemba’s Algorithm and DeepCubeA</strong>.</p>
]]></content:encoded></item><item><title>Solving The Rubiks Cube #PID1.2</title><link>https://mummanajagadeesh.github.io/blog/solving-the-rubiks-cube/</link><pubDate>Fri, 07 Feb 2025 05:30:00 +0000</pubDate><guid>https://mummanajagadeesh.github.io/blog/solving-the-rubiks-cube/</guid><description>&lt;![CDATA[<p>The Rubik’s Cube, with its intricate design and colorful chaos, can seem overwhelming at first glance. However, solving it is not just about memorizing algorithms but about understanding the mechanics behind each move. There are several well-known solving methods, each with its own advantages and techniques that cater to different solving styles. Whether you&rsquo;re aiming for speed, efficiency, or ergonomic moves, the CFOP, Roux, and ZZ methods offer distinct paths to mastery. These are all speed-solving methods designed for competitive cubing, but there are also beginner-friendly and alternative approaches like the Layer-by-Layer (LBL) and Petrus methods.</p>]]></description><content:encoded>&lt;![CDATA[<p>The Rubik’s Cube, with its intricate design and colorful chaos, can seem overwhelming at first glance. However, solving it is not just about memorizing algorithms but about understanding the mechanics behind each move. There are several well-known solving methods, each with its own advantages and techniques that cater to different solving styles. Whether you&rsquo;re aiming for speed, efficiency, or ergonomic moves, the CFOP, Roux, and ZZ methods offer distinct paths to mastery. These are all speed-solving methods designed for competitive cubing, but there are also beginner-friendly and alternative approaches like the Layer-by-Layer (LBL) and Petrus methods.</p><hr><h4 id="cfop-the-classic-speedcubing-approach">CFOP: The Classic Speedcubing Approach</h4><p>CFOP (Cross – First Two Layers – Orientation of Last Layer – Permutation of Last Layer), also known as the Fridrich Method, is the most widely used method among speedcubers. It breaks the solve into four logical steps, enabling high efficiency and minimal pause between sequences.</p><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="360" src="/images/post/solverubcub/cfop_hu_5b6f1c8e2e8b031.webp" alt="pieces-of-rubiks-cube" onerror="this.onerror='null';this.src='\/images\/post\/solverubcub\/cfop_hu_361e70e1bbc97c6.jpg'"/><script>
window.addEventListener("load", (e) => {
const lightbox = GLightbox();
});</script><h5 id="the-cross">The Cross</h5><p>The first step is solving a cross on one face of the cube, typically white. The aim is to position the four edge pieces correctly while minimizing moves. Advanced solvers focus on efficiency, ensuring that each edge is inserted optimally without unnecessary cube rotations. Many speedcubers practice solving the cross in eight moves or fewer to optimize their solving time.</p><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="563" src="/images/post/solverubcub/cross_hu_af7ae8189f2e14c8.webp" alt="pieces-of-rubiks-cube" onerror="this.onerror='null';this.src='\/images\/post\/solverubcub\/cross_hu_815e466b8271842a.webp'"/><h5 id="first-two-layers-f2l">First Two Layers (F2L)</h5><p>Rather than solving corners and edges separately, F2L pairs them up before inserting them into their respective slots. This is a crucial speed improvement over beginner methods, reducing move count significantly. F2L can be learned intuitively, but advanced cubers memorize key cases and algorithms for increased efficiency.</p><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="563" src="/images/post/solverubcub/f2l_hu_539189eb6ed66a8a.webp" alt="pieces-of-rubiks-cube" onerror="this.onerror='null';this.src='\/images\/post\/solverubcub\/f2l_hu_c0e0072970fcb8dc.webp'"/><h5 id="orientation-of-the-last-layer-oll">Orientation of the Last Layer (OLL)</h5><p>Once the first two layers are completed, the next step is orienting all pieces on the top layer so that the face becomes a uniform color. There are 57 possible cases, but beginners can use a two-step OLL approach with just ten algorithms.</p><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="563" src="/images/post/solverubcub/oll_hu_ab103cab1aff8b20.webp" alt="pieces-of-rubiks-cube" onerror="this.onerror='null';this.src='\/images\/post\/solverubcub\/oll_hu_ca2c9c4224a7f4c5.webp'"/><h5 id="permutation-of-the-last-layer-pll">Permutation of the Last Layer (PLL)</h5><p>The final step is to permute the last layer pieces into their correct positions. This step requires knowledge of 21 algorithms in the full PLL method or just six in the two-look PLL approach. Mastering PLL allows for faster transition times and optimized finger tricks to reduce execution delays.</p><p>CFOP is the go-to method for many speedcubers because of its structured approach and ability to handle high turn-per-second (TPS) solves with ease. Even I use CFOP for solving the cube now—I do an intuitive cross and F2L, then use 2-look OLL and 1-look PLL.</p><p>For 2x2, I use the CLL (Corners Last Layer) method. You can check out my times here:<a href="https://events.cubelelo.com/profile/24CLMUM001" target="_blank">Cubelelo Profile</a> (it&rsquo;s unofficial, but yeah!).</p><hr><h4 id="roux-the-efficient-block-building-method">Roux: The Efficient Block-Building Method</h4><p>Roux, developed by Gilles Roux in 2003, takes a vastly different approach from CFOP. It focuses on reducing move count and minimizing cube rotations, making it ideal for one-handed solving. Unlike CFOP, Roux relies heavily on intuitive solving techniques and block-building rather than strict algorithm memorization.</p><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="563" src="/images/post/solverubcub/roux_hu_830233694da9adaa.webp" alt="pieces-of-rubiks-cube" onerror="this.onerror='null';this.src='\/images\/post\/solverubcub\/roux_hu_f21025be19c4f3fe.webp'"/><h5 id="first-block">First Block</h5><p>The solve begins by constructing a 1x2x3 block on one side of the cube. This is done by strategically placing the edge and corner pieces in the correct positions without disrupting already solved parts.</p><h5 id="second-block">Second Block</h5><p>A second 1x2x3 block is then built on the opposite side of the cube. At this point, the left and right blocks are complete, leaving only the middle slice and top layer unsolved.</p><h5 id="cmll-corner-orientation--permutation">CMLL (Corner Orientation &amp; Permutation)</h5><p>Instead of solving the last layer in steps like OLL and PLL, Roux addresses all four corners at once using CMLL (Corners of the Last Layer). This step requires only 42 algorithms but can be broken into smaller subsets for easier learning.</p><h5 id="lse-last-six-edges">LSE (Last Six Edges)</h5><p>The final stage focuses on solving the remaining six edges using M and U moves exclusively. This step is what makes Roux unique, as it avoids rotations and allows for smooth, fast execution.</p><p>The strength of Roux lies in its efficiency—solves often require fewer moves than CFOP, and its reliance on intuitive solving makes it an excellent alternative for those who prefer a different approach.</p><hr><h4 id="zz-the-method-designed-for-ergonomics">ZZ: The Method Designed for Ergonomics</h4><p>The ZZ method, named after its creator Zbigniew Zborowski, aims to balance efficiency and turning ergonomics. It pre-orients edges early in the solve, allowing the rest of the cube to be solved with only R, U, and L moves, eliminating cube rotations and awkward finger placements.</p><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="563" src="/images/post/solverubcub/zz_hu_262caba6804f46e1.webp" alt="pieces-of-rubiks-cube" onerror="this.onerror='null';this.src='\/images\/post\/solverubcub\/zz_hu_bd7a7635b6184fd8.webp'"/><h5 id="eoline-edge-orientation--line">EOLine (Edge Orientation &amp; Line)</h5><p>The solve begins by orienting all edges while placing two key edges along the bottom. This setup ensures that later steps can be executed smoothly without disrupting edge orientation.</p><h5 id="first-two-layers-f2l-1">First Two Layers (F2L)</h5><p>Unlike CFOP, which requires cube rotations for F2L, the ZZ method allows for rotationless F2L execution. Because edge orientation was handled in EOLine, all remaining F2L pairs can be inserted using only R, U, and L moves.</p><h5 id="last-layer">Last Layer</h5><p>Since all edges are already oriented, solving the last layer can be approached using CFOP-style algorithms or ZZ-specific techniques. The most advanced ZZ solvers use ZBLL (Zborowski-Bruchem Last Layer), which solves the entire last layer in one step, requiring knowledge of over 400 algorithms.</p><p>ZZ is an excellent choice for solvers who want to improve ergonomics while maintaining low move counts. However, EOLine can be challenging to master, making it slightly more difficult for beginners compared to CFOP.</p><hr><h4 id="layer-by-layer-lbl-the-beginner-friendly-method">Layer-by-Layer (LBL): The Beginner-Friendly Method</h4><p>The Layer-by-Layer (LBL) method is the most common beginner method. It involves solving the cube in three distinct layers:</p><ol><li>Solve the<strong>first layer</strong> by completing a cross and inserting corners.</li><li>Solve the<strong>second layer</strong> by inserting edge pieces into their correct slots.</li><li>Solve the<strong>last layer</strong> using algorithms to orient and permute the pieces.</li></ol><p>This method is easy to learn and provides a solid foundation for more advanced techniques like CFOP.</p><hr><h4 id="petrus-method-the-block-building-alternative">Petrus Method: The Block-Building Alternative</h4><p>The Petrus Method, developed by Lars Petrus, is a block-building approach that reduces move count and improves efficiency:</p><ol><li>Solve a<strong>2x2x2 block</strong> anywhere on the cube.</li><li>Expand it to a<strong>2x2x3 block</strong>.</li><li><strong>Orient the edges</strong> early to make solving easier.</li><li>Solve the<strong>remaining pieces</strong> with minimal moves.</li></ol><p>This method is useful for those who want an alternative to CFOP and prefer a more flexible solving approach.</p><hr><h4 id="choosing-the-right-method">Choosing the Right Method</h4><p>Each method has its strengths, and the best one depends on your goals:</p><ul><li><strong>CFOP</strong> is the best choice for speedcubers aiming for high TPS and efficiency.</li><li><strong>Roux</strong> is ideal for solvers who prefer intuitive solving and minimal rotations.</li><li><strong>ZZ</strong> is suited for those who want ergonomic solves with fewer rotations.</li><li><strong>LBL</strong> is great for beginners starting with the cube.</li><li><strong>Petrus</strong> is perfect for those who enjoy a block-building approach.</li></ul><p>No matter which method you choose, improving your lookahead, finger tricks, and efficiency will always be key to becoming a faster solver. Try out different approaches and see what works best for you!</p><p>Happy cubing!</p>
]]></content:encoded></item><item><title>Mechanics of Rubiks Cube #PID1.1</title><link>https://mummanajagadeesh.github.io/blog/mechanics-of-rubiks-cube/</link><pubDate>Fri, 31 Jan 2025 05:30:00 +0000</pubDate><guid>https://mummanajagadeesh.github.io/blog/mechanics-of-rubiks-cube/</guid><description>&lt;![CDATA[<p>The Rubik’s Cube is a 3D combination puzzle that has fascinated minds for decades. Invented in<strong>1974 by Ernő Rubik</strong>, a Hungarian architect and professor, it was originally called the &ldquo;Magic Cube.&rdquo; Designed as a teaching tool to explain 3D movement, it quickly became a global sensation. The challenge? Scramble it, then restore each face to a single color—sounds simple, but millions have struggled (and succeeded) at it since!</p>]]></description><content:encoded>&lt;![CDATA[<p>The Rubik’s Cube is a 3D combination puzzle that has fascinated minds for decades. Invented in<strong>1974 by Ernő Rubik</strong>, a Hungarian architect and professor, it was originally called the &ldquo;Magic Cube.&rdquo; Designed as a teaching tool to explain 3D movement, it quickly became a global sensation. The challenge? Scramble it, then restore each face to a single color—sounds simple, but millions have struggled (and succeeded) at it since!</p><h4 id="how-a-rubiks-cube-is-structured"><strong>How a Rubik’s Cube is Structured</strong></h4><p>At first glance, the Rubik’s Cube appears to be just 27 smaller cubes arranged in a 3x3 grid, but its internal mechanics are far more sophisticated. The core mechanism allows for smooth rotations, holding everything together while letting the outer pieces move freely.</p><p>The cube consists of three main types of pieces:</p><ul><li><strong>Core:</strong> The core holds the entire structure intact. It consists of six fixed center pieces that never move relative to each other.</li><li><strong>Edges:</strong> The cube has<strong>12 edge pieces</strong>, each with two colors, positioned between the centers.</li><li><strong>Corners:</strong> There are<strong>8 corner pieces</strong>, each with three colors, which determine the cube’s orientation.</li></ul><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="743" src="/images/post/rubcubemech/image_hu_1837300996c217a2.webp" alt="pieces-of-rubiks-cube" onerror="this.onerror='null';this.src='\/images\/post\/rubcubemech\/image_hu_ffa8f041449e65e3.png'"/><script>
window.addEventListener("load", (e) => {
const lightbox = GLightbox();
});</script><p>Each of these pieces interlocks in a way that allows rotation without disassembling the puzzle. When twisted, the cube rearranges itself by moving these smaller components around the core, yet the entire structure remains stable.</p><h4 id="how-it-rotates-and-functions"><strong>How It Rotates and Functions</strong></h4><p>Despite its scrambled look, a Rubik’s Cube follows a well-structured mechanical system:</p><ul><li><strong>Each face rotates independently</strong>, thanks to the internal core mechanism.</li><li>The<strong>center pieces remain static</strong>, acting as reference points for solving.</li><li><strong>Edge and corner pieces move around freely</strong>, rearranging their positions to create new patterns with every turn.</li></ul><p>Each move you make affects multiple pieces at once, creating complex shifts that can be solved using known algorithms. The key is to understand how these pieces interact with each turn to work towards a solution.</p><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="649" src="/images/post/rubcubemech/moves_hu_499aeee538e88e39.webp" alt="pieces-of-rubiks-cube" onerror="this.onerror='null';this.src='\/images\/post\/rubcubemech\/moves_hu_f2875e8dbb5ed35b.jpg'"/><h4 id="rubiks-cube-notation-3x3-standard-moves"><strong>Rubik’s Cube Notation (3x3 Standard Moves)</strong></h4><p>To communicate Rubik’s Cube solutions, we use standard notation:</p><ul><li><p><strong>Face Turns:</strong></p><ul><li><strong>R</strong> (Right) – Rotate the right face 90° clockwise</li><li><strong>R&rsquo;</strong> (Right Prime) – Rotate the right face 90° counterclockwise</li><li><strong>L</strong> (Left) – Rotate the left face 90° clockwise</li><li><strong>L&rsquo;</strong> (Left Prime) – Rotate the left face 90° counterclockwise</li><li><strong>U</strong> (Up) – Rotate the top face 90° clockwise</li><li><strong>U&rsquo;</strong> (Up Prime) – Rotate the top face 90° counterclockwise</li><li><strong>D</strong> (Down) – Rotate the bottom face 90° clockwise</li><li><strong>D&rsquo;</strong> (Down Prime) – Rotate the bottom face 90° counterclockwise</li><li><strong>F</strong> (Front) – Rotate the front face 90° clockwise</li><li><strong>F&rsquo;</strong> (Front Prime) – Rotate the front face 90° counterclockwise</li><li><strong>B</strong> (Back) – Rotate the back face 90° clockwise</li><li><strong>B&rsquo;</strong> (Back Prime) – Rotate the back face 90° counterclockwise</li></ul></li><li><p><strong>Double Turns:</strong></p><ul><li>Moves followed by<strong>2</strong> (e.g., R2, U2) indicate a 180° turn in either direction.</li></ul></li></ul><p>This notation is used universally among cubers, making it easier to follow solving guides and algorithms.</p><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="" src="/images/post/rubcubemech/cube_notation.gif" alt="pieces-of-rubiks-cube" onerror="this.onerror='null';this.src=''"/><h4 id="color-schemes-and-face-orientation"><strong>Color Schemes and Face Orientation</strong></h4><p>Most official Rubik’s Cubes follow a standardized<strong>color arrangement</strong>, which helps speedcubers recognize patterns quickly. The opposite face pairs are usually:</p><ul><li><strong>White ↔ Yellow</strong></li><li><strong>Blue ↔ Green</strong></li><li><strong>Red ↔ Orange</strong></li></ul><p>This scheme remains consistent across most cubes, ensuring uniformity in solving strategies. Recognizing the relationship between opposite faces is crucial when learning solving techniques, as many algorithms rely on the cube’s color orientation.</p><h4 id="different-types-of-rubiks-cubes-and-modifications"><strong>Different Types of Rubik’s Cubes and Modifications</strong></h4><p>The classic 3x3 is just the beginning! There are countless variations, each adding its own twist to the challenge:</p><h5 id="nxn-cubes-larger-and-smaller-variants"><strong>NxN Cubes (Larger and Smaller Variants)</strong></h5><ul><li><strong>2x2 (Mini Cube)</strong> – A simpler version, great for beginners.</li><li><strong>3x3 (Standard Cube)</strong> – The original and most popular.</li><li><strong>4x4 (Rubik’s Revenge)</strong> – Extra layers mean extra complexity.</li><li><strong>5x5 (Professor’s Cube)</strong> – More layers, more challenge.</li><li>Even larger cubes like<strong>6x6, 7x7, and beyond</strong> exist for advanced solvers.</li></ul><h5 id="shape-modifications"><strong>Shape Modifications</strong></h5><ul><li><strong>Fisher Cube</strong> – A 3x3 shape mod that appears to shift its center axis, making it visually deceptive.</li><li><strong>Windmill Cube</strong> – Features diagonal cuts, making rotations feel unpredictable.</li><li><strong>Axis Cube</strong> – Turns into a bizarre, asymmetric mess when scrambled.</li><li><strong>Mirror Cube</strong> – Solved by shape instead of color, adding an extra challenge.</li></ul><h5 id="other-unique-cubes"><strong>Other Unique Cubes</strong></h5><ul><li><strong>Pyraminx</strong> – A triangular-based twisty puzzle with simpler movement mechanics.</li><li><strong>Megaminx</strong> – A 12-sided dodecahedron cube with an extra layer of complexity.</li><li><strong>Ghost Cube</strong> – A shape-shifting cube that must be solved by aligning irregularly shaped pieces rather than colors.</li></ul><p>Each of these variations presents a unique solving challenge, keeping cubers engaged for years!</p><h4 id="final-thoughts"><strong>Final Thoughts</strong></h4><p>The Rubik’s Cube is more than just a toy—it’s a<strong>mechanical marvel, a brain workout, and an endless source of fun</strong>. Whether you’re solving a classic 3x3 or diving into the wild world of modded cubes, the principles remain the same:<strong>patterns, patience, and persistence</strong>. Solving a Rubik’s Cube can enhance cognitive skills like problem-solving, pattern recognition, and spatial awareness, making it a fantastic hobby for all ages.</p><p>So, grab a cube, start twisting, and embrace the puzzle madness! 🔄✨</p>
]]></content:encoded></item><item><title>Why Should You Start Solving Puzzles? #PID1.0</title><link>https://mummanajagadeesh.github.io/blog/why-should-you-start-solving-puzzles/</link><pubDate>Fri, 24 Jan 2025 05:30:00 +0000</pubDate><guid>https://mummanajagadeesh.github.io/blog/why-should-you-start-solving-puzzles/</guid><description>&lt;![CDATA[<p>Puzzles. Whether it’s a crossword, Sudoku, or that infuriating jigsaw piece that just won’t fit, puzzles have a unique way of grabbing our attention and holding it hostage. But have you ever wondered why solving puzzles feels so satisfying? Beyond the fun, puzzles sharpen your mind, boost your problem-solving skills, and, let’s be honest, give you bragging rights.</p>]]></description><content:encoded>&lt;![CDATA[<p>Puzzles. Whether it’s a crossword, Sudoku, or that infuriating jigsaw piece that just won’t fit, puzzles have a unique way of grabbing our attention and holding it hostage. But have you ever wondered why solving puzzles feels so satisfying? Beyond the fun, puzzles sharpen your mind, boost your problem-solving skills, and, let’s be honest, give you bragging rights.</p><p>At their core, puzzles are challenges—a structured way to ask, “How do I figure this out?” And that question is more important than you might think. Life, in many ways, is a giant puzzle: decisions, strategies, unexpected twists. The better we get at solving little puzzles, the more equipped we are for the big ones.</p><h4 id="why-solve-puzzles">Why Solve Puzzles?</h4><p>Solving puzzles is like a gym session for your brain. They improve cognitive functions, boost memory, and enhance focus. But beyond brainpower, puzzles teach patience, perseverance, and the art of thinking outside the box—skills that are valuable in any field, from coding to cooking to simply navigating life.</p><p>And let’s not forget the dopamine rush! Every solved puzzle is a small victory, and our brains love rewarding us for it. Who doesn’t like a mental high five?</p><hr><h4 id="coding-the-ultimate-mental-puzzle">Coding: The Ultimate Mental Puzzle</h4><p>Now, let’s talk about coding—arguably one of the most satisfying puzzles you can tackle. Writing code is essentially solving a problem using logic, creativity, and a touch of wizardry. Each bug is a mini-puzzle, each feature a challenge to implement. And with coding puzzles, you’re not just solving; you’re building.</p><p>For those who’ve wrestled with algorithms, debugging, or optimizing code, you’ll agree: coding is a puzzle that never gets old. The beauty lies in its versatility. Problems evolve, solutions change, and the learning never stops.</p><hr><h4 id="two-hands-one-brain-the-magic-of-physical-puzzles">Two Hands, One Brain: The Magic of Physical Puzzles</h4><p>Now, let’s take this a step further: puzzles that involve not just your mind but your hands too. Think about the Rubik’s Cube, where both hemispheres of your brain are engaged. The left brain loves the logical sequences and patterns, while the right brain enjoys the spatial awareness and creativity. It’s like a full-brain workout.</p><p>Physical puzzles improve hand-eye coordination, dexterity, and your ability to multitask. Plus, there’s something incredibly satisfying about manipulating an object and watching it transform under your hands. For me, working with physical puzzles has always been a humbling experience—you can’t brute force them; they demand focus and finesse.</p><hr><h4 id="the-rubiks-cube-a-personal-journey">The Rubik’s Cube: A Personal Journey</h4><p>Ah, the Rubik’s Cube—a classic that’s equal parts intriguing and intimidating. My first encounter with it was… chaotic, to say the least. Scrambling it was easy, but solving it? Let’s just say it took me more than a few attempts (and some YouTube tutorials) to get it right.</p><p>But once I did, it was magical. Solving a Rubik’s Cube is a dance of logic and intuition. It teaches you to think in layers, anticipate moves, and stay calm under pressure. And honestly, it’s just plain cool to whip one out and solve it in front of friends.</p><hr><h4 id="puzzles-meet-tech-the-rubiks-cube-solver">Puzzles Meet Tech: The Rubik’s Cube Solver</h4><p>Now imagine combining the worlds of physical and digital puzzles. That’s exactly what we’ll be doing in the<strong>Rubik’s Cube Solver</strong> series. This project is a blend of hardware and software challenges—a perfect playground for anyone who loves puzzles.</p><p>The idea is to build a system that can solve a Rubik’s Cube autonomously. From understanding the cube’s mechanics to writing algorithms that optimize solutions, this project is as rewarding as it is complex. And here’s the best part: you’ll get to learn and build alongside me.</p><p><em>Note</em>: This project is still in its simulation phase. There are optimizations to be made, and the hardware implementation is yet to be completed. But that’s the beauty of this series—we’ll question every step, learn from every mistake, and inch closer to the solution together.</p><hr><h4 id="introducing-the-pid-series">Introducing the PID Series</h4><p>The Rubik’s Cube Solver is just the beginning. The<strong>PID: Project IN Detail</strong> series is a deep dive into my projects, where I share insights, challenges, and breakthroughs. There’s a lot more to come, and I’m thrilled to have you along for the ride.</p><p>In the next few posts, we’ll start with the basics of solving a Rubik’s Cube—the foundational techniques that will help you tackle the puzzle manually. From there, we’ll explore how computers can do it better, faster, and with optimizations we can only dream of.</p><hr><h4 id="why-wait-lets-get-started">Why Wait? Let’s Get Started!</h4><p>Puzzles are more than a pastime; they’re a way of thinking, learning, and growing. Whether you’re a coding enthusiast, a Rubik’s Cube aficionado, or just someone who loves a good challenge, this series has something for you.</p><p>So grab a cube, roll up your sleeves, and let’s dive into the fascinating world of puzzles—one twist at a time.</p>
]]></content:encoded></item><item><title>My RosConIN&amp;#39;24 Experience</title><link>https://mummanajagadeesh.github.io/blog/my-rosconin24-experience/</link><pubDate>Fri, 17 Jan 2025 05:30:00 +0000</pubDate><guid>https://mummanajagadeesh.github.io/blog/my-rosconin24-experience/</guid><description>&lt;![CDATA[<p>Last year, I missed ROSCon India due to exams and, honestly, had no idea what I was missing out on. This year, though, I made it, and it turned out to be more than I ever imagined. The two days I spent at ROSConIN'24 were nothing short of transformative, and this blog itself is a result of the inspiration I drew from the event.</p>]]></description><content:encoded>&lt;![CDATA[<p>Last year, I missed ROSCon India due to exams and, honestly, had no idea what I was missing out on. This year, though, I made it, and it turned out to be more than I ever imagined. The two days I spent at ROSConIN'24 were nothing short of transformative, and this blog itself is a result of the inspiration I drew from the event.</p><h4 id="setting-the-stage">Setting the Stage</h4><p>The second edition of ROSCon India, following the overwhelming success of last year’s event with 750+ attendees, was hosted by ARTPARK at the Indian Institute of Science (IISc), Bangalore. This event brought together developers, hobbyists, researchers, and industry professionals to share, learn, and network over all things ROS (Robot Operating System).</p><p>What makes ROSCon India special is its resemblance to the international ROSCon, with a local flavor that emphasizes India’s growing influence in robotics and automation. It was heartwarming to see support from Open Robotics and the enthusiastic efforts of Acceleration Robotics, RigBetel Labs, and ARTPARK in organizing such an impactful gathering.</p><h4 id="day-0-workshops-galore">Day 0: Workshops Galore</h4><p>The event kicked off with Workshop Day on December 4th. We could choose one out of three workshops, and I opted for the third one:</p><ol><li><p><strong>Workshop 1: Next-Gen Robotics Development with NVIDIA ISAAC, GenAI, and ROS</strong> – Organized by NVIDIA, this workshop offered a comprehensive dive into cutting-edge robotics development. Though I couldn’t attend it, the buzz from the attendees made it clear how enriching it was.</p></li><li><p><strong>Workshop 2: Leveraging Zenoh as a ROS 2 Middleware Layer</strong> – Conducted by Zettascale Technology, this session explored Zenoh as an innovative middleware layer for ROS 2. It intrigued many participants and opened up discussions on its potential.</p></li><li><p><strong>Workshop 3: ROS 2 Controls, Navigation, and Advanced Communication Study</strong> – Organized by ARTPARK, this was my pick! A deep dive into ROS 2 controls, navigation, and communication felt perfectly aligned with my interests. The hands-on experience and insights I gained were invaluable.</p></li></ol><h4 id="day-1-a-conference-to-remember">Day 1: A Conference to Remember</h4><p>December 5th began with registrations and a warm welcome address by the organizers, setting an enthusiastic tone. Some highlights from the day included:</p><ul><li><strong>Keynotes</strong>: Geoffrey Biggs (CTO, Open Robotics) and Yadunund Vijay (Intrinsic) discussed the state of Open Robotics in 2024, followed by Jigar Halani from NVIDIA sharing insights on robotics development.</li><li><strong>Inspiring Talks</strong>: From Yuvraj Mehta’s session on RoboGPT to Sarvesh Kumar Malladi’s insights on Universal Robots’ ROS2 features, each talk added depth to the learning experience.</li><li><strong>Industry Focus</strong>: Anish Dalvi from TCS delved into automotive protocols, while Somdeb Saha highlighted retail automation with ROS. Both sessions demonstrated the vast industrial applications of ROS.</li><li><strong>Panel Discussion</strong>: The day ended with an engaging panel on cracking the product-market fit in robotics, featuring founders and investors sharing valuable insights.</li></ul><h4 id="day-2-deep-dives-and-future-directions">Day 2: Deep Dives and Future Directions</h4><p>December 6th brought more enlightening sessions, including:</p><ul><li><strong>Keynote by Angelo Corsaro (ZettaScale Technology)</strong>: An in-depth look at Zenoh as an alternative middleware layer for ROS 2.</li><li><strong>Technical Sessions</strong>: From Ajay Sethi and Prateek Nagras introducing the Robotics Application Stack to Nidhi Choudhary’s integration of physics-based neural networks with Gazebo, the variety of topics covered was astounding.</li><li><strong>Fireside Chat</strong>: The event concluded with a thought-provoking discussion on the future of ROS, featuring Geoffrey Biggs, Yadunund Vijay, and Angelo Corsaro.</li></ul><h4 id="networking-and-personal-highlights">Networking and Personal Highlights</h4><p>One of the most fulfilling aspects of ROSConIN’24 was the chance to meet incredible people. I connected with alumni from my college, including Aryan Jaguste and Jatin Vera, whose experience in robotics left me inspired. I also met many other professionals who have been in this field for years, generously sharing their knowledge and encouraging me to keep learning and experimenting.</p><p>It was this vibrant exchange of ideas and stories that inspired me to start this blog. ROSConIN’24 wasn’t just a conference; it was a catalyst for my growth in the robotics domain.</p><h4 id="a-big-shoutout">A Big Shoutout</h4><p>A massive thanks to all the companies and individuals who made this event possible, including Acceleration Robotics, RigBetel Labs, ARTPARK, NVIDIA, Zettascale Technology, Tata Consultancy Services, Universal Robots, and many others like Autodiscovery, Botsoverkill, Bullwork, Golain, I-Hub Jodhpur, IISc, Kikobot, Nawe, Neuralzome, Thundroids, Vicharak, Virya, and xTerra.</p><h4 id="closing-thoughts">Closing Thoughts</h4><p>As I reflect on my experience, I can’t help but marvel at how much this event has broadened my horizons. It’s not just about the technical knowledge but also the sense of community and the shared passion for innovation. If you’re even remotely interested in robotics or automation, attending ROSConIN should be a no-brainer. Who knows? You might walk away with a new project idea, a mentor, or even the inspiration to start a blog, just like I did!</p><p>As if ROSConIN’24 wasn’t enough to fill my plate with excitement, the very next day, I found myself at the GNOME Asia Summit in Bangalore. It was an incredible opportunity to connect with open-source contributors and Linux enthusiasts. The vibe there was equally inspiring, and I came away with even more appreciation for the open-source community.</p><p>Speaking of Bangalore, let’s not forget the city&rsquo;s legendary traffic. I’ll admit, navigating those jam-packed streets tested my patience (and Google Maps’ sanity), but the morning chill and the sheer energy of being in India’s tech hub more than made up for it. If you’ve never tried squeezing in two major conferences in one trip, let me tell you—it’s like a crash course in networking and caffeine dependency.</p><p>Meeting passionate individuals at both events has only fueled my curiosity to dive deeper into robotics and open source. In fact, it was during ROSConIN’24 that I felt inspired to start this tech blog, channeling my excitement and insights into something more tangible. And hey, if I can navigate Bangalore traffic and two back-to-back conferences, I’m pretty sure I can handle just about anything life throws my way!</p><p>Interestingly, I was one of the few people there who received an ID with a handwritten name instead of a printed one. A few people were even unsure if it was genuine—which honestly just added to the uniqueness of the experience.</p><img title="" loading="lazy" decoding="async" class="img  " width="300" height="425" src="/images/post/roscon/roscon_hu_cca74c7c1ba4f819.webp" alt="Original Image 1" onerror="this.onerror='null';this.src='\/images\/post\/roscon\/roscon_hu_7f7d1e57116a4709.jpeg'"/><script>
window.addEventListener("load", (e) => {
const lightbox = GLightbox();
});</script>
]]></content:encoded></item><item><title>Why Blog in 2025? (And How to Get Started)</title><link>https://mummanajagadeesh.github.io/blog/why-blog-in-2025/</link><pubDate>Fri, 10 Jan 2025 05:30:00 +0000</pubDate><guid>https://mummanajagadeesh.github.io/blog/why-blog-in-2025/</guid><description>&lt;![CDATA[<p>In 2025, with the internet brimming with TikToks, reels, and AI-generated articles, you might wonder—<strong>is blogging still worth it?</strong><br>
The answer is a resounding<strong>YES</strong>, and here’s why.</p>]]></description><content:encoded>&lt;![CDATA[<p>In 2025, with the internet brimming with TikToks, reels, and AI-generated articles, you might wonder—<strong>is blogging still worth it?</strong><br>
The answer is a resounding<strong>YES</strong>, and here’s why.</p><hr><h5 id="why-blog-in-2025"><strong>Why Blog in 2025?</strong></h5><h6 id="share-your-unique-perspective"><strong>Share Your Unique Perspective</strong></h6><p>In a world of AI-generated content,<strong>your personal voice matters more than ever</strong>. AI might generate the basics, but<strong>stories, experiences, and personal insights</strong> are uniquely human. Whether you’re building your first robot, sharing parenting tips, or learning a new language, your journey can inspire others.</p><blockquote><p>Think about it: How many times have you Googled a problem, stumbled upon a blog, and found exactly what you needed? That could be you helping someone else.</p></blockquote><hr><h6 id="build-your-digital-legacy"><strong>Build Your Digital Legacy</strong></h6><p>Your blog is<strong>your corner of the internet</strong>—a space to leave your mark. Unlike fleeting social media posts, blogs are evergreen, searchable, and<strong>build a record of your growth</strong>. For developers, it can be a portfolio of your work; for creatives, it’s a gallery of your creations.</p><blockquote><p>I started my blog to document my tech projects, but I realized it’s also helping me keep track of my ideas, progress, and experiments. Plus, I’ve already met people who share the same passions—thanks to this little space!</p></blockquote><hr><h6 id="learn-as-you-share"><strong>Learn as You Share</strong></h6><p>Writing is an incredible teacher. To explain something clearly, you need to truly understand it yourself.</p><ul><li>Developers often blog about solutions to bugs or coding techniques, which not only helps others but reinforces their own knowledge.</li><li>For non-tech folks, writing about personal projects—whether it’s DIY, cooking, or fitness—gives clarity and keeps you motivated.</li></ul><blockquote><p><strong>Pro Tip:</strong> Blogging can make you a better problem-solver. Breaking down problems into digestible steps is the essence of both writing and coding.</p></blockquote><hr><h6 id="build-connections-and-opportunities"><strong>Build Connections and Opportunities</strong></h6><p>Blogging isn’t just about putting your thoughts into words; it’s about<strong>starting conversations</strong>. Your blog can:</p><ul><li>Attract collaborators who resonate with your ideas.</li><li>Impress potential employers or clients by showcasing your expertise.</li><li>Connect you with a like-minded community.</li></ul><blockquote><p>Think of it as networking without the awkward handshakes.</p></blockquote><hr><h6 id="stay-relevant-in-the-ai-era"><strong>Stay Relevant in the AI Era</strong></h6><p>AI is great for automating tasks, but<strong>creativity, originality, and storytelling</strong>? That’s all you. A blog lets you flex those creative muscles and prove you’re not just keeping up with the times—you’re shaping them.</p><hr><h6 id="why-students-should-start-blogging-"><strong>Why Students Should Start Blogging</strong> 🎓</h6><p>As a student, blogging can be a game-changer for your personal and professional growth. Here’s why:</p><ol><li><p><strong>Showcase Your Skills</strong>:<br>
Your blog can act as a dynamic portfolio. Whether it’s coding projects, research papers, or even creative writing, it’s a platform to demonstrate your expertise and passion. Employers and professors love seeing initiative.</p></li><li><p><strong>Document Your Learning</strong>:<br>
Writing about what you’re learning—whether it’s a tough algorithm, a robotics project, or study hacks—helps reinforce your understanding and creates a resource for others.</p></li><li><p><strong>Stand Out</strong>:<br>
In a competitive world, a well-maintained blog sets you apart. It shows that you’re not just a passive learner but someone who actively contributes to the community.</p></li><li><p><strong>Build Connections</strong>:<br>
Blogging opens doors to collaborations, internships, and mentorships. Sharing your work publicly can attract like-minded peers, professors, or even recruiters.</p></li></ol><blockquote><p><strong>Pro Tip for Students</strong>: Start small. Write about a project or concept you recently worked on in class—it’s a great way to begin!</p></blockquote><hr><h5 id="how-to-start-blogging-in-2025"><strong>How to Start Blogging in 2025</strong></h5><p>If all this has convinced you, let’s talk about how to get started! Whether you’re a dev documenting code or someone sharing life hacks, blogging has never been easier.</p><hr><h6 id="choose-your-purpose"><strong>Choose Your Purpose</strong></h6><p>Ask yourself:<strong>Why do I want to blog?</strong></p><ul><li>Is it to document your journey (like me)?</li><li>Share your expertise?</li><li>Build a personal brand?</li><li>Just for fun?</li></ul><blockquote><p>Defining your purpose will help you stay motivated and give your blog a clear focus.</p></blockquote><hr><h6 id="pick-the-right-platform"><strong>Pick the Right Platform</strong></h6><p>Here are a few options to suit different needs:</p><ul><li><strong>Techies</strong>: Use GitHub Pages for free hosting or platforms like Jekyll/Hugo for custom setups.</li><li><strong>Beginners</strong>: Try WordPress or Ghost—they’re user-friendly and have tons of templates.</li><li><strong>Minimalists</strong>: Substack or Medium are great for simple, distraction-free writing.</li></ul><blockquote><p><strong>What I Use</strong>: I opted for GitHub Pages because I love having full control over my blog’s look and feel.</p></blockquote><hr><h6 id="write-what-you-know-and-love"><strong>Write What You Know (And Love)</strong></h6><p>Find your niche. You don’t need to be an expert—just share your journey as you learn.</p><ul><li>Devs: Write about side projects, tutorials, or debugging solutions.</li><li>Non-devs: Document hobbies, productivity hacks, or personal experiences.</li></ul><blockquote><p>Remember: What’s obvious to you might be groundbreaking to someone else.</p></blockquote><hr><h6 id="keep-it-simple-at-first"><strong>Keep It Simple (At First)</strong></h6><p>Don’t overcomplicate it. Your first post can be:</p><ul><li>An introduction to who you are.</li><li>A story about a project you worked on.</li><li>A simple “lesson learned” post.</li></ul><blockquote><p>It’s okay if your first post isn’t perfect—it’s better to start and improve as you go.</p></blockquote><hr><h6 id="leverage-ai-to-help-you"><strong>Leverage AI to Help You</strong></h6><p>In 2025, AI tools can make blogging easier:</p><ul><li>Use<strong>ChatGPT</strong> for brainstorming post ideas.</li><li><strong>Grammarly</strong> can polish your grammar.</li><li>Tools like<strong>Jasper AI</strong> can even generate draft content.</li></ul><p>But remember:<strong>your voice is the star.</strong> AI can assist, but authenticity is irreplaceable.</p><hr><h6 id="promote-your-blog"><strong>Promote Your Blog</strong></h6><p>Once your blog is live, share it!</p><ul><li>Post about it on LinkedIn, Instagram, or Twitter.</li><li>Join communities (Reddit, Discord, forums) related to your niche.</li><li>Collaborate with others by guest-posting or linking to their work.</li></ul><blockquote><p>If you’re consistent, people will notice.</p></blockquote><hr><h6 id="embrace-the-process"><strong>Embrace the Process</strong></h6><p>Blogging is a journey. Don’t stress about being perfect—just keep writing, experimenting, and learning. Tools like<strong>Google Analytics</strong> can show you what’s working and help you refine your style.</p><hr><h5 id="final-thoughts"><strong>Final Thoughts</strong></h5><p>Blogging in 2025 is about more than just writing—it’s about<strong>sharing your voice, building connections, and leaving a legacy</strong>. Whether you’re a coder, a hobbyist, or someone with a passion to share, there’s never been a better time to start.</p><blockquote><p><strong>Your Blog, Your Rules:</strong> It doesn’t have to be fancy. It just has to be<strong>you.</strong></p></blockquote><hr><h5 id="whats-next"><strong>What’s Next?</strong></h5><p>If you’re thinking of starting a blog, go for it! Your ideas are worth sharing. Feel free to reach out if you need help setting things up or brainstorming ideas—I’d love to hear from you.</p><p>Ready to take the plunge? Hit that<strong>&ldquo;New Blog&rdquo;</strong> button and let the world hear your voice!</p><hr>
]]></content:encoded></item><item><title>Hello World!! (A MUST READ)</title><link>https://mummanajagadeesh.github.io/blog/hello-world/</link><pubDate>Fri, 03 Jan 2025 05:30:00 +0000</pubDate><guid>https://mummanajagadeesh.github.io/blog/hello-world/</guid><description>&lt;![CDATA[<p>Hello, and thank you for visiting. My name is<strong>Jagadeesh</strong>, and this blog is a personal and professional record of my journey through college, projects, and the experiences that continue to shape my interests and aspirations.</p>]]></description><content:encoded>&lt;![CDATA[<p>Hello, and thank you for visiting. My name is<strong>Jagadeesh</strong>, and this blog is a personal and professional record of my journey through college, projects, and the experiences that continue to shape my interests and aspirations.</p><p>Currently, I am pursuing my undergraduate studies at<strong>NIT Calicut</strong>, where I am learning, building, and continuously exploring new ideas across engineering, technology, and beyond.<br>
This space reflects not only my technical pursuits but also my broader experiences—from milestones and challenges to reflections on the people and places that have influenced my path.</p><p>I know no one would care to read this, even I sound like someone who thinks they do&hellip; :( but anyways.</p><p>Through this blog, I aim to document my work thoughtfully, share insights, and connect with others who are equally passionate about learning and creating.</p><hr><h5 id="purpose-of-this-blog">Purpose of This Blog</h5><p>The main purpose of this blog is to<strong>organize and document</strong> my projects, ideas, and experiences in a structured way.</p><p>Over time, I realized that many of my projects ended up scattered across private repositories or forgotten folders. This space is simply an effort to gather them in one place, with a bit more context and thoughtfulness.
While most of the projects here are small or experimental, documenting them helps me reflect and learn. If anyone else happens to find something useful along the way, that would just be a nice bonus.</p><p>The content here includes technical notes, project write-ups, tutorials, and personal reflections from different experiences.</p><hr><h5 id="about-me">About Me</h5><p>I am<strong>Jagadeesh Mummana</strong>, a<strong>sophomore at NIT Calicut</strong>, majoring in<strong>Electronics and Communication Engineering</strong> with a minor in<strong>Robotics and Automation</strong>.</p><p>My focus is on<strong>electronics</strong> and<strong>hardware design</strong>, particularly in<strong>VLSI systems</strong>, and how these can be used to optimize hardware for<strong>AI applications</strong>. I’m interested in improving hardware performance to support the intensive computational needs of AI models. I also enjoy the hands-on process of building<strong>real-world robots</strong> from scratch, where integrating hardware and software effectively is essential to their operation.</p><p>I enjoy taking the time to understand systems; building them, exploring how they work, and trying to make small improvements through trial and error.</p><p>Outside of academics, I spend time on personal projects, club activities, and technical experiments, as well as trying to slowly build both technical and creative skills.</p><hr><h5 id="about-this-site">About This Site</h5><p>This site originally began as a portfolio project for my final submission in the<strong>CS50x 2024 Course</strong>.<br>
The initial version remains accessible<a href="https://jagadeesh-mummana.vercel.app/" target="_blank">here</a> or through the navigation bar.</p><p>Since then, it has grown into a broader personal and technical blog.<br>
The site is built with<strong>Hugo</strong>, using the<a href="https://github.com/statichunt/geeky-hugo" target="_blank">Geeky-Hugo theme</a> as a foundation, which I have<strong>extensively customized</strong> to better meet my evolving requirements. It is hosted via<strong>GitHub Pages</strong>.</p><p>The inspiration to start a formal blog came during my visit to<strong>ROSCon India 2024 (IISc Banglore)</strong>.<br>
At the event, I had the opportunity to interact with peers and seniors from several institutions, many of whom were doing exceptional work across different technical domains. I was struck by their clarity when explaining their projects, yet I noticed that structured documentation and public presentation of their work were often lacking.</p><p>Through conversations, it became apparent that this was not due to a lack of ability but rather due to documentation being a lower priority amidst academic and project commitments. This realization stayed with me and motivated me to establish a platform where I could not only share the technical details but also convey the motivations, processes, challenges, and insights behind each project.</p><p>It is my belief that even a<strong>basic</strong> project, when fully understood and properly documented, holds more educational and developmental value than a<strong>great</strong> one with only a surface-level understanding.<br>
Taking the time to work through each part of a project helps me learn more and feel more connected to the work.</p><p>You can find more about this inspiration and my reflections on the event in<a href="https://mummanajagadeesh.github.io/blog/my-rosconin24-experience/" target="_blank">this post</a>.</p><hr><h5 id="why-i-built-this-blog">Why I Built This Blog</h5><p>This site serves as a<strong>central platform</strong> to organize, document, and share my technical work and experiments.</p><p>Since many of my<strong>GitHub repositories are private</strong>, this blog acts as an open and structured window into the projects I am pursuing, providing detailed insights, learnings, and future directions.</p><p>Some of the projects are actively maintained, others are works in progress, and a few have reached functional milestones but are continually refined as my understanding evolves.</p><blockquote><p>All projects featured here are either<strong>independently developed</strong> or<strong>draw upon external inspirations</strong>, in which case appropriate credits are acknowledged.</p></blockquote><hr><h5 id="for-industry-professionalsrecruiters">For Industry Professionals/Recruiters</h5><p>If you are an industry professional or recruiter visiting this site, I encourage you to explore the &ldquo;<a href="https://mummanajagadeesh.github.io/projects/" target="_blank">Projects</a>&rdquo; and &ldquo;<a href="https://mummanajagadeesh.github.io/about/" target="_blank">About</a>&rdquo; sections for a clearer perspective on my work, interests, and approach.</p><p>I am currently seeking internship opportunities in areas such as<strong>digital and mixed-signal VLSI design</strong>, particularly those that intersect with<strong>robotics</strong> and<strong>AI-enhanced systems</strong>.</p><p>I am always eager to learn and explore new challenges, and I would love the opportunity to contribute to innovative projects in these areas. If you&rsquo;d like to explore any of my private repositories or discuss a project in more detail, I&rsquo;d be happy to share them upon request.</p><p>Thank you for considering my work.</p><hr><h5 id="for-peers-friends-and-fellow-explorers">For Peers, Friends, and Fellow Explorers</h5><p>If you are a peer, a friend, or simply an interested visitor, I warmly welcome discussions, collaborations, and exchange of ideas.</p><p>I believe that collaboration enriches learning for all parties involved, providing diverse perspectives and new ways of thinking.<br>
If you have an idea to discuss, a project to collaborate on, or simply wish to connect, please feel free to reach out through the<a href="https://mummanajagadeesh.github.io/contact/" target="_blank">Contact Page</a> or via direct messages on the platforms listed below.</p><p>While I am active across multiple channels, my activity on Instagram is currently limited.</p><hr><h5 id="why-i-focus-on-projects">Why I Focus on Projects</h5><p>For those wondering why there is an emphasis on building and documenting projects, the reason is simple:<strong>I believe true learning comes through hands-on experience</strong>.</p><p>Academic projects often serve their purpose within fixed timelines, but rarely do they encourage continuous exploration beyond submission. When I initiate and manage a project entirely—starting from its original motivation through to real-world deployment—I develop a deeper sense of ownership and a stronger commitment to refine and improve it over time.</p><p>Many of my projects span across disciplines, and this intersectional nature drives me to think critically and holistically. I am always open to collaborations that allow for exploration across varied fields.</p><p>In my view, engaging actively with projects provides a form of learning that cannot be replaced by passive consumption, whether through videos or multiple online courses.<br>
While these resources are important, I believe that their true value is realized when they are used as stepping stones to build something original and substantial.</p><hr><h5 id="connect-with-me">Connect With Me</h5><ul><li>🌐<strong>Portfolio</strong>:<a href="https://mummanajagadeesh.github.io" target="_blank">mummanajagadeesh.github.io</a></li><li>💼<strong>LinkedIn</strong>:<a href="https://www.linkedin.com/in/jagadeesh-mummana" target="_blank">Jagadeesh Mummana</a></li><li>🔧<strong>GitHub</strong>:<a href="https://github.com/Mummanajagadeesh" target="_blank">Mummanajagadeesh</a></li><li>📸<strong>Instagram</strong>:<a href="https://www.instagram.com/jagadeesh__97__" target="_blank">@jagadeesh__97__</a></li></ul><p>For additional social links and informal contact options, please visit the<a href="https://mummanajagadeesh.github.io/contact/" target="_blank">Contact Page</a>.</p><hr><p>Thank you for your time and interest. I look forward to continuing this journey of learning, building, and sharing, and I hope you find something here that resonates with you.</p>
]]></content:encoded></item><item><title/><link>https://mummanajagadeesh.github.io/projects/gpbot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/projects/gpbot/</guid><description>&lt;![CDATA[<h2 id="gpbot---basic-sensor-based-general-purpose-amr"><a href="https://github.com/Mummanajagadeesh/gpbot-w" target="_blank">GPBOT - Basic Sensor based General Purpose AMR</a></h2><h5 id="do-checkout-">Do checkout :</h5><h5 id="basic-line-following-robot"><strong><a href="https://mummanajagadeesh.github.io/projects/gpbot/lfr" target="_blank">Basic Line Following Robot</a></strong></h5><h5 id="obstacle-avoidance-robot"><strong><a href="https://mummanajagadeesh.github.io/projects/gpbot/obstacle" target="_blank">Obstacle Avoidance Robot</a></strong></h5><h5 id="wall-follower-robot"><strong><a href="https://mummanajagadeesh.github.io/projects/gpbot/wallfollow" target="_blank">Wall Follower Robot</a></strong></h5><h5 id="differential-drive-robot"><strong><a href="https://mummanajagadeesh.github.io/projects/gpbot/diffdrive" target="_blank">Differential Drive Robot</a></strong></h5><table><thead><tr><th><strong>Name</strong></th><th>GPBOT</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>This 4-wheeled robot is equipped with GPS, IMU, LiDAR, Distance Sensors, and a 2-DOF camera (using linear and rotary actuators). It detects objects using computer vision, avoids obstacles, and navigates autonomously.</td></tr><tr><td><strong>Start</strong></td><td>June 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/gpbot-w" target="_blank">GPBOT🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Simulation, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Webots, Python</td></tr><tr><td><strong>Current Status</strong></td><td>On Hold</td></tr></tbody></table><h3 id="overview">Overview</h3><p>The robot is equipped with<strong>GPS, IMU, LiDAR, and a 2-DOF camera</strong>, enabling it to detect objects using computer vision, avoid obstacles, and navigate autonomously.</p>]]></description><content:encoded>&lt;![CDATA[<h2 id="gpbot---basic-sensor-based-general-purpose-amr"><a href="https://github.com/Mummanajagadeesh/gpbot-w" target="_blank">GPBOT - Basic Sensor based General Purpose AMR</a></h2><h5 id="do-checkout-">Do checkout :</h5><h5 id="basic-line-following-robot"><strong><a href="https://mummanajagadeesh.github.io/projects/gpbot/lfr" target="_blank">Basic Line Following Robot</a></strong></h5><h5 id="obstacle-avoidance-robot"><strong><a href="https://mummanajagadeesh.github.io/projects/gpbot/obstacle" target="_blank">Obstacle Avoidance Robot</a></strong></h5><h5 id="wall-follower-robot"><strong><a href="https://mummanajagadeesh.github.io/projects/gpbot/wallfollow" target="_blank">Wall Follower Robot</a></strong></h5><h5 id="differential-drive-robot"><strong><a href="https://mummanajagadeesh.github.io/projects/gpbot/diffdrive" target="_blank">Differential Drive Robot</a></strong></h5><table><thead><tr><th><strong>Name</strong></th><th>GPBOT</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>This 4-wheeled robot is equipped with GPS, IMU, LiDAR, Distance Sensors, and a 2-DOF camera (using linear and rotary actuators). It detects objects using computer vision, avoids obstacles, and navigates autonomously.</td></tr><tr><td><strong>Start</strong></td><td>June 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/gpbot-w" target="_blank">GPBOT🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Simulation, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Webots, Python</td></tr><tr><td><strong>Current Status</strong></td><td>On Hold</td></tr></tbody></table><h3 id="overview">Overview</h3><p>The robot is equipped with<strong>GPS, IMU, LiDAR, and a 2-DOF camera</strong>, enabling it to detect objects using computer vision, avoid obstacles, and navigate autonomously.</p><h4 id="live-interaction">Live Interaction</h4><p>Interact with the robot in real-time via the following link:</p><p><a href="https://webots.cloud/ScKiz83?upload=webots" target="_blank">CLOUD INTERACTION_WEBOTS</a></p><h3 id="demo-videos">Demo Videos</h3><p>Explore the robot’s functionalities through these demo videos:</p><table><thead><tr><th>Camera Test</th><th>Object Detection</th><th>LiDAR in Action</th></tr></thead><tbody><tr><td><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/rqTKV85uOz4" frameborder="0" allowfullscreen=/></div></div></td><td><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/wl0mYWiO184" frameborder="0" allowfullscreen=/></div></div></td><td><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/9qBLSOo2feE" frameborder="0" allowfullscreen=/></div></div></td></tr></tbody></table><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="576" src="/images/projects/gpbot/gpbot_hu_71ed4d21a06c88ca.webp" alt="GPBOT" onerror="this.onerror='null';this.src='\/images\/projects\/gpbot\/gpbot_hu_bbb56c03af335fa8.png'"/><script>
window.addEventListener("load", (e) => {
const lightbox = GLightbox();
});</script><h3 id="robot-overview">Robot Overview</h3><h4 id="features">Features:</h4><ul><li><strong>4-Wheeled Mobility:</strong> Designed for efficient movement in diverse environments.</li><li><strong>Sensor Suite:</strong><ul><li><strong>GPS:</strong> Provides global positioning and navigation.</li><li><strong>IMU:</strong> Tracks orientation and movement.</li><li><strong>LiDAR:</strong> Maps the surroundings and detects obstacles.</li><li><strong>2-DOF Camera:</strong> Offers flexible vision capabilities with linear and rotary actuation.</li></ul></li><li><strong>Autonomous Operation:</strong> The robot navigates without relying on complex algorithms, utilizing basic control mechanisms.</li></ul><h3 id="controls">Controls</h3><p>The following key mappings define the robot’s movement and camera operations:</p><h4 id="movement-controls">Movement Controls</h4><table><thead><tr><th>Key</th><th>Action</th></tr></thead><tbody><tr><td>↑</td><td>Move forward</td></tr><tr><td>↓</td><td>Move backward</td></tr><tr><td>←</td><td>Turn left</td></tr><tr><td>→</td><td>Turn right</td></tr></tbody></table><h4 id="camera-controls">Camera Controls</h4><table><thead><tr><th>Key</th><th>Action</th></tr></thead><tbody><tr><td>W</td><td>Move camera up</td></tr><tr><td>S</td><td>Move camera down</td></tr><tr><td>A</td><td>Rotate camera left (ACW)</td></tr><tr><td>D</td><td>Rotate camera right (CW)</td></tr></tbody></table><h4 id="control-visualization">Control Visualization</h4><h5 id="movement">Movement</h5><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span> ┌───┐┌───┐┌───┐</span></span><span style="display:flex;"><span> │ ← ││ ↑ ││ → │</span></span><span style="display:flex;"><span> │ ← │└───┘│ → │</span></span><span style="display:flex;"><span> │ ← │┌───┐│ → │</span></span><span style="display:flex;"><span> │ ← ││ ↓ ││ → │</span></span><span style="display:flex;"><span> └───┘└───┘└───┘</span></span></code></pre></div><h5 id="camera">Camera</h5><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span> ┌───┐</span></span><span style="display:flex;"><span> │ W │</span></span><span style="display:flex;"><span> └───┘</span></span><span style="display:flex;"><span> ┌───┐┌───┐┌───┐</span></span><span style="display:flex;"><span> │ A ││ S ││ D │</span></span><span style="display:flex;"><span> └───┘└───┘└───┘</span></span></code></pre></div><h3 id="implementation-details">Implementation Details</h3><p>The robot has been built from scratch, integrating multiple sensors to enhance its navigation and detection abilities.</p><ul><li><strong>Camera Functionality:</strong> The 2-DOF camera captures images and analyzes the environment for object recognition.</li><li><strong>Object Detection:</strong> Using computer vision, the robot identifies and reacts to obstacles in its path.</li></ul><h3 id="installation-and-usage">Installation and Usage</h3><h4 id="requirements">Requirements</h4><ul><li><strong>Webots:</strong> Download and install Webots from<a href="https://cyberbotics.com/" target="_blank">here</a>.</li></ul><h4 id="setup-instructions">Setup Instructions</h4><ol><li>Clone the repository:<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/Mummanajagadeesh/gpbot-w.git</span></span><span style="display:flex;"><span>cd gpbot-w</span></span></code></pre></div></li><li>Open Webots and load the robot’s world file.</li><li>Start the simulation to test the robot’s capabilities.</li></ol><h3 id="future-enhancements">Future Enhancements</h3><ul><li><strong>Algorithm Integration:</strong> Implement advanced path planning and control algorithms.</li><li><strong>Enhanced Object Recognition:</strong> Utilize AI-based models for improved object detection.</li><li><strong>Multi-Robot Coordination:</strong> Develop cooperative strategies for multiple robots in a shared space.</li></ul>
]]></content:encoded></item><item><title/><link>https://mummanajagadeesh.github.io/projects/gpbot/diffdrive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/projects/gpbot/diffdrive/</guid><description>&lt;![CDATA[<h2 id="differential-drive-robot-simulation"><a href="https://github.com/Mummanajagadeesh/differential-drive-robot-w" target="_blank">Differential Drive Robot Simulation</a></h2><table><thead><tr><th><strong>Name</strong></th><th>Differential Drive Robot</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>A custom-built robot featuring a differential drive system that calculates its position and movement based on wheel rotations</td></tr><tr><td><strong>Start</strong></td><td>June 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/differential-drive-robot-w" target="_blank">DDR🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Simulation, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Webots, Python</td></tr><tr><td><strong>Current Status</strong></td><td>On Hold</td></tr></tbody></table><hr><p>This repository contains the simulation of a<strong>Differential Drive Robot</strong>, built from scratch, which uses basic odometry to track its position. The robot is equipped with motors and position sensors to calculate its movement and orientation while navigating the environment.</p>]]></description><content:encoded>&lt;![CDATA[<h2 id="differential-drive-robot-simulation"><a href="https://github.com/Mummanajagadeesh/differential-drive-robot-w" target="_blank">Differential Drive Robot Simulation</a></h2><table><thead><tr><th><strong>Name</strong></th><th>Differential Drive Robot</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>A custom-built robot featuring a differential drive system that calculates its position and movement based on wheel rotations</td></tr><tr><td><strong>Start</strong></td><td>June 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/differential-drive-robot-w" target="_blank">DDR🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Simulation, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Webots, Python</td></tr><tr><td><strong>Current Status</strong></td><td>On Hold</td></tr></tbody></table><hr><p>This repository contains the simulation of a<strong>Differential Drive Robot</strong>, built from scratch, which uses basic odometry to track its position. The robot is equipped with motors and position sensors to calculate its movement and orientation while navigating the environment.</p><h4 id="demo-video"><strong>Demo Video</strong></h4><p>Click the image below to watch a demo of the robot in action:</p><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/FwafE7gGxxY" frameborder="0" allowfullscreen=/></div></div><hr><h4 id="how-it-works"><strong>How It Works</strong></h4><h5 id="robot-design"><strong>Robot Design</strong></h5><p>The robot moves using<strong>two independent motors</strong> and tracks its position using<strong>two position sensors</strong>.</p><ul><li><strong>Motors</strong>: The left and right wheels have separate motors, allowing the robot to move forward, turn, or rotate in place.</li><li><strong>Position Sensors</strong>: These track how much each wheel has moved, which is used for odometry (position tracking).</li></ul><h5 id="odometry-calculation"><strong>Odometry Calculation</strong></h5><p>The robot calculates its position based on:</p><ul><li><strong>Wheel Movement</strong>: It reads the sensor values to determine how far each wheel has traveled.</li><li><strong>Position Updates</strong>: Using this data, the robot calculates its new<code>(x, y, θ)</code> coordinates.</li><li><strong>Velocity Computation</strong>: The robot computes both<strong>linear speed (v)</strong> and<strong>angular speed (w)</strong> to track its movement.</li></ul><p>The position updates continuously as the robot moves in the simulation.</p><hr><h4 id="code-explanation"><strong>Code Explanation</strong></h4><h5 id="1-odometry-calculation-tracking-the-robots-position"><strong>1. Odometry Calculation (Tracking the Robot’s Position)</strong></h5><ul><li>Reads values from the<strong>position sensors</strong> to determine how far the wheels have moved.</li><li>Converts sensor readings into<strong>distance traveled</strong>.</li><li>Updates the<strong>robot’s position and orientation</strong> using trigonometric calculations.</li><li>Continuously prints the<strong>current position (x, y, θ)</strong> of the robot.</li></ul><h5 id="2-robot-motion-control-making-the-robot-move"><strong>2. Robot Motion Control (Making the Robot Move)</strong></h5><ul><li>Sets<strong>both wheels at full speed</strong> to move forward.</li><li>Adjusts<strong>wheel speeds differently</strong> to turn:<ul><li>Turns<strong>right</strong> when needed.</li><li>Moves<strong>straight</strong> otherwise.</li></ul></li></ul><h5 id="3-square-path-movement"><strong>3. Square Path Movement</strong></h5><ul><li>The robot moves in a<strong>square pattern</strong> by following a sequence:<ul><li><strong>Moves forward for a fixed distance</strong>.</li><li><strong>Rotates 90 degrees</strong>.</li><li><strong>Repeats</strong> until a full square is completed.</li></ul></li><li>The<strong>timing of movements and turns</strong> is pre-calculated to ensure accuracy.</li></ul><hr><h4 id="installation-and-usage"><strong>Installation and Usage</strong></h4><h5 id="requirements"><strong>Requirements</strong></h5><ul><li><strong>Webots</strong>: Download and install the Webots robotics simulator from<a href="https://cyberbotics.com/" target="_blank">here</a>.</li><li><strong>Python</strong>: Ensure Python is installed for running the controller code.</li></ul><h5 id="steps-to-run"><strong>Steps to Run</strong></h5><ol><li>Clone this repository to your local machine:<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/Mummanajagadeesh/differential-drive-robot-w.git</span></span><span style="display:flex;"><span>cd differential-drive-robot-w</span></span></code></pre></div></li><li>Open Webots and load the<strong>differential_drive_robot.wbt</strong> world file in the simulation folder.</li><li>Run the simulation to observe the robot’s movement and odometry in action.</li></ol><hr><h4 id="future-enhancements"><strong>Future Enhancements</strong></h4><ul><li><strong>Path Following</strong>: Implement algorithms for following predefined paths.</li><li><strong>Advanced Sensors</strong>: Add ultrasonic sensors for obstacle detection.</li><li><strong>Improved Control</strong>: Implement PID controllers for smoother movement.</li></ul><hr>
]]></content:encoded></item><item><title/><link>https://mummanajagadeesh.github.io/projects/gpbot/lfr/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/projects/gpbot/lfr/</guid><description>&lt;![CDATA[<h2 id="line-follower-robot"><a href="https://github.com/Mummanajagadeesh/line-follower-robot-w" target="_blank">Line Follower Robot</a></h2><table><thead><tr><th><strong>Name</strong></th><th>LFRBOT</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>This project features a robot that follows a line using basic sensors. It detects the line on the ground and adjusts its movement to stay on track. The robot can navigate turns and intersections without needing complex algorithms</td></tr><tr><td><strong>Start</strong></td><td>June 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/line-follower-robot-w" target="_blank">LFRBOT🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Simulation, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Webots, Python</td></tr><tr><td><strong>Current Status</strong></td><td>On Hold</td></tr></tbody></table><hr><p>This project simulates a line-following robot using the<strong>Webots</strong> robotics simulator. The robot, based on the<strong>e-puck</strong> model, follows a black track created in<strong>Tinkercad</strong> using two IR sensors.</p>]]></description><content:encoded>&lt;![CDATA[<h2 id="line-follower-robot"><a href="https://github.com/Mummanajagadeesh/line-follower-robot-w" target="_blank">Line Follower Robot</a></h2><table><thead><tr><th><strong>Name</strong></th><th>LFRBOT</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>This project features a robot that follows a line using basic sensors. It detects the line on the ground and adjusts its movement to stay on track. The robot can navigate turns and intersections without needing complex algorithms</td></tr><tr><td><strong>Start</strong></td><td>June 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/line-follower-robot-w" target="_blank">LFRBOT🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Simulation, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Webots, Python</td></tr><tr><td><strong>Current Status</strong></td><td>On Hold</td></tr></tbody></table><hr><p>This project simulates a line-following robot using the<strong>Webots</strong> robotics simulator. The robot, based on the<strong>e-puck</strong> model, follows a black track created in<strong>Tinkercad</strong> using two IR sensors.</p><h3 id="features">Features</h3><ul><li>Simulation of a simple line-following robot using two infrared (IR) sensors placed on either side of the robot.</li><li>Black track designed in Tinkercad, exported and used in the Webots simulation.</li><li>Simple control logic based on IR sensor values to adjust the robot&rsquo;s movement.</li><li>No PID controller is used; instead, the robot makes decisions using basic conditional statements to steer left or right based on sensor readings.</li></ul><h3 id="demo-video">Demo Video</h3><p>Click the image below to watch a demo of the simulation in action:</p><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/rejXYcaX9NQ" frameborder="0" allowfullscreen=/></div></div><h3 id="how-it-works">How It Works</h3><h4 id="robot-design">Robot Design</h4><p>The robot used in this simulation is the<strong>e-puck</strong>, a simple differential drive robot with two IR sensors positioned on the left and right sides. These sensors detect the black line against the background, and based on their readings, the robot adjusts its movement.</p><ul><li><strong>Left IR Sensor (<code>ir0</code>)</strong>: Detects the black line on the left side.</li><li><strong>Right IR Sensor (<code>ir1</code>)</strong>: Detects the black line on the right side.</li><li><strong>Wheels</strong>: Two differential drive motors control the movement of the robot (left and right wheels).</li></ul><h4 id="track-design">Track Design</h4><p>The black track was created in<strong>Tinkercad</strong> and exported into the simulation environment. You can find the track mesh file in the<strong>meshes</strong> folder.</p><h4 id="control-logic">Control Logic</h4><p>The robot&rsquo;s movement is controlled by checking the values of the left and right IR sensors and adjusting the wheel velocities accordingly:</p><ul><li><strong>Straight Movement</strong>: If both sensors detect similar values, the robot moves forward.</li><li><strong>Turning</strong>:<ul><li>If the left IR sensor detects the black line (i.e., its value increases), the robot turns<strong>left</strong> by reducing the left motor&rsquo;s speed and potentially reversing it.</li><li>If the right IR sensor detects the black line, the robot turns<strong>right</strong> by reducing the right motor&rsquo;s speed.</li></ul></li></ul><p>The control logic does not involve a<strong>PID controller</strong>. Instead, basic threshold-based conditions are used to decide the robot’s steering direction.</p><h4 id="code-explanation">Code Explanation</h4><h6 id="key-points">Key Points:</h6><ul><li><strong>Timestep</strong>: The simulation steps are updated every 32ms.</li><li><strong>Max Speed</strong>: The maximum angular velocity for the motors is set to 25% of the full motor speed (6.28 rad/s).</li><li><strong>IR Sensor Values</strong>: The values of the IR sensors are used to detect the black line. A value between 6 and 15 indicates the robot is over the line, and the respective motor is slowed or reversed to turn the robot.</li><li><strong>Motor Control</strong>: The motors are set to velocity mode, and their speed is adjusted based on the sensor inputs. When one sensor detects a stronger signal, the robot turns in that direction.</li></ul><h3 id="installation-and-usage">Installation and Usage</h3><h4 id="requirements">Requirements</h4><ul><li><strong>Webots</strong>: Install the Webots robotics simulator from<a href="https://cyberbotics.com/" target="_blank">here</a>.</li><li><strong>Python</strong>: Ensure that you have Python installed to run the robot controller.</li></ul><h4 id="steps-to-run">Steps to Run</h4><ol><li>Clone this repository to your local machine:<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/Mummanajagadeesh/line-follower-robot-w.git</span></span><span style="display:flex;"><span>cd line-follower-robot-w</span></span></code></pre></div></li><li>Open Webots and load the<strong>line_follower_robot.wbt</strong> world file in the simulation folder.</li><li>Run the simulation and observe the robot following the line on the black track.</li></ol><h4 id="meshes">Meshes</h4><p>The<strong>meshes</strong> folder contains the black track design exported from<strong>Tinkercad</strong>. This is used in the Webots simulation for the robot to follow.</p><h3 id="future-enhancements">Future Enhancements</h3><ul><li><strong>PID Control</strong>: Although the current implementation uses basic threshold logic, PID control can be added for smoother and more accurate line following.</li><li><strong>Speed Optimization</strong>: The robot speed can be adjusted dynamically based on how sharply it needs to turn.</li><li><strong>Additional Sensors</strong>: Adding more IR sensors could improve the robot’s accuracy when following complex curves or intersections in the track.</li></ul>
]]></content:encoded></item><item><title/><link>https://mummanajagadeesh.github.io/projects/gpbot/obstacle/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/projects/gpbot/obstacle/</guid><description>&lt;![CDATA[<h2 id="obstacle-avoidance-robot"><a href="https://github.com/Mummanajagadeesh/obstacle-avoidance-robot-w" target="_blank">Obstacle Avoidance Robot</a></h2><table><thead><tr><th><strong>Name</strong></th><th>Obstacle Avoidance Robot</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>A robot equipped with basic sensors that detects obstacles and changes direction to avoid collisions without using advanced algorithms</td></tr><tr><td><strong>Start</strong></td><td>June 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/obstacle-avoidance-robot-w" target="_blank">OAR🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Simulation, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Webots, Python</td></tr><tr><td><strong>Current Status</strong></td><td>On Hold</td></tr></tbody></table><hr><p>This project simulates a line-following robot using the<strong>Webots</strong> robotics simulator. The robot, based on the<strong>e-puck</strong> model, follows a black track created in<strong>Tinkercad</strong> using two IR sensors.</p>]]></description><content:encoded>&lt;![CDATA[<h2 id="obstacle-avoidance-robot"><a href="https://github.com/Mummanajagadeesh/obstacle-avoidance-robot-w" target="_blank">Obstacle Avoidance Robot</a></h2><table><thead><tr><th><strong>Name</strong></th><th>Obstacle Avoidance Robot</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>A robot equipped with basic sensors that detects obstacles and changes direction to avoid collisions without using advanced algorithms</td></tr><tr><td><strong>Start</strong></td><td>June 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/obstacle-avoidance-robot-w" target="_blank">OAR🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Simulation, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Webots, Python</td></tr><tr><td><strong>Current Status</strong></td><td>On Hold</td></tr></tbody></table><hr><p>This project simulates a line-following robot using the<strong>Webots</strong> robotics simulator. The robot, based on the<strong>e-puck</strong> model, follows a black track created in<strong>Tinkercad</strong> using two IR sensors.</p><h3 id="features">Features</h3><ul><li>Simulation of a simple line-following robot using two infrared (IR) sensors placed on either side of the robot.</li><li>Black track designed in Tinkercad, exported and used in the Webots simulation.</li><li>Simple control logic based on IR sensor values to adjust the robot&rsquo;s movement.</li><li>No PID controller is used; instead, the robot makes decisions using basic conditional statements to steer left or right based on sensor readings.</li></ul><h3 id="demo-video">Demo Video</h3><p>Click the image below to watch a demo of the simulation in action:</p><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/rejXYcaX9NQ" frameborder="0" allowfullscreen=/></div></div><h3 id="how-it-works">How It Works</h3><h4 id="robot-design">Robot Design</h4><p>The robot used in this simulation is the<strong>e-puck</strong>, a simple differential drive robot with two IR sensors positioned on the left and right sides. These sensors detect the black line against the background, and based on their readings, the robot adjusts its movement.</p><ul><li><strong>Left IR Sensor (<code>ir0</code>)</strong>: Detects the black line on the left side.</li><li><strong>Right IR Sensor (<code>ir1</code>)</strong>: Detects the black line on the right side.</li><li><strong>Wheels</strong>: Two differential drive motors control the movement of the robot (left and right wheels).</li></ul><h4 id="track-design">Track Design</h4><p>The black track was created in<strong>Tinkercad</strong> and exported into the simulation environment. You can find the track mesh file in the<strong>meshes</strong> folder.</p><h4 id="control-logic">Control Logic</h4><p>The robot&rsquo;s movement is controlled by checking the values of the left and right IR sensors and adjusting the wheel velocities accordingly:</p><ul><li><strong>Straight Movement</strong>: If both sensors detect similar values, the robot moves forward.</li><li><strong>Turning</strong>:<ul><li>If the left IR sensor detects the black line (i.e., its value increases), the robot turns<strong>left</strong> by reducing the left motor&rsquo;s speed and potentially reversing it.</li><li>If the right IR sensor detects the black line, the robot turns<strong>right</strong> by reducing the right motor&rsquo;s speed.</li></ul></li></ul><p>The control logic does not involve a<strong>PID controller</strong>. Instead, basic threshold-based conditions are used to decide the robot’s steering direction.</p><h4 id="code-explanation">Code Explanation</h4><h6 id="key-points">Key Points:</h6><ul><li><strong>Timestep</strong>: The simulation steps are updated every 32ms.</li><li><strong>Max Speed</strong>: The maximum angular velocity for the motors is set to 25% of the full motor speed (6.28 rad/s).</li><li><strong>IR Sensor Values</strong>: The values of the IR sensors are used to detect the black line. A value between 6 and 15 indicates the robot is over the line, and the respective motor is slowed or reversed to turn the robot.</li><li><strong>Motor Control</strong>: The motors are set to velocity mode, and their speed is adjusted based on the sensor inputs. When one sensor detects a stronger signal, the robot turns in that direction.</li></ul><h3 id="installation-and-usage">Installation and Usage</h3><h4 id="requirements">Requirements</h4><ul><li><strong>Webots</strong>: Install the Webots robotics simulator from<a href="https://cyberbotics.com/" target="_blank">here</a>.</li><li><strong>Python</strong>: Ensure that you have Python installed to run the robot controller.</li></ul><h4 id="steps-to-run">Steps to Run</h4><ol><li>Clone this repository to your local machine:<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/Mummanajagadeesh/line-follower-robot-w.git</span></span><span style="display:flex;"><span>cd line-follower-robot-w</span></span></code></pre></div></li><li>Open Webots and load the<strong>line_follower_robot.wbt</strong> world file in the simulation folder.</li><li>Run the simulation and observe the robot following the line on the black track.</li></ol><h4 id="meshes">Meshes</h4><p>The<strong>meshes</strong> folder contains the black track design exported from<strong>Tinkercad</strong>. This is used in the Webots simulation for the robot to follow.</p><h3 id="future-enhancements">Future Enhancements</h3><ul><li><strong>PID Control</strong>: Although the current implementation uses basic threshold logic, PID control can be added for smoother and more accurate line following.</li><li><strong>Speed Optimization</strong>: The robot speed can be adjusted dynamically based on how sharply it needs to turn.</li><li><strong>Additional Sensors</strong>: Adding more IR sensors could improve the robot’s accuracy when following complex curves or intersections in the track.</li></ul>
]]></content:encoded></item><item><title/><link>https://mummanajagadeesh.github.io/projects/gpbot/wallfollow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/projects/gpbot/wallfollow/</guid><description>&lt;![CDATA[<h2 id="wall-follower-robot-simulation"><a href="https://github.com/Mummanajagadeesh/wall-follower-robot-w" target="_blank">Wall Follower Robot Simulation</a></h2><table><thead><tr><th><strong>Name</strong></th><th>Wall Follower Robot</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>The wall-following robot travels along walls in a maze. It uses basic sensors to detect the distance to the wall and adjusts its path to stay close. The robot explores all possible paths to find its destination without relying on algorithms, simply following the wall as it moves</td></tr><tr><td><strong>Start</strong></td><td>June 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/wall-follower-robot-w" target="_blank">WFR🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Simulation, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Webots, Python</td></tr><tr><td><strong>Current Status</strong></td><td>On Hold</td></tr></tbody></table><hr><p>This project contains the<strong>Wall Follower Robot</strong> simulation, which utilizes an<strong>e-puck</strong> model to navigate a maze using proximity sensors. The robot follows walls and explores paths randomly until it reaches its destination. This implementation does not use pathfinding algorithms or PID controllers; instead, it relies on basic logic to avoid obstacles and move along the walls.</p>]]></description><content:encoded>&lt;![CDATA[<h2 id="wall-follower-robot-simulation"><a href="https://github.com/Mummanajagadeesh/wall-follower-robot-w" target="_blank">Wall Follower Robot Simulation</a></h2><table><thead><tr><th><strong>Name</strong></th><th>Wall Follower Robot</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>The wall-following robot travels along walls in a maze. It uses basic sensors to detect the distance to the wall and adjusts its path to stay close. The robot explores all possible paths to find its destination without relying on algorithms, simply following the wall as it moves</td></tr><tr><td><strong>Start</strong></td><td>June 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/wall-follower-robot-w" target="_blank">WFR🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Simulation, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Webots, Python</td></tr><tr><td><strong>Current Status</strong></td><td>On Hold</td></tr></tbody></table><hr><p>This project contains the<strong>Wall Follower Robot</strong> simulation, which utilizes an<strong>e-puck</strong> model to navigate a maze using proximity sensors. The robot follows walls and explores paths randomly until it reaches its destination. This implementation does not use pathfinding algorithms or PID controllers; instead, it relies on basic logic to avoid obstacles and move along the walls.</p><h4 id="maze">Maze</h4><p>The robot navigates through a structured maze, as shown below:</p><p>[<img src="https://github.com/Mummanajagadeesh/wall-follower-robot/blob/main/maze_reference.jpg" alt="Maze">]</p><h4 id="demo-video">Demo Video</h4><p>Click the image below to watch a demo of the simulation in action:</p><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/aZBT_TVdFZY" frameborder="0" allowfullscreen=/></div></div><h4 id="how-it-works">How It Works</h4><h5 id="robot-design">Robot Design</h5><p>The<strong>e-puck</strong> robot is equipped with multiple proximity sensors positioned around its body. These sensors allow the robot to detect nearby walls and navigate through the maze by adjusting its movement. The robot makes decisions based on sensor readings to move forward or turn as needed.</p><ul><li><strong>Proximity Sensors</strong>: The robot has eight proximity sensors (<code>ps0</code> to<code>ps7</code>) that detect walls and obstacles on all sides.</li><li><strong>Motors</strong>: Independent left and right wheel motors control the robot’s movement, enabling it to move forward, turn in place, or steer based on sensor inputs.</li></ul><h5 id="maze-exploration-strategy">Maze Exploration Strategy</h5><ul><li>The robot starts at a predefined position within the maze.</li><li>It explores the maze by following walls and avoiding obstacles until it reaches the target area.</li><li>The robot does not attempt to find the shortest path but instead explores all possible routes until it reaches the destination.</li></ul><h5 id="control-logic">Control Logic</h5><ul><li>The robot detects walls using its proximity sensors and adjusts its movement accordingly.</li><li>If there is a wall directly in front, it turns right.</li><li>If there is a wall on the left but none in front, it moves forward along the wall.</li><li>If no walls are detected, the robot makes a right turn.</li><li>The robot stops when it reaches the designated target region within the maze.</li></ul><hr><h4 id="installation-and-usage">Installation and Usage</h4><h5 id="requirements">Requirements</h5><ul><li><strong>Webots</strong>: Install the Webots robotics simulator from<a href="https://cyberbotics.com/" target="_blank">here</a>.</li><li><strong>Python</strong>: Ensure Python is installed to run the robot controller.</li></ul><h5 id="steps-to-run">Steps to Run</h5><ol><li>Clone this repository to your local machine:<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/Mummanajagadeesh/wall-follower-robot-w.git</span></span><span style="display:flex;"><span>cd wall-follower-robot-w</span></span></code></pre></div></li><li>Open Webots and load the<strong>wall_follower_robot.wbt</strong> world file in the simulation folder.</li><li>Run the simulation and observe the robot navigating through the maze.</li></ol><hr><h4 id="future-enhancements">Future Enhancements</h4><ul><li><strong>Optimized Pathfinding</strong>: Implementing algorithms like DFS, BFS, or A* to find the shortest path.</li><li><strong>PID Controller</strong>: Enhancing the robot’s movement with a PID controller for smoother turns and wall-following.</li><li><strong>Increased Maze Complexity</strong>: Introducing more challenging mazes with multiple solutions and dead-ends.</li></ul><hr>
]]></content:encoded></item><item><title/><link>https://mummanajagadeesh.github.io/projects/improve/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/projects/improve/</guid><description>&lt;![CDATA[<h2 id="improve-image-processing-using-verilog"><a href="https://github.com/Mummanajagadeesh/ImProVe" target="_blank">ImProVe: IMage PROcessing using VErilog</a></h2><h5 id="do-checkout--neural-network-in-verilog">Do checkout :<strong><a href="http://mummanajagadeesh.github.io/projects/improve/never/" target="_blank">NEural NEtwork in VERilog</a></strong></h5><table><thead><tr><th><strong>Name</strong></th><th>ImProVe</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>ImProVe (Image Processing using Verilog) is a project focused on implementing image processing techniques using Verilog. It involves building image processing logic from the ground up, exploring various algorithms and approaches within HDL</td></tr><tr><td><strong>Start</strong></td><td>27 Nov 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/ImProVe" target="_blank">ImProVe🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Image Processing, HDL, Computer Vision, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Verilog, SystemVerilog, Icarus, Xilinx, Python, OpenCV</td></tr><tr><td><strong>Current Status</strong></td><td>Ongoing (Active)</td></tr><tr><td><strong>Progress</strong></td><td>- Implemented edge detection algorithms: Prewitt, Sobel, Canny, Moravec corner detection, and Emboss.<br> - Applied blurring filters: Gaussian, Median, Box, and Bilateral.<br> - Completed geometric operations: Rotation, Translation, Shearing, Cropping, Reflection, and Perspective Transform.<br> - Integrated thresholding techniques: Global Thresholding, Adaptive Thresholding, Otsu&rsquo;s Method, and Color Thresholding.<br> - Color effects: Grayscale, Sepia, Contrast, Brightness, Invert, Negative, Saturation, Gamma correction, and Sharpening.<br> - Developed subprojects: Label detection (Done), Document scanner (Ongoing), Stereo camera matching (Almost done), MNIST Digit Recognition and OCR [EMNIST] (In Working Condition).</td></tr><tr><td><strong>Next Steps</strong></td><td>- Develop a synthesizable module as a proof of concept (Almost Done)<br> - Implement morphological operations: Dilation, Closing, Opening.</td></tr></tbody></table><hr><table><thead><tr><th><strong>ImProVe Project Versions</strong></th><th><strong>Linked Projects</strong></th></tr></thead><tbody><tr><td><strong><a href="https://github.com/Mummanajagadeesh/ImProVeR" target="_blank">ImProVeR: ImProVe Revised Version</a></strong><br> Revised version with improved documentation and structure for better clarity and usability</td><td><strong><a href="https://github.com/Mummanajagadeesh/ImProVe" target="_blank">CoVer: CORDIC math modules in VERilog</a></strong><br> Replaces non-synthesizable math constructs by utilizing the CORDIC algorithm for more efficient hardware implementation</td></tr><tr><td><strong><a href="https://github.com/Mummanajagadeesh/ImProVeS" target="_blank">ImProVeS: ImProVe with Synthesizable Modules</a></strong><br> Focuses on making all modules synthesizable and aims for simulation on Xilinx Vivado</td><td><strong><a href="http://mummanajagadeesh.github.io/projects/improve/never/" target="_blank">NeVer: NEural NEtwork in VERilog</a></strong><br> Implements a neural network in Verilog for better hardware acceleration of image processing tasks</td></tr><tr><td><strong><a href="https://github.com/Mummanajagadeesh/ImProVeD" target="_blank">ImProVeD: ImProVe with Deep Learning</a></strong><br> Adds deep learning techniques to enhance image processing capabilities</td><td><strong><a href="https://github.com/Mummanajagadeesh/ProtoN" target="_blank">ProtoN: PROTOcol comparison in verilog</a></strong><br> Compares different communication protocols in Verilog for efficient high-speed data transfer, focusing on synthesizability and module communication</td></tr></tbody></table><hr><h4 id="project-overview"><strong>Project Overview</strong></h4><p>ImProVe is an initiative to implement core image processing algorithms using Verilog. It aims to achieve real-time performance for advanced applications in fields like robotics, medical imaging, and computer vision.</p>]]></description><content:encoded>&lt;![CDATA[<h2 id="improve-image-processing-using-verilog"><a href="https://github.com/Mummanajagadeesh/ImProVe" target="_blank">ImProVe: IMage PROcessing using VErilog</a></h2><h5 id="do-checkout--neural-network-in-verilog">Do checkout :<strong><a href="http://mummanajagadeesh.github.io/projects/improve/never/" target="_blank">NEural NEtwork in VERilog</a></strong></h5><table><thead><tr><th><strong>Name</strong></th><th>ImProVe</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>ImProVe (Image Processing using Verilog) is a project focused on implementing image processing techniques using Verilog. It involves building image processing logic from the ground up, exploring various algorithms and approaches within HDL</td></tr><tr><td><strong>Start</strong></td><td>27 Nov 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/ImProVe" target="_blank">ImProVe🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Image Processing, HDL, Computer Vision, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Verilog, SystemVerilog, Icarus, Xilinx, Python, OpenCV</td></tr><tr><td><strong>Current Status</strong></td><td>Ongoing (Active)</td></tr><tr><td><strong>Progress</strong></td><td>- Implemented edge detection algorithms: Prewitt, Sobel, Canny, Moravec corner detection, and Emboss.<br> - Applied blurring filters: Gaussian, Median, Box, and Bilateral.<br> - Completed geometric operations: Rotation, Translation, Shearing, Cropping, Reflection, and Perspective Transform.<br> - Integrated thresholding techniques: Global Thresholding, Adaptive Thresholding, Otsu&rsquo;s Method, and Color Thresholding.<br> - Color effects: Grayscale, Sepia, Contrast, Brightness, Invert, Negative, Saturation, Gamma correction, and Sharpening.<br> - Developed subprojects: Label detection (Done), Document scanner (Ongoing), Stereo camera matching (Almost done), MNIST Digit Recognition and OCR [EMNIST] (In Working Condition).</td></tr><tr><td><strong>Next Steps</strong></td><td>- Develop a synthesizable module as a proof of concept (Almost Done)<br> - Implement morphological operations: Dilation, Closing, Opening.</td></tr></tbody></table><hr><table><thead><tr><th><strong>ImProVe Project Versions</strong></th><th><strong>Linked Projects</strong></th></tr></thead><tbody><tr><td><strong><a href="https://github.com/Mummanajagadeesh/ImProVeR" target="_blank">ImProVeR: ImProVe Revised Version</a></strong><br> Revised version with improved documentation and structure for better clarity and usability</td><td><strong><a href="https://github.com/Mummanajagadeesh/ImProVe" target="_blank">CoVer: CORDIC math modules in VERilog</a></strong><br> Replaces non-synthesizable math constructs by utilizing the CORDIC algorithm for more efficient hardware implementation</td></tr><tr><td><strong><a href="https://github.com/Mummanajagadeesh/ImProVeS" target="_blank">ImProVeS: ImProVe with Synthesizable Modules</a></strong><br> Focuses on making all modules synthesizable and aims for simulation on Xilinx Vivado</td><td><strong><a href="http://mummanajagadeesh.github.io/projects/improve/never/" target="_blank">NeVer: NEural NEtwork in VERilog</a></strong><br> Implements a neural network in Verilog for better hardware acceleration of image processing tasks</td></tr><tr><td><strong><a href="https://github.com/Mummanajagadeesh/ImProVeD" target="_blank">ImProVeD: ImProVe with Deep Learning</a></strong><br> Adds deep learning techniques to enhance image processing capabilities</td><td><strong><a href="https://github.com/Mummanajagadeesh/ProtoN" target="_blank">ProtoN: PROTOcol comparison in verilog</a></strong><br> Compares different communication protocols in Verilog for efficient high-speed data transfer, focusing on synthesizability and module communication</td></tr></tbody></table><hr><h4 id="project-overview"><strong>Project Overview</strong></h4><p>ImProVe is an initiative to implement core image processing algorithms using Verilog. It aims to achieve real-time performance for advanced applications in fields like robotics, medical imaging, and computer vision.</p><hr><h4 id="motivation">Motivation</h4><blockquote><p>On November 26, 2024, while preparing for my Verilog elective exam, I needed to scan and rotate images of handwritten notes. That&rsquo;s when I thought:</p></blockquote><blockquote><p>&ldquo;Can image rotation be implemented using Verilog?&rdquo;</p></blockquote><p>This led me to experiment with basic transformations like rotation, cropping, translation, and shearing. As I explored further, I expanded the scope to include edge detection methods (Prewitt, Sobel, Kirsch Compass, Robinson Compass, and Canny) and noise reduction techniques (median, Gaussian, and box filters).</p><p>Initially, I wasn&rsquo;t familiar with the mathematical foundations of these techniques, so I learned them as I implemented each one. The project started as<strong>RoVer – Rotation using Verilog</strong>, focusing solely on image rotation. Over time, it evolved beyond simple transformations, leading to the creation of<strong>ImProVe – Image Processing Using Verilog</strong>.</p><h4 id="aim"><strong>Aim</strong></h4><p>The goal of this project is simple yet ambitious:<em>Build a set of foundational image processing blocks using Verilog that can be deployed on hardware for real-world applications.</em></p><p>These blocks will enable practical applications such as label detection, document scanning, object recognition, and more, making real-life automation and AI-driven vision tasks possible.</p><h6 id="the-challenge"><strong>THE Challenge</strong></h6><p><em>The project isn&rsquo;t just about implementing these techniques—it&rsquo;s about making them synthesizable for hardware. Currently, there are many simulation constructs that aren&rsquo;t FPGA-friendly. Fixing this is a big part of the journey.</em></p><hr><h4 id="features"><strong>Features</strong></h4><p>ImProVe supports a wide array of image processing functionalities categorized into multiple domains:</p><h6 id="edge-detection-and-feature-extraction"><strong><a href="https://github.com/Mummanajagadeesh/ImProVe/tree/main/1" target="_blank">Edge Detection and Feature Extraction</a></strong></h6><ul><li><strong>Sobel Operator</strong>: Detects edges by computing gradients in horizontal and vertical directions</li><li><strong>Prewitt Operator</strong>: Similar to Sobel but uses different kernel values</li><li><strong>Roberts Cross Operator</strong>: Uses a 2x2 kernel for edge detection</li><li><strong>Robinson Compass Operator</strong>: Uses eight directional masks to detect edges with specific orientation sensitivity</li><li><strong>Kirsch Compass Operator</strong>: Detects edges by applying a set of 3x3 masks to enhance edges in various directions</li><li><strong>Laplacian Operator</strong>: Captures edges by computing the second derivative of the image, highlighting regions of rapid intensity change</li><li><strong>Laplacian of Gaussian (LoG)</strong>: Combines edge detection with noise reduction</li><li><strong>Canny Edge Detection</strong>: Advanced edge detection using gradients, non-maximum suppression, and thresholding</li><li><strong>Emboss Filter</strong>: Applies a convolution kernel to highlight edges and create a 3D relief effect</li><li><strong>Moravec Corner Detection</strong>: Detects corners by evaluating changes in intensity in various directions using a 3x3 window</li></ul><h6 id="noise-reduction-and-smoothing"><strong><a href="https://github.com/Mummanajagadeesh/ImProVe/tree/main/2" target="_blank">Noise Reduction and Smoothing</a></strong></h6><ul><li><strong>Gaussian Blur</strong>: Smoothens and reduces noise using a Gaussian kernel</li><li><strong>Median Filter</strong>: Replaces each pixel with the median of its neighborhood to remove Noise</li><li><strong>Box Filter (Mean Filter)</strong>: Averages pixel values within a window for Smoothing</li><li><strong>Bilateral Filter</strong>: Preserves edges while reducing noise by combining spatial and intensity information</li></ul><h6 id="thresholding-and-binarization"><strong><a href="https://github.com/Mummanajagadeesh/ImProVe/tree/main/3" target="_blank">Thresholding and Binarization</a></strong></h6><ul><li><strong>Global Thresholding</strong>: Converts grayscale images to binary using a fixed thresholding</li><li><strong>Adaptive Thresholding</strong>: Dynamically computes thresholds based on local Intensity</li><li><strong>Otsu&rsquo;s Method</strong>: Automatically finds the optimal threshold for binarization</li><li><strong>Color Thresholding</strong>: Applies thresholding on color spaces (e g , RGB, HSV, LAB) to segment specific color ranges</li></ul><h6 id="eometric-transformations"><strong><a href="https://github.com/Mummanajagadeesh/ImProVe/tree/main/6" target="_blank">eometric Transformations</a></strong></h6><ul><li><strong>Rotation</strong>: Rotates the image by a given angle</li><li><strong>Scaling</strong>: Resizes the image while preserving aspect ratio</li><li><strong>Translation</strong>: Shifts the image position</li><li><strong>Shearing</strong>: Distorts the image by shifting rows or columns</li><li><strong>Cropping</strong>: Extracts a rectangular region from the image</li><li><strong>Reflection</strong>: Flips the image across a specified axis (horizontal, vertical, or diagonal)</li><li><strong>3D Perspective Transformation</strong>: Applies a projective transformation that distorts the image to simulate depth</li></ul><h6 id="color-and-intensity-transformations"><strong><a href="https://github.com/Mummanajagadeesh/ImProVe/tree/main/9" target="_blank">Color and Intensity Transformations</a></strong></h6><ul><li><strong>Negative Transformation</strong>: Converts an image to its negative by replacing each pixel value ( R ) with ( 255 - R )</li><li><strong>Inversion</strong>: Converts an image to its negative by inverting pixel values</li><li><strong>Sepia Effect</strong>: Applies a warm brown tone to an image for a vintage look</li><li><strong>Brightness Adjustment</strong>: Modifies image brightness by increasing or decreasing pixel intensity</li><li><strong>Gamma Correction</strong>: Adjusts brightness using a gamma function</li><li><strong>Saturation Adjustment</strong>: Enhances or reduces color intensity using a scaling factor</li><li><strong>Sharpness Enhancement</strong>: Increases edge contrast to make the image appear clearer</li></ul><hr><h4 id="todo">TODO</h4><ul><li>Ensure the code is synthesizable by removing all non-synthesizable constructs.</li><li>Use CORDIC to eliminate functions like<code>$cos</code>,<code>$sin</code>,<code>$exp</code>,<code>$sqrt</code>, and other complex mathematical operations.</li><li>For constructs related to file I/O and read/write operations, use BRAM or RAM memory to store the input image in pixels.</li><li>The output image data for all three channels should be connected to a VGA display via a communication bus like PCIe or AXI.</li><li>Optimize the solution for performance and resource efficiency.</li></ul><p><strong>Currently working on CORDIC:</strong><a href="https://github.com/Mummanajagadeesh/cordic-algorithm-verilog" target="_blank">CORDIC Algorithm in Verilog</a></p><p><strong>Currently working on OCR using Verilog as well, building a neural network from scratch for MNIST number recognition</strong></p><hr><h4 id="tools-and-technologies"><strong>Tools and Technologies</strong></h4><ul><li><strong>Icarus Verilog 12 0</strong>: Core HDL used to implement all image processing algorithms</li><li><strong>Python 3 12 1</strong>: For preprocessing image data into a format compatible with verilog</li></ul><hr><h2 id="applications-irl"><a href="https://github.com/Mummanajagadeesh/ImProVe/tree/main/Applications-IRL" target="_blank">Applications IRL</a></h2><h4 id="label-detection"><strong>Label-Detection</strong></h4><ul><li><p>Label detection was implemented using the<strong>Prewitt operator</strong> for edge detection. The process begins by splitting the image into three separate RGB channels using Python. Next, the<strong>luminance formula (NTSC)</strong> is applied to obtain a grayscale image.</p></li><li><p>If the image is too noisy or requires preprocessing,<strong>Gaussian blur</strong> and<strong>morphological operations</strong> are applied to refine the grayscale image before edge detection.</p></li><li><p>Edge detection algorithms are then used to identify strong edges. From these edges, the<strong>flood-fill algorithm</strong> is applied to detect the<strong>largest possible contour</strong>. A bounding rectangle is drawn in the red channel to enclose this contour, which is then superimposed on the original image for clear label detection.</p></li><li><p>This entire process is executed in Verilog using text files for data handling. Python is then used for visualization to generate the final output.</p></li></ul><h6 id="heading"><a href="images/projects/improve/label-detection/">*For more details</a></h6><table><thead><tr><th><strong>Original Image</strong></th><th><strong>After Vertical Prewitt</strong></th><th><strong>After Horizontal Prewitt</strong></th><th><strong>After Full Prewitt</strong></th></tr></thead><tbody><tr><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="300" src="/images/projects/improve/label-detection/ocr_test_1_hu_3d99269ca3515b86.webp" alt="Original Image 1" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/label-detection\/ocr_test_1_hu_9b199f12722c693b.jpeg'"/><script>
window.addEventListener("load", (e) => {
const lightbox = GLightbox();
});</script></td><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="300" src="/images/projects/improve/label-detection/ver_hu_bc7673c1dec43754.webp" alt="After Vertical Prewitt" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/label-detection\/ver_hu_989ec0ee51159b75.jpg'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="300" src="/images/projects/improve/label-detection/hor_hu_cf56ed43bf58879.webp" alt="After Horizontal Prewitt" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/label-detection\/hor_hu_1c3c5d416ed815a2.jpg'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="300" src="/images/projects/improve/label-detection/sumapp_hu_f70617466753fb1d.webp" alt="After Full Prewitt" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/label-detection\/sumapp_hu_b164130afece14ce.jpg'"/></td></tr></tbody></table><table><thead><tr><th><strong>Original Image</strong></th><th><strong>After Full Prewitt</strong></th><th><strong>Binary Box</strong></th><th><strong>Overlayed Image with Box</strong></th></tr></thead><tbody><tr><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="300" src="/images/projects/improve/label-detection/ocr_test_1_hu_3d99269ca3515b86.webp" alt="Original Image 1" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/label-detection\/ocr_test_1_hu_9b199f12722c693b.jpeg'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="300" src="/images/projects/improve/label-detection/sumapp_hu_f70617466753fb1d.webp" alt="After Full Prewitt" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/label-detection\/sumapp_hu_b164130afece14ce.jpg'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="300" src="/images/projects/improve/label-detection/output_image_hu_da2dd020563418ec.webp" alt="Binary Box" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/label-detection\/output_image_hu_9e5e6e98a32dea7f.jpg'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="300" src="/images/projects/improve/label-detection/output_image_with_box_hu_d591735390127c72.webp" alt="Overlayed Image with Box" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/label-detection\/output_image_with_box_hu_66a8d5cd456a7d92.jpg'"/></td></tr></tbody></table><table><thead><tr><th><strong>Original Image</strong></th><th><strong>After Full Prewitt</strong></th><th><strong>Binary Box</strong></th><th><strong>Overlayed Image with Box</strong></th></tr></thead><tbody><tr><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="300" src="/images/projects/improve/label-detection/ocr_test_2_hu_cb51bdc0cb345a30.webp" alt="Original Image 2" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/label-detection\/ocr_test_2_hu_7f922f3f606bae8c.jpg'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="300" src="/images/projects/improve/label-detection/prewcomb2_hu_97a16e9cb54d00c1.webp" alt="After Full Prewitt" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/label-detection\/prewcomb2_hu_6d8e904b5fedb83c.jpg'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="300" src="/images/projects/improve/label-detection/binarybox2_hu_d005e976c96656a2.webp" alt="Binary Box" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/label-detection\/binarybox2_hu_9b56ae83e99083fa.jpg'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="300" src="/images/projects/improve/label-detection/boxedlabel2_hu_6e330dfbabc56069.webp" alt="Overlayed Image with Box" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/label-detection\/boxedlabel2_hu_eef6aae6e4f4ccbb.jpg'"/></td></tr></tbody></table><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/IXgA1Ih9BAo" frameborder="0" allowfullscreen=/></div></div><h4 id="document-scanner"><strong>Document Scanner</strong></h4><ul><li><p>The initial steps of the document scanning process are similar to label detection, with<strong>Canny edge detection</strong> being the preferred method for stronger edge identification.</p></li><li><p>After detecting edges, instead of directly superimposing a bounding rectangle, the<strong>boundary-fill algorithm</strong> is used to fill the detected region with binary 1s (255). A<strong>Boolean operation</strong> is then applied to remove irrelevant parts of the image that do not contain the document.</p></li><li><p>Next, corner detection is performed using the<strong>Moravec operator</strong> (as<strong>Harris</strong> is computationally expensive). Once the corners are identified,<strong>Bresenham&rsquo;s algorithm</strong> is used to draw lines connecting them, forming a quadrilateral around the document.</p></li><li><p>This quadrilateral is then mapped to a rectangle using<strong>homogeneous perspective transformation</strong>, followed by<strong>shearing</strong> and<strong>scaling</strong> operations if needed to refine the final output.</p></li></ul><p>OTW [Stuck at Bresenham&rsquo;s algorithm implementation]</p><h4 id="stereo-vision"><strong>Stereo Vision</strong></h4><ul><li><p>The process begins by converting the input stereo image pair from RGB to grayscale. Using the camera parameters, a<strong>disparity map</strong> is computed to determine the pixel shifts between the left and right images. This disparity information is then used to generate a<strong>depth map</strong>, which represents the distance of objects in the scene.</p></li><li><p>Both the disparity and depth map calculations are implemented entirely in Verilog, using text files for intermediate data storage and processing. Once the depth information is obtained, Python is used to continue the 3D reconstruction process, where the depth map is converted into a point cloud or a<strong>3D mesh representation</strong> for visualization.</p></li></ul><p>OTW [Accuracy of disparity and depth maps is low]</p><h4 id="mnist-digit-detection-"><strong>MNIST Digit Detection [NeVer ∩ ImProVeD]</strong></h4><ul><li><p>I implemented a simple neural network from scratch in<strong>Google Colab</strong> without using<strong>TensorFlow</strong> or<strong>Keras</strong>, relying solely on<strong>NumPy</strong> for numerical computations,<strong>Pandas</strong> for data handling, and<strong>Matplotlib</strong> for visualization. The dataset used was<strong>sample_data/mnist_train_small.csv</strong>, containing handwritten digit images in a flattened<strong>784-pixel format</strong>. Data preprocessing involved<strong>normalizing pixel values</strong> (dividing by<strong>255</strong>) and splitting it into a<strong>training set</strong> and a<strong>development set</strong>, with the first<strong>1000 samples</strong> reserved for development and the rest for training. Labels (digits 0-9) were stored separately, and data was shuffled before training to ensure randomness.</p></li><li><p>The neural network consists of an<strong>input layer (784 neurons)</strong>, a<strong>hidden layer (128 neurons, ReLU activation)</strong>, and an<strong>output layer (10 neurons, softmax activation)</strong>. Model parameters (weights and biases) were initialized randomly and updated using<strong>gradient descent</strong> over<strong>500 iterations</strong> with a learning rate of<strong>0.1</strong>. Training involved<strong>forward propagation</strong> to compute activations and<strong>backpropagation</strong> to update parameters. Accuracy was printed every<strong>10 iterations</strong>. To make the trained model compatible with<strong>Verilog</strong>, weights and biases were<strong>scaled by 10,000</strong> and saved as<strong>integer values</strong> in text files (<code>W1.txt</code>,<code>b1.txt</code>, etc.), preventing<strong>floating-point multiplication</strong> in hardware. These parameters were later reloaded for predictions on new images, verifying model accuracy on the<strong>development set</strong> before deployment in Verilog for real-time digit classification.</p></li><li><p>The model, trained on<strong>sample_data/mnist_train_small.csv</strong>, achieved over<strong>90% accuracy</strong>. It generates<code>W1</code>,<code>W2</code>,<code>b1</code>, and<code>b2</code> text files containing shape information. The trained parameters are used in Verilog to predict digits from an input image stored in<code>input_vector.txt</code>, which consists of<strong>784 space-separated integers</strong>. The predicted output is displayed in the terminal using<code>$display</code>. The original CSV file was converted into a space-separated text format where each line contains a digit followed by<strong>784 pixel values (785 total)</strong>. During prediction, the first value (label) is removed to test if the model correctly classifies the input image.</p></li><li><p>The<strong>Verilog module</strong> implements a neural network to classify handwritten digits from the MNIST dataset. It comprises an<strong>input layer (784 neurons), hidden layer (128 neurons), and output layer (10 neurons)</strong>. The module reads<strong>pre-trained weights and biases</strong> from text files (<code>W1.txt</code>,<code>b1.txt</code>,<code>W2.txt</code>,<code>b2.txt</code>) along with an<strong>input vector</strong> from<code>input_vector.txt</code>. Input values are normalized by dividing by<strong>255.0</strong>, while weights and biases are scaled using a<strong>factor of 10,000</strong>. The hidden layer performs a fully connected transformation (<code>W1 * input + b1</code>) followed by<strong>ReLU activation</strong>. The output layer computes another weighted sum (<code>W2 * hidden + b2</code>) but does not apply softmax; instead, the predicted digit is determined by selecting the index of the highest output value.</p></li><li><p>The file reading process ensures proper loading of weights, biases, and input values before computation begins. Forward propagation occurs sequentially, with an initial delay for data loading. After computing activations in both layers, the module iterates through the output layer to identify the highest value, representing the predicted class. The classification result is then displayed. This hardware implementation streamlines neural network inference by<strong>eliminating complex activation functions</strong> like softmax while preserving classification accuracy through maximum output activation.</p></li><li><p>In newer versions of the code, the text files are converted into<strong>synthesizable memory blocks</strong> using Python scripts. These scripts store<strong>weights and biases</strong> in register modules, which are finally instantiated in the<strong>top-level module</strong>. The image data is also handled in a similar way.</p></li><li><p>Only the<code>$display</code> statement,<code>$finish</code>, and the<code>real</code> datatype in the final top-level module are non-synthesizable constructs. These can be eliminated by directing the predicted output to a seven-segment display using case statements [Moved these constructs to the testbench in later versions for a cleaner top module; currently working with<code>Q24.8</code> as a replacement for the real datatype].</p></li><li><p>I am currently working on replacing the<strong>training process</strong> with a Verilog-based implementation, aiming for a fully synthesizable neural network.</p></li></ul><blockquote><p>This approach can pave a new way for POC, where text files are converted into register modules using Python scripts to automate the process. A top-level module can then connect all the generated modules, and the testbench can include $fopen, etc., to write the output text files. The testbench instantiates the top-level module, ensuring that all files are synthesizable.</p></blockquote><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/QtkdWSq25zQ" frameborder="0" allowfullscreen=/></div></div><h4 id="ocr-optical-character-recognition"><strong>OCR: Optical Character Recognition</strong></h4><p>This model is trained on the<strong>EMNIST ByClass dataset</strong> (<a href="https://greg-cohen.com/datasets/emnist/" target="_blank">source</a>), which contains<strong>62 classes</strong> (digits<code>0-9</code>, uppercase letters<code>A-Z</code>, and lowercase letters<code>a-z</code>). The dataset is preprocessed by converting it into a CSV format, normalizing pixel values, reducing dimensions, and shuffling before training.</p><p>The neural network consists of multiple layers:</p><ul><li>Input layer:<strong>784 neurons</strong></li><li>First hidden layer:<strong>256 neurons</strong> (<code>W1: 256×784</code>,<code>b1: 256×1</code>)</li><li>Second hidden layer:<strong>128 neurons</strong> (<code>W2: 128×256</code>,<code>b2: 128×1</code>)</li><li>Output layer:<strong>62 neurons</strong> (<code>W3: 62×128</code>,<code>b3: 62×1</code>)</li></ul><p>Training is done using forward propagation, computing activations at each layer using matrix multiplications and ReLU activations for hidden layers. Backpropagation is used to update weights using the gradient of the loss function. The dataset is shuffled to improve generalization, and weights (<code>W1</code>,<code>W2</code>,<code>W3</code>) and biases (<code>b1</code>,<code>b2</code>,<code>b3</code>) are updated iteratively until convergence. The model is trained over multiple epochs using stochastic gradient descent (SGD) and Adam&rsquo;s Optimiser.</p><p>To make the trained model compatible with hardware, weights and biases are<strong>scaled by 10,000</strong> and stored as integers in text files (<code>W1.txt</code>,<code>b1.txt</code>, etc.), since Verilog does not support floating-point arithmetic.</p><p>Inference in Verilog follows a similar process but supports extra layers and classes. Input images are read from<code>input_vector.txt</code> and normalized. Weights and biases are loaded from text files. The computation follows:</p><ul><li><code>hidden1 = ReLU(W1 * input + b1)</code></li><li><code>hidden2 = ReLU(W2 * hidden1 + b2)</code></li><li><code>output = W3 * hidden2 + b3</code></li></ul><p>The index of the<strong>maximum output value</strong> determines the predicted character, which is mapped to<code>"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"</code> and displayed using<code>$display</code>.</p><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/7YccFUtydM0" frameborder="0" allowfullscreen=/></div></div><p>The video demonstrates that<code>draw.py</code> allows us to draw anything on a square canvas. At the end, it applies grayscale, inverts the image, compresses it to a 28×28 resolution, and saves it as<code>drawing.jpg</code>. Then,<code>img2bin.py</code> converts this image into a 2D array of pixels (a 28×28 matrix) and saves it in<code>mnist_single_no.txt</code>.</p><p>Next,<code>arr2row.py</code> flattens the 2D array into a 1D array (a row of 784 values) and stores it in<code>input_vector.txt</code>. This file is then used to create<code>image_memory.v</code> with<code>memloader_from_inp_vec.py</code>, which generates a memory module for storing the image.</p><p>Following this,<code>wtbs_loader.py</code> creates six different memory modules from<code>W1</code>,<code>W2</code>,<code>W3</code>,<code>b1</code>,<code>b2</code>, and<code>b3</code> text files, generating corresponding files such as<code>W1_memory.v</code>, and so on.</p><p>All these components are instantiated in the top module<code>emnist_with_tb.v</code>, along with a testbench (<code>emnist_nn_tb.v</code>). This setup ultimately predicts the drawn character. In the demo, I showcased the characters &ldquo;H,&rdquo; &ldquo;f,&rdquo; and &ldquo;7&rdquo;—each representing a different subclass from the 62 available classes (uppercase letters, lowercase letters, and numbers).</p><blockquote><p>Additionally, I developed a coarse-grained pipelined fully connected neural network using Finite State Machine (FSM) and integrated Softmax with a Taylor series approximation to improve computational efficiency</p></blockquote><hr><hr><h4 id="important-links-and-resources"><strong>Important Links and Resources</strong></h4><h6 id="digital-image-processing"><strong>Digital Image Processing</strong></h6><ul><li><a href="https://www.geeksforgeeks.org/digital-image-processing-tutorial" target="_blank"><strong>GeeksforGeeks: Digital Image Processing Tutorial</strong></a></li><li><a href="https://youtu.be/KuXjwB4LzSA" target="_blank"><strong>YouTube: Digital Image Processing Introduction</strong></a></li><li><a href="https://www.youtube.com/live/8rrHTtUzyZA" target="_blank"><strong>YouTube Live: Advanced Digital Image Processing Concepts</strong></a></li></ul><h6 id="mathematics-for-engineering-and-computing"><strong>Mathematics for Engineering and Computing</strong></h6><ul><li><a href="https://youtu.be/w8yWXqWQYmU" target="_blank"><strong>YouTube: Building a neural network FROM SCRATCH</strong></a></li><li><a href="https://youtu.be/cAkMcPfY_Ns" target="_blank"><strong>YouTube: I Built a Neural Network from Scratch</strong></a></li><li><a href="https://youtu.be/cAkMcPfY_Ns" target="_blank"><strong>YouTube: Linear Algebra – Essence of Linear Algebra (Playlist)</strong></a></li></ul><h6 id="verilog"><strong>Verilog</strong></h6><ul><li><a href="http://www.asic.co.in/Index_files/verilog_files/File_IO.htm" target="_blank"><strong>Guide to Verilog File I/O and File Handling</strong></a></li><li><a href="https://steveicarus.github.io/iverilog/" target="_blank"><strong>Official Icarus Verilog Documentation</strong></a></li><li><a href="https://www.youtube.com/playlist?list=PLJ5C_6qdAvBELELTSPgzYkQg3HgclQh-5" target="_blank"><strong>NPTEL Lectures</strong></a></li></ul><h6 id="cordic-algorithm-resources"><strong>CORDIC Algorithm Resources</strong></h6><ul><li><a href="https://ieeexplore.ieee.org/document/6808249" target="_blank"><strong>IEEE Xplore: Hardware Implementation of a Math Module Based on the CORDIC Algorithm Using FPGA</strong></a></li><li><a href="http://ethesis.nitrkl.ac.in/4258/1/CORDIC_Algorithm_and_it%E2%80%99s_Applications_in_DSP.pdf" target="_blank"><strong>CORDIC Algorithm and Its Applications in DSP (NITR Thesis)</strong></a></li><li><a href="https://www.eit.lth.se/fileadmin/eit/courses/eitf35/2017/CORDIC_For_Dummies.pdf" target="_blank"><strong>CORDIC for Dummies (Introductory Guide)</strong></a></li><li><a href="https://www.st.com/resource/en/application_note/an5325-how-to-use-the-cordic-to-perform-mathematical-functions-on-stm32-mcus-stmicroelectronics.pdf" target="_blank"><strong>STMicroelectronics: Using the CORDIC for Mathematical Functions on STM32 MCUs</strong></a></li><li><a href="https://projectf.io/posts/square-root-in-verilog/" target="_blank"><strong>Square Root Calculation Using CORDIC In System Verilog</strong></a></li></ul><h6 id="datasets"><strong>Datasets</strong></h6><ul><li><a href="https://www.kaggle.com/datasets/hojjatk/mnist-dataset" target="_blank"><strong>MNIST Dataset: 0-9 Handwritten Numbers</strong></a></li><li><a href="https://greg-cohen.com/datasets/emnist/" target="_blank"><strong>EMNIST Dataset: Extended MNIST with Alphabet Support</strong></a></li><li><a href="https://www.kaggle.com/datasets/preatcher/standard-ocr-dataset" target="_blank"><strong>Standard OCR Dataset: Various Images of Characters in Different Fonts</strong></a></li></ul><h4 id="contributors"><strong>Contributors</strong></h4><ul><li><strong><a href="https://github.com/Mummanajagadeesh" target="_blank">Jagadeesh</a></strong> mummanajagadeesh97@gmail com</li></ul><p>Feel free to contribute by submitting pull requests or feature suggestions!</p><p>If interested in working together, do drop a DM or mail 🙂</p><hr><h6 id="note">NOTE</h6><blockquote><p><strong>Missing Images</strong>:</p><ul><li>Some of the images may be missing due to unforeseen issues. If you notice any missing images, please inform me</li></ul><p><strong>Code Structure</strong>:</p><ul><li>Not all code snippets follow the same structural order. This is intentional, as some parts are specifically designed to handle their unique mathematical requirements
Priority was given to making each individual piece of code functional rather than ensuring the overall scalability of the project</li></ul><p><strong>AI Assistance</strong>:</p><ul><li>Fair use of AI (e.g., ChatGPT) was employed for syntax suggestions and debugging. Kudos to ChatGPT for its support!</li></ul><p><strong>Math Adaptations</strong>:</p><ul><li>Not every piece of code adheres strictly to its intended mathematical model. Verilog&rsquo;s limitations in computational math have necessitated ample adjustments, with liberties taken to ensure functionality.</li></ul><p><strong>Feedback &amp; Help</strong>:</p><ul><li>Please let me know if you have any suggestions or tips.</li><li>I am in desperate need of help and would greatly appreciate any assistance or advice.</li></ul><p>Thank you for your understanding and support!</p></blockquote><hr><h3 id="selected-image-processing-results">Selected Image Processing Results</h3><p>Below are some of the best results from my image processing work. While there are many more images, including all of them here without relevant explanations wouldn&rsquo;t be meaningful. For a detailed breakdown of the implementation and the mathematical concepts behind each operation, please refer to my repository.</p><h4 id="edge-detection--prewitt-operator">Edge Detection – Prewitt Operator</h4><img title="" loading="lazy" decoding="async" class="img  " width="900" height="174" src="/images/projects/improve/prew_hu_6819bbf47c20e97a.webp" alt="Edge Detection using Prewitt Operator" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/prew_hu_cae11774a5f3ae05.png'"/><h4 id="corner-detection---moravec">Corner Detection - Moravec</h4><img title="" loading="lazy" decoding="async" class="img  " width="900" height="173" src="/images/projects/improve/moravec_hu_f87122e5e51923d4.webp" alt="Corner Detection using Moravec Operator" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/moravec_hu_e23534662717e9e1.png'"/><h4 id="noise-reduction--gaussian-blur">Noise Reduction – Gaussian Blur</h4><img title="" loading="lazy" decoding="async" class="img  " width="900" height="410" src="/images/projects/improve/blur_hu_1309aa6e0f3913f6.webp" alt="Gaussian Blur for Noise Reduction" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/blur_hu_11467b2c1d9e16c.png'"/><h4 id="thresholding--otsus-method">Thresholding – Otsu&rsquo;s Method</h4><p><img title="" loading="lazy" decoding="async" class="img  " width="900" height="492" src="/images/projects/improve/otsu_hu_e8d87d482096e449.webp" alt="Otsu Thresholding" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/otsu_hu_d5e2155ee159c25e.png'"/><br><img title="" loading="lazy" decoding="async" class="img  " width="900" height="308" src="/images/projects/improve/otsu-hist_hu_d4b953f0a4367395.webp" alt="Otsu Thresholding Histogram" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/otsu-hist_hu_83502c73133ae458.png'"/></p><h4 id="geometric-transformations">Geometric Transformations</h4><ul><li><strong>Rotation with Same Dimensions</strong><br><img title="" loading="lazy" decoding="async" class="img  " width="900" height="201" src="/images/projects/improve/rotcut_hu_aca60eebd5524c6.webp" alt="Rotation with Same Dimensions" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/rotcut_hu_944508777e6bb54b.png'"/></li><li><strong>Rotation with Diagonal Dimensions</strong><br><img title="" loading="lazy" decoding="async" class="img  " width="900" height="386" src="/images/projects/improve/rotcutfull_hu_d2026ddce75f2726.webp" alt="Rotation with Diagonal Dimensions" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/rotcutfull_hu_a41f92946d782f1b.png'"/></li><li><strong>Scaling</strong><br><img title="" loading="lazy" decoding="async" class="img  " width="900" height="323" src="/images/projects/improve/scale_hu_14a74c180907bf52.webp" alt="Image Scaling" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/scale_hu_3ac3ff8f2cb6e63e.png'"/></li><li><strong>Translation</strong><br><img title="" loading="lazy" decoding="async" class="img  " width="900" height="326" src="/images/projects/improve/trans_hu_87c6b2bb6fa514d.webp" alt="Image Translation" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/trans_hu_9b446bb953610e3a.png'"/></li><li><strong>Shearing</strong><br><img title="" loading="lazy" decoding="async" class="img  " width="900" height="288" src="/images/projects/improve/shear_hu_ffea19883dde3029.webp" alt="Image Shearing" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/shear_hu_a86e35bec854dc82.png'"/></li><li><strong>Cropping</strong><br><img title="" loading="lazy" decoding="async" class="img  " width="900" height="322" src="/images/projects/improve/crop_hu_7e217a24de298805.webp" alt="Image Cropping" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/crop_hu_70d254fb8ee93d08.png'"/></li><li><strong>Reflection (Both Axes)</strong><br><img title="" loading="lazy" decoding="async" class="img  " width="900" height="328" src="/images/projects/improve/reflect_hu_a8d5f4c4ef3ef3fb.webp" alt="Reflection Across Both Axes" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/reflect_hu_5f67e42a8b272f5d.png'"/></li><li><strong>3D Homogeneous Perspective Transformation</strong><img title="" loading="lazy" decoding="async" class="img  " width="900" height="303" src="/images/projects/improve/perspective_hu_3332bb0ad7ec32a2.webp" alt="Homogeneous Perspective Transformation" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/perspective_hu_586dabdfa07e9bfe.png'"/></li></ul><h4 id="color-and-intensity-transformations-1">Color and Intensity Transformations</h4><ul><li><strong>Gamma Correction</strong><br><img title="" loading="lazy" decoding="async" class="img  " width="900" height="251" src="/images/projects/improve/gamma_hu_a6a9e4149fd3a735.webp" alt="Gamma Correction" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/gamma_hu_3685eff1ab3f89b1.png'"/></li><li><strong>Image Inversion</strong><br><img title="" loading="lazy" decoding="async" class="img  " width="900" height="387" src="/images/projects/improve/invert_hu_3b71e33861036406.webp" alt="Image Inversion" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/invert_hu_5bf2626c995ff548.png'"/></li><li><strong>Sepia Effect</strong><br><img title="" loading="lazy" decoding="async" class="img  " width="900" height="364" src="/images/projects/improve/sepia_hu_ad06a396fb2ae96.webp" alt="Sepia Effect" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/sepia_hu_4264c6d1f954818a.png'"/></li><li><strong>Negative Transformation</strong><br><img title="" loading="lazy" decoding="async" class="img  " width="900" height="381" src="/images/projects/improve/negative_hu_efef42a6f014d32c.webp" alt="Negative Transformation" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/negative_hu_a67e8287eec9554e.png'"/></li><li><strong>Grayscale Conversion</strong><br><img title="" loading="lazy" decoding="async" class="img  " width="900" height="374" src="/images/projects/improve/gray_hu_e4774ae21064c74a.webp" alt="Grayscale Conversion" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/gray_hu_1f70bb264d0ea3d7.png'"/></li><li><strong>Contrast Adjustment</strong><br><img title="" loading="lazy" decoding="async" class="img  " width="900" height="203" src="/images/projects/improve/contrast_hu_78a90758f4b7d2da.webp" alt="Contrast Adjustment" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/contrast_hu_b05a3a383315312b.png'"/></li><li><strong>Brightness Adjustment</strong><br><img title="" loading="lazy" decoding="async" class="img  " width="900" height="373" src="/images/projects/improve/bright_hu_8e4e1f917b69bceb.webp" alt="Brightness Adjustment" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/bright_hu_e1a87e89f44ae95c.png'"/></li><li><strong>Saturation Adjustment</strong><br><img title="" loading="lazy" decoding="async" class="img  " width="900" height="221" src="/images/projects/improve/saturation_hu_bdc54ed6809ca8a0.webp" alt="Saturation Adjustment" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/saturation_hu_b7d5a85b15f763cd.png'"/></li><li><strong>Sharpness Enhancement</strong><br><img title="" loading="lazy" decoding="async" class="img  " width="900" height="201" src="/images/projects/improve/sharpness_hu_a38362eacb39367a.webp" alt="Sharpness Enhancement" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/sharpness_hu_7659c96d732db161.png'"/></li></ul><p>For more insights into the implementation, visit my repository, where I provide a comprehensive explanation of the mathematical foundations behind each operation.</p><hr><h3 id="proof-of-concept-proposal-fpga-implementation-of-improve"><strong>Proof of Concept Proposal: FPGA Implementation of ImProVe</strong></h3><p>The goal of this proof of concept is to transition<strong>ImProVe</strong> from a Verilog-based image processing simulation to a fully FPGA-compatible implementation. The current approach, while functional, relies heavily on software-based file I/O, lacks real-time processing capabilities, and does not take advantage of hardware acceleration. This proposal outlines a new FPGA-based workflow that optimizes performance, enables parallel processing, and integrates efficient mathematical computation techniques like<strong>CORDIC for trigonometric and square root calculations</strong>.</p><h4 id="current-workflow-and-its-limitations"><strong>Current Workflow and Its Limitations</strong></h4><p>The existing implementation is purely<strong>simulation-based</strong>, using<strong>Icarus Verilog</strong> without testbenches. Images are pre-processed using<strong>Python (NumPy)</strong>, converted into<strong>.hex or .txt</strong> files, and then read into Verilog through file I/O functions. After processing, the results are stored in a corresponding output file, converted back into an image, and visually verified or cross-checked with a reference Python implementation.</p><p>While this approach enables functional validation, it suffers from several limitations:</p><ul><li><strong>File I/O is slow</strong> and does not reflect real-world FPGA-based image processing.</li><li><strong>No hardware optimizations</strong>, making it unsuitable for real-time applications.</li><li><strong>No pipelining or parallelism</strong>, leading to inefficient processing for large images.</li><li><strong>Mathematical operations</strong> (like square root and trigonometric functions) rely on direct computation rather than optimized hardware-friendly methods.</li></ul><h4 id="proposed-fpga-based-workflow"><strong>Proposed FPGA-Based Workflow</strong></h4><p>The<strong>FPGA implementation</strong> will be designed to replace file-based image processing with a<strong>high-speed AXI-based pipeline</strong> that processes images in real time. Key improvements include:</p><h5 id="cordic-for-mathematical-computation"><strong>CORDIC for Mathematical Computation</strong></h5><p>Operations like<strong>image rotation, edge detection, and geometric transformations</strong> require trigonometric functions (sin, cos) and square root calculations. Instead of using costly multipliers or look-up tables, these will be implemented using<strong>CORDIC (COordinate Rotation DIgital Computer)</strong>, which efficiently computes trigonometric functions, logarithms, and square roots in hardware without requiring floating-point arithmetic.</p><h5 id="memory-optimization-ddr-for-image-storage-bram-for-intermediate-buffers"><strong>Memory Optimization: DDR for Image Storage, BRAM for Intermediate Buffers</strong></h5><ul><li><strong>DDR (Dynamic RAM)</strong> will be used to store the original image and final processed output. This allows handling large images without running into FPGA memory constraints.</li><li><strong>BRAM (Block RAM)</strong> will serve as an intermediate buffer, storing smaller<strong>overlapping regions</strong> of the image during processing.</li><li><strong>AXI4 (Advanced eXtensible Interface)</strong> will manage data transfer between<strong>DDR and the processing modules</strong>, ensuring efficient memory access without bottlenecks.</li></ul><h5 id="axi-stream-for-processing-pipelines"><strong>AXI-Stream for Processing Pipelines</strong></h5><p>Rather than processing an image sequentially,<strong>AXI-Stream will enable a pipeline approach</strong>, where multiple processing stages operate in parallel. This allows continuous data flow, reducing latency and improving throughput.</p><h5 id="parallel-processing-using-image-splitting"><strong>Parallel Processing Using Image Splitting</strong></h5><p>For<strong>operations that do not involve geometric transformations</strong>, the image will be<strong>split into overlapping regions</strong> with necessary padding to avoid edge artifacts. These smaller blocks will be processed<strong>simultaneously</strong> and later recombined. This significantly speeds up computation while ensuring accuracy. For<strong>geometric transformations</strong>, special handling will be required to correctly map pixel positions.</p><h5 id="axi-dma-for-efficient-data-transfer"><strong>AXI DMA for Efficient Data Transfer</strong></h5><p>To avoid CPU intervention in moving image data,<strong>AXI DMA (Direct Memory Access)</strong> will be used to transfer pixel data<strong>directly between DDR and processing units</strong>, allowing continuous streaming of images into the pipeline without stalling.</p><h5 id="vgawireless-display-output-optional-enhancement"><strong>VGA/Wireless Display Output (Optional Enhancement)</strong></h5><p>If real-time visualization of processed images is needed, a<strong>VGA output or a wireless display interface (such as HDMI over Wi-Fi or LVDS panels)</strong> can be integrated. This eliminates the need for software-based file conversions and external host reconstruction, allowing direct, real-time monitoring of image processing results. While not mandatory, this enhancement can significantly improve debugging efficiency and system usability.</p><h5 id="testbenches-for-validation"><strong>Testbenches for Validation</strong></h5><p>To ensure<strong>mathematical accuracy</strong>, the new FPGA implementation will include<strong>systematic testbenches</strong> that validate outputs<strong>pixel-by-pixel</strong> against a Python reference. Unlike the current workflow, which relies on visual verification, this will ensure exact mathematical equivalence.</p><h4 id="expected-outcomes"><strong>Expected Outcomes</strong></h4><p>By implementing this optimized FPGA-based workflow,<strong>ImProVe</strong> will transition from a<strong>simulated Verilog project to a real-time, hardware-accelerated image processing system</strong>. The use of<strong>CORDIC, AXI-based memory architecture, parallel processing, pipelining, and real-time display output</strong> will significantly enhance performance, making the system viable for embedded vision applications in robotics, medical imaging, and autonomous systems.</p><p>I&rsquo;m working on the POC in parallel, and as of now, I have implemented square root using CORDIC, though it still requires some fine-tuning. This is just a proposal and may evolve further based on implementation challenges and optimizations needed along the way. The PoC will serve as a proof of concept for all the modules, ensuring they can be adapted for FPGA implementation. By the end, I aim to demonstrate at least one module as synthesizable and successfully implement it on an FPGA board.</p>
]]></content:encoded></item><item><title/><link>https://mummanajagadeesh.github.io/projects/improve/never/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/projects/improve/never/</guid><description>&lt;![CDATA[<h2 id="never-neural-network-in-verilog"><a href="https://github.com/Mummanajagadeesh/NeVer" target="_blank">NeVer: NEural NEtwork in VERilog</a></h2><h5 id="do-checkout-main-project-improve-image-processing-using-verilog">Do Checkout Main Project:<strong><a href="http://mummanajagadeesh.github.io/projects/improve/" target="_blank">ImProVe: IMage PROcessing using VErilog</a></strong></h5><table><thead><tr><th><strong>Name</strong></th><th>NeVer</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>NeVer implements a neural network in Verilog for better hardware acceleration of image processing tasks</td></tr><tr><td><strong>Start</strong></td><td>28 Feb 2025</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/NeVer" target="_blank">NeVer🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Image Processing, HDL, Computer Vision, Programming, ML</td></tr><tr><td><strong>Tools Used</strong></td><td>Verilog, Icarus, Perl, TCL, Quartus, Python, NumPy</td></tr><tr><td><strong>Current Status</strong></td><td>Ongoing (Active)</td></tr><tr><td><strong>Progress</strong></td><td>- Implemented detection of MNIST digits (0-9)<br> - Added support for EMNIST, enabling classification of 62 character classes<br> - Integrated real-time inference with a Tkinter-based character drawing interface, achieving sub-5s latency per prediction in simulation</td></tr><tr><td><strong>Next Steps</strong></td><td>- Ensure the top module is synthesizable by eliminating the use of the<code>real</code> data type<br> - Optimize the design for parallel processing, leveraging Multiply-Accumulate (MAC) operations<br> - Enhance floating-point multiplication and division support for improved computational efficiency</td></tr></tbody></table><hr><h4 id="project-overview"><strong>Project Overview</strong></h4><p>I highly recommend checking out the main project, as this is just a subset. The main project focuses on image processing algorithms, and working on<a href="../">ImProVe (IMage PROcessing using VErilog)</a> has made the learning curve for this project much easier.</p>]]></description><content:encoded>&lt;![CDATA[<h2 id="never-neural-network-in-verilog"><a href="https://github.com/Mummanajagadeesh/NeVer" target="_blank">NeVer: NEural NEtwork in VERilog</a></h2><h5 id="do-checkout-main-project-improve-image-processing-using-verilog">Do Checkout Main Project:<strong><a href="http://mummanajagadeesh.github.io/projects/improve/" target="_blank">ImProVe: IMage PROcessing using VErilog</a></strong></h5><table><thead><tr><th><strong>Name</strong></th><th>NeVer</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>NeVer implements a neural network in Verilog for better hardware acceleration of image processing tasks</td></tr><tr><td><strong>Start</strong></td><td>28 Feb 2025</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/NeVer" target="_blank">NeVer🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Image Processing, HDL, Computer Vision, Programming, ML</td></tr><tr><td><strong>Tools Used</strong></td><td>Verilog, Icarus, Perl, TCL, Quartus, Python, NumPy</td></tr><tr><td><strong>Current Status</strong></td><td>Ongoing (Active)</td></tr><tr><td><strong>Progress</strong></td><td>- Implemented detection of MNIST digits (0-9)<br> - Added support for EMNIST, enabling classification of 62 character classes<br> - Integrated real-time inference with a Tkinter-based character drawing interface, achieving sub-5s latency per prediction in simulation</td></tr><tr><td><strong>Next Steps</strong></td><td>- Ensure the top module is synthesizable by eliminating the use of the<code>real</code> data type<br> - Optimize the design for parallel processing, leveraging Multiply-Accumulate (MAC) operations<br> - Enhance floating-point multiplication and division support for improved computational efficiency</td></tr></tbody></table><hr><h4 id="project-overview"><strong>Project Overview</strong></h4><p>I highly recommend checking out the main project, as this is just a subset. The main project focuses on image processing algorithms, and working on<a href="../">ImProVe (IMage PROcessing using VErilog)</a> has made the learning curve for this project much easier.</p><p>NeVer (Neural Network in Verilog) is one of my favorite subprojects under ImProVe. In this subproject, my goal is to implement a fully functional neural network purely in Verilog and optimize it for efficient hardware acceleration.</p><p>Please note that the names of these projects are not meant to be taken too seriously. The names like ImProVe or NeVer may not fully reflect their functions – ImProVe doesn’t actually improve images, but processes them, and NeVer isn&rsquo;t about something &ldquo;never-implemented&rdquo; – many have done it before. The names just make it easier for me to organize folders and code</p><h4 id="motivation"><strong>Motivation</strong></h4><p>I was inspired by this video:<a href="https://www.youtube.com/watch?v=w8yWXqWQYmU&amp;ab_channel=SamsonZhang" target="_blank">Building a neural network FROM SCRATCH (no TensorFlow/PyTorch, just NumPy &amp; math)</a> by<strong>Samson Zhang</strong>. The video demonstrates a simple 2 layer neural network for recognizing MNIST digits (0-9)</p><p>I expanded on this by:</p><ul><li>Using<strong>two hidden layers</strong> instead of none</li><li>Implementing<strong>Adam optimizer</strong> along with<strong>vanilla SGD</strong>, replacing basic gradient descent</li><li>Extending support for<strong>62 classes</strong> (0-9, A-Z, a-z)</li><li>Using<strong>Verilog</strong> for inference instead of Python</li><li>Incorporating<strong>Tkinter</strong> for manually inputting handwritten characters irl, allowing direct interaction with the trained model</li></ul><h4 id="project-roadmap"><strong>Project Roadmap</strong></h4><ol><li>Train a model using<strong>TensorFlow/PyTorch</strong> for quick validation → Infer in Python</li><li>Train using<strong>NumPy only</strong> → Infer in Python</li><li>Train using<strong>NumPy only</strong> → Infer in Verilog</li><li>Train using<strong>pure Python (no NumPy, user-defined functions)</strong> → Infer in Verilog</li><li>Implement a<strong>single neuron in Verilog</strong> for training</li><li>Train<strong>directly in Verilog</strong> → Infer in Verilog</li><li><strong>Optimize</strong> the implementation using<strong>parallel processing, MAC units, etc.</strong></li><li>Make the<strong>entire Verilog implementation synthesizable</strong></li></ol><hr><h4 id="current-status"><strong>Current Status</strong></h4><ul><li>Successfully performed inference in Verilog using parameters trained in<strong>Python</strong> (Colab) with<strong>NumPy</strong> , achieving<strong>>90% accuracy</strong> on the test dataset</li><li>Implemented inference for both<strong>MNIST (digits 0-9)</strong> and<strong>EMNIST (62 classes: 0-9, A-Z, a-z)</strong></li><li>Training with<strong>2000 iterations</strong>:<ul><li><strong>First 1500 iterations</strong>: Adam optimizer</li><li><strong>Last 500 iterations</strong>: Vanilla SGD (Reason: Adam converges faster initially, but switching to SGD helps refine convergence | Later switched to<strong>&ldquo;SGD with Momentum&rdquo;</strong>)</li></ul></li><li>Using<strong>Tkinter</strong> for drawing input characters, which are then processed and fed into Verilog for inference</li><li>Achieved<strong>&lt;5s inference latency</strong> per prediction in simulation with FSM-driven layer evaluation</li><li>Actively working on<strong>image reconstruction</strong> from NumPy-trained model parameters</li><li>Replacing<code>real</code><strong>(IEEE 754)</strong> in Verilog top module with fixed-point formats like<strong>Q32.32, Q32.16, Q24.8 using sfixed from IEEE 1076.3 (ieee.fixed_pkg)</strong><ul><li>Aiming for better synthesis compatibility, but currently observing<strong>accuracy loss</strong> due to quantization</li><li>Actively tuning<strong>fixed-point precision</strong> to balance accuracy and hardware performance</li></ul></li></ul><hr><h4 id="current-workflow"><strong>Current Workflow</strong></h4><ol><li><strong>Image Processing</strong>:<code>draw.py</code> → Converts the Tkinter drawing into<code>drawing.jpg</code></li><li><strong>Grayscale Image Conversion</strong>:<code>img2bin.py</code> → Generates<code>mnist_single_no.txt</code> (integer values from 0-255)</li><li><strong>Vectorization</strong>:<code>arr2row.v</code> → Flattens the<strong>2D 28×28</strong> matrix into<code>input_vector.txt</code> after preprocessing</li><li><strong>Memory Preloading</strong>:<code>memloader_from_inp_vec.py</code> → Converts<code>input_vector.txt</code> into Verilog memory (<code>image_memory.v</code>)</li><li><strong>Weight &amp; Bias Preloading</strong>:<code>wtbs_loader.py</code> → Converts pretrained weight &amp; bias TXT files into Verilog memory (<code>W1_memory.v</code>,<code>b1_memory.v</code>, etc.)</li><li><strong>Inference in Verilog</strong>:<code>emnist_with_tb.v</code> → Top module:<code>emnist.v</code></li></ol><h5 id="folder-structure"><strong>Folder Structure</strong></h5><p>Can be found in the<code>/clean</code> directory under the emnist folders in the repo</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>.</span></span><span style="display:flex;"><span>├───bin/<span style="color:#75715e"># Intermediate image text files</span></span></span><span style="display:flex;"><span>│ └───*.txt</span></span><span style="display:flex;"><span>├───build/<span style="color:#75715e"># Compiled Verilog simulation output</span></span></span><span style="display:flex;"><span>│ └───*.vvp</span></span><span style="display:flex;"><span>├───data/<span style="color:#75715e"># Stores trained model parameters and images</span></span></span><span style="display:flex;"><span>│ ├───biases/<span style="color:#75715e"># Text files containing trained bias values</span></span></span><span style="display:flex;"><span>│ │ └───*.txt</span></span><span style="display:flex;"><span>│ ├───weights/<span style="color:#75715e"># Text files containing trained weight values</span></span></span><span style="display:flex;"><span>│ │ └───*.txt</span></span><span style="display:flex;"><span>│ └───*.png<span style="color:#75715e"># Image files</span></span></span><span style="display:flex;"><span>├───run/<span style="color:#75715e"># Setup and automation scripts for different platforms</span></span></span><span style="display:flex;"><span>│ ├───linux-or-macOs/<span style="color:#75715e"># Setup and automation scripts for Linux or macOS</span></span></span><span style="display:flex;"><span>│ │ └───Makefile<span style="color:#75715e"># Makefile for automating simulation/build steps</span></span></span><span style="display:flex;"><span>│ ├───perl/<span style="color:#75715e"># Scripts for automating simulation and clean-up (Perl)</span></span></span><span style="display:flex;"><span>│ │ └───*.pl</span></span><span style="display:flex;"><span>│ ├───tcl/<span style="color:#75715e"># Scripts for automating simulation and clean-up (TCL)</span></span></span><span style="display:flex;"><span>│ │ └───*.tcl</span></span><span style="display:flex;"><span>│ └───windows/<span style="color:#75715e"># Setup and automation scripts for Windows</span></span></span><span style="display:flex;"><span>│ └───*.bat</span></span><span style="display:flex;"><span>├───scripts/<span style="color:#75715e"># Python scripts for image preprocessing</span></span></span><span style="display:flex;"><span>│ └───*.py</span></span><span style="display:flex;"><span>├───sim/<span style="color:#75715e"># Test benches for verifying Verilog modules</span></span></span><span style="display:flex;"><span>│ └───*.v</span></span><span style="display:flex;"><span>└───src/<span style="color:#75715e"># Source Verilog code</span></span></span><span style="display:flex;"><span> ├───image/<span style="color:#75715e"># Image memory modules</span></span></span><span style="display:flex;"><span> │ └───*.v</span></span><span style="display:flex;"><span> ├───params/<span style="color:#75715e"># Memory modules for weights and biases</span></span></span><span style="display:flex;"><span> │ └───*.v</span></span><span style="display:flex;"><span> └───top/<span style="color:#75715e"># Top-level design module and supporting modules</span></span></span><span style="display:flex;"><span> └───*.v</span></span></code></pre></div><table><thead><tr><th><strong>Platform</strong></th><th><strong>Command</strong></th></tr></thead><tbody><tr><td><strong>Windows</strong></td><td><code>cd path-to-clean-dir/</code><br><code>./run/win32/setup.bat</code></td></tr><tr><td><strong>Linux/macOS</strong></td><td><code>cd path-to-clean-dir/run/</code><br><code>make</code></td></tr><tr><td><strong>Perl (Cross-Platform)</strong></td><td><code>cd path-to-clean-dir/</code><br><code>perl ./run/perl/workflow.pl</code></td></tr><tr><td><strong>Tcl (Cross-Platform)</strong></td><td><code>cd path-to-clean-dir/</code><br><code>tclsh ./run/tcl/workflow.tcl</code></td></tr></tbody></table><h5 id="command-workflow">Command Workflow</h5><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>cd path-to-clean-dir/<span style="color:#75715e"># navigate to your project directory</span></span></span><span style="display:flex;"><span>python ./scripts/draw.py<span style="color:#75715e"># opens a Tkinter interface to draw an image by hand</span></span></span><span style="display:flex;"><span>python ./scripts/img2bin.py<span style="color:#75715e"># converts the drawn image into a text-based binary file</span></span></span><span style="display:flex;"><span>iverilog -o ./build/imgvec.vvp ./src/*.v<span style="color:#75715e"># compiles Verilog code that returns a 1D vector after all preprocessing</span></span></span><span style="display:flex;"><span>vvp ./build/imgvec.vvp<span style="color:#75715e"># runs the compiled imgvec Verilog simulation</span></span></span><span style="display:flex;"><span>python ./scripts/wtbs_loader.py<span style="color:#75715e"># (only needed for first build) generates weight and bias memory Verilog files</span></span></span><span style="display:flex;"><span>python ./scripts/memloader_from_inp_vec.py<span style="color:#75715e"># loads the converted image vector into the image_memory module</span></span></span><span style="display:flex;"><span>iverilog -o build/prediction_test.vvp ./sim/*.v ./src/top/*.v ./src/image/*.v ./src/params/*.v<span style="color:#75715e"># compiles entire prediction design</span></span></span><span style="display:flex;"><span>vvp ./build/prediction_test.vvp<span style="color:#75715e"># runs the compiled prediction simulation</span></span></span></code></pre></div><h5 id="state-diagram-generated-using-graphviz-tool">State Diagram (generated using GraphViz tool)</h5><img title="" loading="lazy" decoding="async" class="img  " width="312" height="800" src="/images/projects/improve/never/fsm_hu_2c7319fa8741805d.webp" alt="FSM Mealy" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/fsm_hu_cce4f5e203857f46.png'"/><script>
window.addEventListener("load", (e) => {
const lightbox = GLightbox();
});</script><hr><h5 id="memory-initialization">Memory Initialization</h5><ul><li><strong>Structure &amp; Initialization</strong>: A simple<code>reg [WIDTH-1:0] mem [0:DEPTH-1];</code> array whose entries are hard‑coded in an<code>initial</code> block at elaboration time, causing the synthesis tool to bake those constants into the FPGA configuration bitstream citeturn0search6.</li><li><strong>Run‑Time Behavior</strong>: Acts as a pure, read‑only memory (ROM)—you present an address, and on the next clock (or combinationally, depending on inference) you get the stored value. No write ports or file‑I/O occur after configuration citeturn1search1.</li><li><strong>Implementation Styles</strong>:<ul><li><strong>Small arrays</strong> (tens to low hundreds of bits) typically map to<strong>distributed LUT‑RAM</strong>.</li><li><strong>Larger arrays</strong> (kilobits) are inferred as<strong>dedicated block RAM</strong> macros.</li></ul></li><li><strong>Why This Pattern</strong>: Embedding all data directly in RTL avoids reliance on external files and runtime system tasks like<code>$readmemh</code>,<code>$fopen</code> or custom parsers. That makes simulation setups simpler (no file‑path issues), keeps testbenches file‑agnostic, and ensures cross‑tool portability and deterministic initialization.</li></ul><h4 id="technical-details"><strong>Technical Details</strong></h4><ul><li>The<strong>top module</strong> (<code>emnist.v</code>) follows an<strong>FSM-based approach</strong> with minimal to no overlap</li><li><strong>Softmax Approximation</strong>: Using<strong>Taylor series expansion</strong> for exponentiation</li><li><strong>Pipeline Strategy</strong>:<ul><li><strong>Currently</strong>: Using<strong>coarse-grained pipelining</strong>, meaning major computation blocks execute sequentially with some latency</li><li><strong>Next Steps</strong>: Implement<strong>fine-grained pipelining</strong>, where smaller operations are parallelized for higher throughput</li></ul></li></ul><hr><h4 id="to-do"><strong>To-Do</strong></h4><ul><li>Implement<strong>LUTs</strong> for efficient exponential computation in Softmax</li><li>Remove the<strong><code>real</code> datatype</strong> in the top module to ensure full synthesizability</li><li>Extend support for<strong>all ASCII characters</strong> (optional)</li><li>Enable<strong>OCR functionality</strong> to detect any character in a given image</li><li>Optimize for<strong>parallel processing</strong>,<strong>better pipelining</strong>, and<strong>hardware acceleration</strong></li></ul><hr><h4 id="demos"><strong>Demos</strong></h4><p>Here are some demo videos</p><h5 id="mnist-digit-recognition"><strong>Mnist Digit Recognition</strong></h5><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/QtkdWSq25zQ" frameborder="0" allowfullscreen=/></div></div><p><br><br/><ul><li><p>I developed a<strong>fully connected neural network</strong> from scratch in<strong>Google Colab</strong>, avoiding frameworks like<strong>TensorFlow</strong> and<strong>Keras</strong>. Instead, I relied on<strong>NumPy</strong> for numerical operations,<strong>Pandas</strong> for data handling, and<strong>Matplotlib</strong> for visualization. The model was trained on<strong>sample_data/mnist_train_small.csv</strong>, a dataset containing flattened<strong>784-pixel images</strong> of handwritten digits. Data preprocessing included<strong>normalizing pixel values</strong> (dividing by<strong>255</strong>) and splitting the dataset into a<strong>training set</strong> and a<strong>development set</strong>, with the first<strong>1000 samples</strong> reserved for validation. The dataset was shuffled before training to enhance randomness, and labels (digits 0-9) were stored separately</p></li><li><p>The network architecture consists of an<strong>input layer (784 neurons)</strong>, a<strong>hidden layer (128 neurons, ReLU activation)</strong>, and an<strong>output layer (10 neurons, softmax activation)</strong>. Model parameters (weights and biases) were initialized randomly and updated via<strong>gradient descent</strong> over<strong>500 iterations</strong> with a learning rate of<strong>0.1</strong>. Training followed the standard<strong>forward propagation</strong> for computing activations and<strong>backpropagation</strong> for updating parameters. Accuracy was recorded every<strong>10 iterations</strong>. To ensure compatibility with<strong>Verilog</strong>, all weights and biases were<strong>scaled by 10,000</strong> and stored as<strong>integer values</strong> in text files (<code>W1.txt</code>,<code>b1.txt</code>, etc.), eliminating the need for<strong>floating-point operations</strong> in hardware. These trained parameters were later used for inference on new images, verifying accuracy on the<strong>development set</strong> before deployment in Verilog for real-time classification</p></li><li><p>The trained model, based on<strong>sample_data/mnist_train_small.csv</strong>, achieved<strong>over 90% accuracy</strong>. It generates<code>W1</code>,<code>W2</code>,<code>b1</code>, and<code>b2</code> text files containing weight and bias values. These parameters are used in Verilog to predict digits from an<strong>input image</strong> stored in<code>input_vector.txt</code>, formatted as<strong>784 space-separated integers</strong>. The Verilog module reads this data, performs inference, and displays the predicted output using<code>$display</code>. The original CSV file was converted into a<strong>space-separated text format</strong>, where each row contains a digit followed by<strong>784 pixel values (785 total)</strong>. During inference, the first value (label) is discarded, ensuring the model classifies the input image without prior knowledge of its actual label</p></li><li><p>The<strong>Verilog implementation</strong> of the neural network consists of an<strong>input layer (784 neurons)</strong>, a<strong>hidden layer (128 neurons)</strong>, and an<strong>output layer (10 neurons)</strong>. It loads<strong>pre-trained weights and biases</strong> from<code>W1.txt</code>,<code>b1.txt</code>,<code>W2.txt</code>, and<code>b2.txt</code>, along with an<strong>input vector</strong> from<code>input_vector.txt</code>. Input values are<strong>normalized</strong> by dividing by<strong>255.0</strong>, while weights and biases are<strong>scaled by 10,000</strong> for fixed-point arithmetic. The hidden layer applies a<strong>fully connected transformation</strong> (<code>W1 * input + b1</code>) followed by<strong>ReLU activation</strong>, while the output layer computes another weighted sum (<code>W2 * hidden + b2</code>). Instead of applying softmax, the model identifies the predicted class by selecting the index of the<strong>highest output value</strong></p></li><li><p>The module ensures correct file reading before computation begins. Forward propagation is executed sequentially, with an initial delay for<strong>loading weights, biases, and input values</strong>. After processing activations in both layers, the output layer iterates through its neurons to determine the class with the highest activation. The classification result is displayed via<code>$display</code>. This hardware implementation<strong>bypasses complex activation functions</strong> like softmax while maintaining classification accuracy through direct maximum-value selection</p></li><li><p>In the latest iterations,<strong>Python scripts</strong> convert text-based weight and bias files into<strong>synthesizable Verilog memory blocks</strong>. These are stored in<strong>register modules</strong>, which are instantiated in the<strong>top-level module</strong>. The image input is handled in a similar way</p></li><li><p>Currently, the<strong>top module</strong> includes a few non-synthesizable constructs, such as<code>$display</code>,<code>$finish</code>, and the<strong><code>real</code> datatype</strong>. These were relocated to the testbench in later versions to improve synthesizability. Additionally, I am replacing<code>real</code> with a<strong>fixed-point representation (Q24.8)</strong> to make the design fully synthesizable. Future versions will output the classification result to a<strong>seven-segment display</strong> via<strong>case statements</strong>, replacing<code>$display</code></p></li><li><p>Moving forward, I am working on transitioning<strong>training from Python to Verilog</strong>, aiming to implement a fully<strong>synthesizable neural network</strong> for hardware-based learning and inference</p></li></ul><hr><h5 id="emnist-character-recognition"><strong>EMNIST Character Recognition</strong></h5><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/7YccFUtydM0" frameborder="0" allowfullscreen=/></div></div><p><br><br/><p>This model is trained on the<strong>EMNIST ByClass dataset</strong> (<a href="https://greg-cohen.com/datasets/emnist/" target="_blank">source</a>), which includes<strong>62 character classes</strong>—digits (<code>0-9</code>), uppercase letters (<code>A-Z</code>), and lowercase letters (<code>a-z</code>). The dataset undergoes preprocessing, where it is converted into a<strong>CSV format</strong>, normalized, reduced in dimensionality, and shuffled before training to improve generalization</p><h6 id="neural-network-architecture"><strong>Neural Network Architecture</strong></h6><p>The model consists of multiple layers:</p><ul><li><strong>Input Layer</strong>:<strong>784 neurons</strong> (28×28 grayscale pixel values)</li><li><strong>First Hidden Layer</strong>:<strong>256 neurons</strong> (<code>W1: 256×784</code>,<code>b1: 256×1</code>)</li><li><strong>Second Hidden Layer</strong>:<strong>128 neurons</strong> (<code>W2: 128×256</code>,<code>b2: 128×1</code>)</li><li><strong>Output Layer</strong>:<strong>62 neurons</strong> (<code>W3: 62×128</code>,<code>b3: 62×1</code>)</li></ul><h6 id="training-process"><strong>Training Process</strong></h6><p>The network is trained using<strong>forward propagation</strong>, where activations are computed at each layer through<strong>matrix multiplications</strong> and<strong>ReLU activation functions</strong> for hidden layers.<strong>Backpropagation</strong> is used to update weights based on the gradient of the loss function. The dataset is shuffled before each epoch to prevent overfitting. The model is trained over multiple epochs using a combination of<strong>Stochastic Gradient Descent (SGD)</strong> and the<strong>Adam optimizer</strong> to improve convergence</p><p>To ensure compatibility with hardware, weights and biases are<strong>scaled by 10,000</strong> and stored as integers in text files (<code>W1.txt</code>,<code>b1.txt</code>, etc.), since Verilog does not support floating-point arithmetic</p><h6 id="inference-in-verilog"><strong>Inference in Verilog</strong></h6><p>The inference process in Verilog follows a similar structure but accommodates additional layers and character classes. Input images are read from<code>input_vector.txt</code>, normalized, and processed through the neural network using preloaded weights and biases. The computation follows:</p><ul><li><code>hidden1 = ReLU(W1 * input + b1)</code></li><li><code>hidden2 = ReLU(W2 * hidden1 + b2)</code></li><li><code>output = W3 * hidden2 + b3</code></li></ul><p>The<strong>index of the maximum output value</strong> corresponds to the predicted character, which is mapped to<code>"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"</code> and displayed using<code>$display</code></p><hr><h6 id="pipeline-and-data-flow"><strong>Pipeline and Data Flow</strong></h6><p>The following scripts handle image processing, vectorization, and memory loading for Verilog inference:</p><ol><li><strong>Drawing &amp; Image Processing</strong>:<ul><li><code>draw.py</code>: Creates a square canvas in Tkinter for character input</li><li>After drawing, the script<strong>grayscales, inverts, compresses</strong> the image to<strong>28×28 resolution</strong>, and saves it as<code>drawing.jpg</code></li></ul></li><li><strong>Data Conversion &amp; Preprocessing</strong>:<ul><li><code>img2bin.py</code>: Converts<code>drawing.jpg</code> into a<strong>28×28</strong> grayscale pixel matrix (<code>mnist_single_no.txt</code>)</li><li><code>Other Verilog Files</code>: Flattens the<strong>2D array</strong> into a<strong>1D vector (784 values)</strong> and stores it in<code>input_vector.txt</code> after all preprocessing</li></ul></li><li><strong>Memory Module Generation</strong>:<ul><li><code>memloader_from_inp_vec.py</code>: Converts<code>input_vector.txt</code> into a<strong>synthesizable Verilog memory module</strong> (<code>image_memory.v</code>)</li><li><code>wtbs_loader.py</code>: Converts<code>W1</code>,<code>W2</code>,<code>W3</code>,<code>b1</code>,<code>b2</code>,<code>b3</code> into Verilog memory modules (<code>W1_memory.v</code>,<code>b1_memory.v</code>, etc.)</li></ul></li></ol><h6 id="final-hardware-implementation"><strong>Final Hardware Implementation</strong></h6><p>All these components are instantiated in the<strong>top module</strong> (<code>emnist_with_tb.v</code>), along with a<strong>testbench</strong> (<code>emnist_nn_tb.v</code>). The system successfully predicts handwritten characters in real-time</p><p>In the<strong>demo</strong>, I tested the characters<strong>&ldquo;H&rdquo;</strong>,<strong>&ldquo;f&rdquo;</strong>, and<strong>&ldquo;7&rdquo;</strong>, each representing different EMNIST subclasses (uppercase letters, lowercase letters, and numbers)</p><p>Additionally, I implemented a<strong>coarse-grained pipelined</strong> fully connected neural network using a<strong>Finite State Machine (FSM)</strong>, integrating a<strong>Softmax function approximation</strong> via<strong>Taylor series expansion</strong> to improve computational efficiency</p><h4 id="raw-demo-shots-present"><strong>Raw Demo Shots (Present)</strong></h4><table><thead><tr><th><strong>UpperCase Alphabet</strong></th><th><strong>LowerCase Alphabet</strong></th><th><strong>Single Digit Number</strong></th></tr></thead><tbody/></table><table><thead><tr><th><strong>Actual: R</strong></th><th><strong>Prediction: R</strong></th><th><strong>Actual: i</strong></th><th><strong>Prediction: i</strong></th><th><strong>Actual: 9</strong></th><th><strong>Prediction: 9</strong></th></tr></thead><tbody/></table><table><thead><tr><th><img title="" loading="lazy" decoding="async" class="img  " width="300" height="" src="/images/projects/improve/never/R.gif" alt="R" onerror="this.onerror='null';this.src=''"/></th><th><img title="" loading="lazy" decoding="async" class="img  " width="300" height="" src="/images/projects/improve/never/i.gif" alt="i" onerror="this.onerror='null';this.src=''"/></th><th><img title="" loading="lazy" decoding="async" class="img  " width="300" height="" src="/images/projects/improve/never/9.gif" alt="9" onerror="this.onerror='null';this.src=''"/></th></tr></thead><tbody/></table><hr><p><br><br/><h4 id="python-based-pre-processing-workflow-present"><strong>Python Based Pre-processing Workflow (Present)</strong></h4><h6 id="user-is-instructed-to-fill-the-canvas-to-skip-roi-steps">USER is instructed to fill the canvas to skip ROI steps</h6><table><thead><tr><th><strong>Original Drawing</strong></th><th><strong>Grayscale Image</strong></th><th><strong>Inverted Image</strong></th><th><strong>Text Matrix</strong></th></tr></thead><tbody><tr><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="300" src="/images/projects/improve/never/drawing_hu_d0025e211bebb1d4.webp" alt="Org Drawing" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/drawing_hu_aa607f95db2e4166.png'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="300" src="/images/projects/improve/never/drawing_gray_hu_1aa6c217779fa83f.webp" alt="Grayscale Image" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/drawing_gray_hu_1ec3127228ae2c9f.png'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="300" src="/images/projects/improve/never/drawing_inverted_hu_5998a1c57c3dc469.webp" alt="Inverted Image" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/drawing_inverted_hu_1771a2c107cd69a2.png'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="299" src="/images/projects/improve/never/drawing_txt_hu_763c9a05a044497f.webp" alt="Text Matrix" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/drawing_txt_hu_e93f287e5d6bbf4a.png'"/></td></tr></tbody></table><h4 id="verilog-based-pre-processing-workflow-currently-working-to-refine"><strong>Verilog Based Pre-processing Workflow (Currently Working to Refine)</strong></h4><h6 id="exploring-bus-interconnection-options-for-automated-workflow">Exploring bus interconnection options for automated workflow</h6><table><thead><tr><th><strong>Original</strong></th><th><strong>Downscaled (28×28)</strong></th><th><strong>Grayscale</strong></th><th><strong>Contrasted</strong></th></tr></thead><tbody><tr><td><img title="" loading="lazy" decoding="async" class="img  " width="200" height="200" src="/images/projects/improve/never/ver-preproc/drawing_hu_142ff9cac423d406.webp" alt="Original" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/ver-preproc\/drawing_hu_77cfd1ed5924331.png'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="200" height="200" src="/images/projects/improve/never/ver-preproc/rgb_28_hu_824266d2b48a95cb.webp" alt="Downscaled" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/ver-preproc\/rgb_28_hu_a68bc88659874c18.png'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="200" height="200" src="/images/projects/improve/never/ver-preproc/gray_28_hu_d427e88f7c4d63f9.webp" alt="Grayscale" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/ver-preproc\/gray_28_hu_25eb9e4edc5f8609.png'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="200" height="200" src="/images/projects/improve/never/ver-preproc/grayc_28_hu_6c74984bebdcf846.webp" alt="Contrasted" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/ver-preproc\/grayc_28_hu_f7fd1289e0f46e17.png'"/></td></tr></tbody></table><table><thead><tr><th><strong>Edge Detection</strong></th><th><strong>Bounding Box</strong></th><th><strong>Box Overlayed</strong></th><th><strong>Region of Interest</strong></th></tr></thead><tbody><tr><td><img title="" loading="lazy" decoding="async" class="img  " width="200" height="200" src="/images/projects/improve/never/ver-preproc/sumapp_hu_31f86090b367940.webp" alt="Edge Detection" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/ver-preproc\/sumapp_hu_a982b9fe5fdfbb9c.png'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="200" height="200" src="/images/projects/improve/never/ver-preproc/bound_box_hu_ca9fd7b3ed3f5635.webp" alt="Bounding Box" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/ver-preproc\/bound_box_hu_b7696a979e31eeaf.png'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="200" height="200" src="/images/projects/improve/never/ver-preproc/box_overlayed_hu_d5841ab1f65753df.webp" alt="Box Overlayed" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/ver-preproc\/box_overlayed_hu_c38c2054a436358.png'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="200" height="200" src="/images/projects/improve/never/ver-preproc/roi_hu_dcd9effc3e0d1bc1.webp" alt="ROI" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/ver-preproc\/roi_hu_3184694d9f973b10.png'"/></td></tr></tbody></table><table><thead><tr><th><strong>ROI Zoom to Fit</strong></th><th><strong>ROI Resized 28x28</strong></th><th><strong>ROI Padded</strong></th><th><strong>Inverted</strong></th></tr></thead><tbody><tr><td><img title="" loading="lazy" decoding="async" class="img  " width="200" height="186" src="/images/projects/improve/never/ver-preproc/rois_hu_e857a299ed77902b.webp" alt="ROI Zoom" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/ver-preproc\/rois_hu_64eabc826c5144b8.png'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="200" height="200" src="/images/projects/improve/never/ver-preproc/roi_28_hu_be49d0488b4e91be.webp" alt="ROI Resized" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/ver-preproc\/roi_28_hu_6770dd8acad97c2a.png'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="200" height="200" src="/images/projects/improve/never/ver-preproc/roip_28_hu_bc0d197d2e438345.webp" alt="ROI Padded" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/ver-preproc\/roip_28_hu_c0cd10585c412861.png'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="200" height="200" src="/images/projects/improve/never/ver-preproc/roipi_28_hu_ed5014128a0e5a8a.webp" alt="Inverted" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/ver-preproc\/roipi_28_hu_e5645f8b1a7f3df3.png'"/></td></tr></tbody></table><table><thead><tr><th><strong>Mirrored (Vertical Axis)</strong></th><th><strong>Rotated 90° CCW</strong></th></tr></thead><tbody><tr><td><img title="" loading="lazy" decoding="async" class="img  " width="200" height="200" src="/images/projects/improve/never/ver-preproc/roipim_28_hu_11c7be39cd7eacfd.webp" alt="Mirrored" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/ver-preproc\/roipim_28_hu_8397089da3673e85.png'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="200" height="200" src="/images/projects/improve/never/ver-preproc/roipimr_28_hu_66bf90397ae111cc.webp" alt="Rotated 90 CCW" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/ver-preproc\/roipimr_28_hu_bf06f0adf6c3092.png'"/></td></tr></tbody></table><table><thead><tr><th><strong>File</strong></th><th><strong>What it Does</strong></th><th><strong>Why It&rsquo;s Done</strong></th><th><strong>Output</strong></th></tr></thead><tbody><tr><td><code>draw.py</code></td><td>User draws a character</td><td>Generates the initial input</td><td><code>drawing.png</code></td></tr><tr><td><code>img2rgb.py</code></td><td>Splits image into R, G, B channels</td><td>Needed for hardware-friendly memory modules</td><td><code>r_memory.v</code>,<code>g_memory.v</code>,<code>b_memory.v</code></td></tr><tr><td><code>downscale.v</code></td><td>Downscales 600×600 image to 28×28</td><td>Inference work on 28×28 inputs</td><td><code>r_memory_28.v</code>,<code>g_memory_28.v</code>,<code>b_memory_28.v</code></td></tr><tr><td><code>grayscale.v</code></td><td>Converts RGB to grayscale</td><td>Removes color bias, simplifies processing</td><td><code>gray_28.png</code></td></tr><tr><td><code>contrast.v</code></td><td>Enhances contrast</td><td>Makes strokes stand out better</td><td><code>grayc_28.png</code></td></tr><tr><td><code>prewitt.v</code></td><td>Applies Prewitt edge detection</td><td>Finds the outline of the character</td><td><code>sumapp.png</code></td></tr><tr><td><code>bound.v</code></td><td>Detects bounding box from edge outline</td><td>Identifies Region of Interest (ROI)</td><td><code>bound_box.png</code></td></tr><tr><td><code>boxcut.v</code></td><td>Applies bounding box to mask out external pixels</td><td>Isolates the character region</td><td><code>roi.png</code></td></tr><tr><td><code>boxcutF</code></td><td>Crops the ROI and zooms to fit the canvas</td><td>Removes zero-padding, centers the content</td><td><code>rois.png</code></td></tr><tr><td><code>resize.v</code></td><td>Resizes cropped ROI to 28×28</td><td>Standardizes input size for neural network</td><td><code>roi_28.png</code></td></tr><tr><td><code>padding.v</code></td><td>Adds 1-pixel padding on all sides, resizes again to 28×28</td><td>Adds margin, keeps stroke intact</td><td><code>roip_28.png</code></td></tr><tr><td><code>mirror.v</code></td><td>Mirrors image about the vertical axis</td><td>Matches EMNIST character orientation</td><td><code>roipim_28.png</code></td></tr><tr><td><code>rotate.v</code></td><td>Rotates image 90° counter-clockwise</td><td>Final transformation to match EMNIST format</td><td><code>roipimr_28.png</code></td></tr><tr><td><code>mat2row.v</code></td><td>Converts 28x28 matrix to 784 length vector</td><td>To vectorise the input</td><td><code>input_vector.txt</code></td></tr></tbody></table><p>UPDATED WORKFLOW</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>Step 1: Drawing</span></span><span style="display:flex;"><span> - draw.py → drawing.png</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span>Step 2: Image Preprocessing</span></span><span style="display:flex;"><span> - img2rgb.py → r_memory.v, g_memory.v, b_memory.v</span></span><span style="display:flex;"><span> - downscale.v → r_memory_28.v, g_memory_28.v, b_memory_28.v</span></span><span style="display:flex;"><span> - grayscale.v → gray_28.png</span></span><span style="display:flex;"><span> - contrast.v → grayc_28.png</span></span><span style="display:flex;"><span> - prewitt.v → edge outline<span style="color:#f92672">(</span>sumapp.png<span style="color:#f92672">)</span></span></span><span style="display:flex;"><span> - bound.v → bounding box<span style="color:#f92672">(</span>bound_box.png<span style="color:#f92672">)</span></span></span><span style="display:flex;"><span> - boxcut.v → masked ROI<span style="color:#f92672">(</span>roi.png<span style="color:#f92672">)</span></span></span><span style="display:flex;"><span> - boxcutF → cropped, zoomed ROI<span style="color:#f92672">(</span>rois.png<span style="color:#f92672">)</span></span></span><span style="display:flex;"><span> - resize.v → resize to 28x28<span style="color:#f92672">(</span>roi_28.png<span style="color:#f92672">)</span></span></span><span style="display:flex;"><span> - padding.v → padded and resized 28x28<span style="color:#f92672">(</span>roip_28.png<span style="color:#f92672">)</span></span></span><span style="display:flex;"><span> - mirror.v → mirror about vertical axis<span style="color:#f92672">(</span>roipim_28.png<span style="color:#f92672">)</span></span></span><span style="display:flex;"><span> - rotate.v → rotate 90° CCW<span style="color:#f92672">(</span>roipimr_28.png<span style="color:#f92672">)</span></span></span><span style="display:flex;"><span> - mat2row.v → vectorized input<span style="color:#f92672">(</span>input_vector.txt<span style="color:#f92672">)</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span>Step 3: Memory Loading</span></span><span style="display:flex;"><span> - memloader_from_inp_vec.py → image_memory.v</span></span><span style="display:flex;"><span> - wtbs_loader.py → W1_memory.v, W2_memory.v, W3_memory.v</span></span><span style="display:flex;"><span> - wtbs_loader.py → b1_memory.v, b2_memory.v, b3_memory.v</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span>Step 4: Neural Network Inference<span style="color:#f92672">(</span>Verilog<span style="color:#f92672">)</span></span></span><span style="display:flex;"><span> - emnist_nn.v → Top module</span></span><span style="display:flex;"><span> - Instantiates relu.v, softmax.v</span></span><span style="display:flex;"><span> - Instantiates W1/W2/W3_memory.v and b1/b2/b3_memory.v</span></span><span style="display:flex;"><span> - Instantiates image_memory.v</span></span><span style="display:flex;"><span> - emnist_nn_tb.v → Testbench</span></span><span style="display:flex;"><span> - Loads memories</span></span><span style="display:flex;"><span> - Triggers inference</span></span><span style="display:flex;"><span> - Displays prediction</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span>Step 5: Output</span></span><span style="display:flex;"><span> - Final predicted digit/character from Verilog simulation</span></span></code></pre></div><h4 id="image-reconstruction-trials"><strong>Image Reconstruction Trials</strong></h4><p>Image reconstruction works by taking random noise as the initial seed image and passing it through layers formed with trained parameters, iteratively minimizing the loss. This way, we gradually obtain the average image for each digit. Currently, I&rsquo;m working on refining the process in Python with MNIST data, which will eventually be translated into Verilog. The process is taking a significant amount of time per image (even in Python), so you can imagine the challenge in Verilog. I’m still testing the Verilog implementation and aiming to optimize it to achieve better speed performance compared to the Python version.</p><p>I&rsquo;m fully aware that generating images on edge devices—especially using parameters from a trained model (not even GANs)—doesn&rsquo;t quite align with the main objective of this project, mainly due to its limited real-world relevance. Still, I&rsquo;m pursuing it out of curiosity and for experimental purposes. The idea of creating an image using Verilog is highly conceptual, but it&rsquo;s a challenge I&rsquo;m enjoying exploring—just for the sake of it. :)</p><img title="" loading="lazy" decoding="async" class="img  " width="900" height="436" src="/images/projects/improve/never/imgen_hu_956f3ec8d974c730.webp" alt="image-recon" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/imgen_hu_6c7dd18bc41a507d.png'"/>
]]></content:encoded></item><item><title/><link>https://mummanajagadeesh.github.io/projects/never/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/projects/never/</guid><description>&lt;![CDATA[<h2 id="never-neural-network-in-verilog"><a href="https://github.com/Mummanajagadeesh/NeVer" target="_blank">NeVer: NEural NEtwork in VERilog</a></h2><h5 id="do-checkout-main-project-improve-image-processing-using-verilog">Do Checkout Main Project:<strong><a href="http://mummanajagadeesh.github.io/projects/improve/" target="_blank">ImProVe: IMage PROcessing using VErilog</a></strong></h5><table><thead><tr><th><strong>Name</strong></th><th>NeVer</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>NeVer implements a neural network in Verilog for better hardware acceleration of image processing tasks</td></tr><tr><td><strong>Start</strong></td><td>28 Feb 2025</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/NeVer" target="_blank">NeVer🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Image Processing, HDL, Computer Vision, Programming, ML</td></tr><tr><td><strong>Tools Used</strong></td><td>Verilog, Icarus, Perl, TCL, Quartus, Python, NumPy</td></tr><tr><td><strong>Current Status</strong></td><td>Ongoing (Active)</td></tr><tr><td><strong>Progress</strong></td><td>- Implemented detection of MNIST digits (0-9)<br> - Added support for EMNIST, enabling classification of 62 character classes<br> - Integrated real-time inference with a Tkinter-based character drawing interface, achieving sub-5s latency per prediction in simulation</td></tr><tr><td><strong>Next Steps</strong></td><td>- Ensure the top module is synthesizable by eliminating the use of the<code>real</code> data type<br> - Optimize the design for parallel processing, leveraging Multiply-Accumulate (MAC) operations<br> - Enhance floating-point multiplication and division support for improved computational efficiency</td></tr></tbody></table><hr><h4 id="project-overview"><strong>Project Overview</strong></h4><p>I highly recommend checking out the main project, as this is just a subset. The main project focuses on image processing algorithms, and working on<a href="../">ImProVe (IMage PROcessing using VErilog)</a> has made the learning curve for this project much easier.</p>]]></description><content:encoded>&lt;![CDATA[<h2 id="never-neural-network-in-verilog"><a href="https://github.com/Mummanajagadeesh/NeVer" target="_blank">NeVer: NEural NEtwork in VERilog</a></h2><h5 id="do-checkout-main-project-improve-image-processing-using-verilog">Do Checkout Main Project:<strong><a href="http://mummanajagadeesh.github.io/projects/improve/" target="_blank">ImProVe: IMage PROcessing using VErilog</a></strong></h5><table><thead><tr><th><strong>Name</strong></th><th>NeVer</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>NeVer implements a neural network in Verilog for better hardware acceleration of image processing tasks</td></tr><tr><td><strong>Start</strong></td><td>28 Feb 2025</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/NeVer" target="_blank">NeVer🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Image Processing, HDL, Computer Vision, Programming, ML</td></tr><tr><td><strong>Tools Used</strong></td><td>Verilog, Icarus, Perl, TCL, Quartus, Python, NumPy</td></tr><tr><td><strong>Current Status</strong></td><td>Ongoing (Active)</td></tr><tr><td><strong>Progress</strong></td><td>- Implemented detection of MNIST digits (0-9)<br> - Added support for EMNIST, enabling classification of 62 character classes<br> - Integrated real-time inference with a Tkinter-based character drawing interface, achieving sub-5s latency per prediction in simulation</td></tr><tr><td><strong>Next Steps</strong></td><td>- Ensure the top module is synthesizable by eliminating the use of the<code>real</code> data type<br> - Optimize the design for parallel processing, leveraging Multiply-Accumulate (MAC) operations<br> - Enhance floating-point multiplication and division support for improved computational efficiency</td></tr></tbody></table><hr><h4 id="project-overview"><strong>Project Overview</strong></h4><p>I highly recommend checking out the main project, as this is just a subset. The main project focuses on image processing algorithms, and working on<a href="../">ImProVe (IMage PROcessing using VErilog)</a> has made the learning curve for this project much easier.</p><p>NeVer (Neural Network in Verilog) is one of my favorite subprojects under ImProVe. In this subproject, my goal is to implement a fully functional neural network purely in Verilog and optimize it for efficient hardware acceleration.</p><p>Please note that the names of these projects are not meant to be taken too seriously. The names like ImProVe or NeVer may not fully reflect their functions – ImProVe doesn’t actually improve images, but processes them, and NeVer isn&rsquo;t about something &ldquo;never-implemented&rdquo; – many have done it before. The names just make it easier for me to organize folders and code</p><h4 id="motivation"><strong>Motivation</strong></h4><p>I was inspired by this video:<a href="https://www.youtube.com/watch?v=w8yWXqWQYmU&amp;ab_channel=SamsonZhang" target="_blank">Building a neural network FROM SCRATCH (no TensorFlow/PyTorch, just NumPy &amp; math)</a> by<strong>Samson Zhang</strong>. The video demonstrates a simple 2 layer neural network for recognizing MNIST digits (0-9)</p><p>I expanded on this by:</p><ul><li>Using<strong>two hidden layers</strong> instead of none</li><li>Implementing<strong>Adam optimizer</strong> along with<strong>vanilla SGD</strong>, replacing basic gradient descent</li><li>Extending support for<strong>62 classes</strong> (0-9, A-Z, a-z)</li><li>Using<strong>Verilog</strong> for inference instead of Python</li><li>Incorporating<strong>Tkinter</strong> for manually inputting handwritten characters irl, allowing direct interaction with the trained model</li></ul><h4 id="project-roadmap"><strong>Project Roadmap</strong></h4><ol><li>Train a model using<strong>TensorFlow/PyTorch</strong> for quick validation → Infer in Python</li><li>Train using<strong>NumPy only</strong> → Infer in Python</li><li>Train using<strong>NumPy only</strong> → Infer in Verilog</li><li>Train using<strong>pure Python (no NumPy, user-defined functions)</strong> → Infer in Verilog</li><li>Implement a<strong>single neuron in Verilog</strong> for training</li><li>Train<strong>directly in Verilog</strong> → Infer in Verilog</li><li><strong>Optimize</strong> the implementation using<strong>parallel processing, MAC units, etc.</strong></li><li>Make the<strong>entire Verilog implementation synthesizable</strong></li></ol><hr><h4 id="current-status"><strong>Current Status</strong></h4><ul><li>Successfully performed inference in Verilog using parameters trained in<strong>Python</strong> (Colab) with<strong>NumPy</strong> , achieving<strong>>90% accuracy</strong> on the test dataset</li><li>Implemented inference for both<strong>MNIST (digits 0-9)</strong> and<strong>EMNIST (62 classes: 0-9, A-Z, a-z)</strong></li><li>Training with<strong>2000 iterations</strong>:<ul><li><strong>First 1500 iterations</strong>: Adam optimizer</li><li><strong>Last 500 iterations</strong>: Vanilla SGD (Reason: Adam converges faster initially, but switching to SGD helps refine convergence | Later switched to<strong>&ldquo;SGD with Momentum&rdquo;</strong>)</li></ul></li><li>Using<strong>Tkinter</strong> for drawing input characters, which are then processed and fed into Verilog for inference</li><li>Achieved<strong>&lt;5s inference latency</strong> per prediction in simulation with FSM-driven layer evaluation</li><li>Actively working on<strong>image reconstruction</strong> from NumPy-trained model parameters</li><li>Replacing<code>real</code><strong>(IEEE 754)</strong> in Verilog top module with fixed-point formats like<strong>Q32.32, Q32.16, Q24.8 using sfixed from IEEE 1076.3 (ieee.fixed_pkg)</strong><ul><li>Aiming for better synthesis compatibility, but currently observing<strong>accuracy loss</strong> due to quantization</li><li>Actively tuning<strong>fixed-point precision</strong> to balance accuracy and hardware performance</li></ul></li></ul><hr><h4 id="current-workflow"><strong>Current Workflow</strong></h4><ol><li><strong>Image Processing</strong>:<code>draw.py</code> → Converts the Tkinter drawing into<code>drawing.jpg</code></li><li><strong>Grayscale Image Conversion</strong>:<code>img2bin.py</code> → Generates<code>mnist_single_no.txt</code> (integer values from 0-255)</li><li><strong>Vectorization</strong>:<code>arr2row.v</code> → Flattens the<strong>2D 28×28</strong> matrix into<code>input_vector.txt</code></li><li><strong>Memory Preloading</strong>:<code>memloader_from_inp_vec.py</code> → Converts<code>input_vector.txt</code> into Verilog memory (<code>image_memory.v</code>)</li><li><strong>Weight &amp; Bias Preloading</strong>:<code>wtbs_loader.py</code> → Converts pretrained weight &amp; bias TXT files into Verilog memory (<code>W1_memory.v</code>,<code>b1_memory.v</code>, etc.)</li><li><strong>Inference in Verilog</strong>:<code>emnist_with_tb.v</code> → Top module:<code>emnist.v</code></li></ol><h5 id="folder-structure"><strong>Folder Structure</strong></h5><p>Can be found in the<code>/clean</code> directory under the emnist folders in the repo</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>.</span></span><span style="display:flex;"><span>├───bin/<span style="color:#75715e"># Intermediate image text files</span></span></span><span style="display:flex;"><span>│ └───*.txt</span></span><span style="display:flex;"><span>├───build/<span style="color:#75715e"># Compiled Verilog simulation output</span></span></span><span style="display:flex;"><span>│ └───*.vvp</span></span><span style="display:flex;"><span>├───data/<span style="color:#75715e"># Stores trained model parameters and images</span></span></span><span style="display:flex;"><span>│ ├───biases/<span style="color:#75715e"># Text files containing trained bias values</span></span></span><span style="display:flex;"><span>│ │ └───*.txt</span></span><span style="display:flex;"><span>│ ├───weights/<span style="color:#75715e"># Text files containing trained weight values</span></span></span><span style="display:flex;"><span>│ │ └───*.txt</span></span><span style="display:flex;"><span>│ └───*.png<span style="color:#75715e"># Image files</span></span></span><span style="display:flex;"><span>├───run/<span style="color:#75715e"># Setup and automation scripts for different platforms</span></span></span><span style="display:flex;"><span>│ ├───linux-or-macOs/<span style="color:#75715e"># Setup and automation scripts for Linux or macOS</span></span></span><span style="display:flex;"><span>│ │ └───Makefile<span style="color:#75715e"># Makefile for automating simulation/build steps</span></span></span><span style="display:flex;"><span>│ ├───perl/<span style="color:#75715e"># Scripts for automating simulation and clean-up (Perl)</span></span></span><span style="display:flex;"><span>│ │ └───*.pl</span></span><span style="display:flex;"><span>│ ├───tcl/<span style="color:#75715e"># Scripts for automating simulation and clean-up (TCL)</span></span></span><span style="display:flex;"><span>│ │ └───*.tcl</span></span><span style="display:flex;"><span>│ └───windows/<span style="color:#75715e"># Setup and automation scripts for Windows</span></span></span><span style="display:flex;"><span>│ └───*.bat</span></span><span style="display:flex;"><span>├───scripts/<span style="color:#75715e"># Python scripts for image preprocessing</span></span></span><span style="display:flex;"><span>│ └───*.py</span></span><span style="display:flex;"><span>├───sim/<span style="color:#75715e"># Test benches for verifying Verilog modules</span></span></span><span style="display:flex;"><span>│ └───*.v</span></span><span style="display:flex;"><span>└───src/<span style="color:#75715e"># Source Verilog code</span></span></span><span style="display:flex;"><span> ├───image/<span style="color:#75715e"># Image memory modules</span></span></span><span style="display:flex;"><span> │ └───*.v</span></span><span style="display:flex;"><span> ├───params/<span style="color:#75715e"># Memory modules for weights and biases</span></span></span><span style="display:flex;"><span> │ └───*.v</span></span><span style="display:flex;"><span> └───top/<span style="color:#75715e"># Top-level design module</span></span></span><span style="display:flex;"><span> └───*.v</span></span></code></pre></div><table><thead><tr><th><strong>Platform</strong></th><th><strong>Command</strong></th></tr></thead><tbody><tr><td><strong>Windows</strong></td><td><code>cd path-to-clean-dir/</code><br><code>./run/win32/setup.bat</code></td></tr><tr><td><strong>Linux/macOS</strong></td><td><code>cd path-to-clean-dir/run/</code><br><code>make</code></td></tr><tr><td><strong>Perl (Cross-Platform)</strong></td><td><code>cd path-to-clean-dir/</code><br><code>perl ./run/perl/workflow.pl</code></td></tr><tr><td><strong>Tcl (Cross-Platform)</strong></td><td><code>cd path-to-clean-dir/</code><br><code>tclsh ./run/tcl/workflow.tcl</code></td></tr></tbody></table><h5 id="command-workflow">Command Workflow</h5><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>cd path-to-clean-dir/<span style="color:#75715e"># navigate to your project directory</span></span></span><span style="display:flex;"><span>python ./scripts/draw.py<span style="color:#75715e"># opens a Tkinter interface to draw an image by hand</span></span></span><span style="display:flex;"><span>python ./scripts/img2bin.py<span style="color:#75715e"># converts the drawn image into a text-based binary file</span></span></span><span style="display:flex;"><span>iverilog -o ./build/arr2row.vvp ./src/arr2row.v<span style="color:#75715e"># compiles Verilog code that converts 2D image array into a 1D vector</span></span></span><span style="display:flex;"><span>vvp ./build/arr2row.vvp<span style="color:#75715e"># runs the compiled arr2row Verilog simulation</span></span></span><span style="display:flex;"><span>python ./scripts/wtbs_loader.py<span style="color:#75715e"># (only needed for first build) generates weight and bias memory Verilog files</span></span></span><span style="display:flex;"><span>python ./scripts/memloader_from_inp_vec.py<span style="color:#75715e"># loads the converted image vector into the image_memory module</span></span></span><span style="display:flex;"><span>iverilog -o build/prediction_test.vvp ./sim/*.v ./src/top/*.v ./src/image/*.v ./src/params/*.v<span style="color:#75715e"># compiles entire prediction design</span></span></span><span style="display:flex;"><span>vvp ./build/prediction_test.vvp<span style="color:#75715e"># runs the compiled prediction simulation</span></span></span></code></pre></div><h5 id="state-diagram-generated-using-graphviz-tool">State Diagram (generated using GraphViz tool)</h5><img title="" loading="lazy" decoding="async" class="img  " width="312" height="800" src="/images/projects/improve/never/fsm_hu_2c7319fa8741805d.webp" alt="FSM Mealy" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/fsm_hu_cce4f5e203857f46.png'"/><script>
window.addEventListener("load", (e) => {
const lightbox = GLightbox();
});</script><hr><h5 id="memory-initialization">Memory Initialization</h5><ul><li><strong>Structure &amp; Initialization</strong>: A simple<code>reg [WIDTH-1:0] mem [0:DEPTH-1];</code> array whose entries are hard‑coded in an<code>initial</code> block at elaboration time, causing the synthesis tool to bake those constants into the FPGA configuration bitstream citeturn0search6.</li><li><strong>Run‑Time Behavior</strong>: Acts as a pure, read‑only memory (ROM)—you present an address, and on the next clock (or combinationally, depending on inference) you get the stored value. No write ports or file‑I/O occur after configuration citeturn1search1.</li><li><strong>Implementation Styles</strong>:<ul><li><strong>Small arrays</strong> (tens to low hundreds of bits) typically map to<strong>distributed LUT‑RAM</strong>.</li><li><strong>Larger arrays</strong> (kilobits) are inferred as<strong>dedicated block RAM</strong> macros.</li></ul></li><li><strong>Why This Pattern</strong>: Embedding all data directly in RTL avoids reliance on external files and runtime system tasks like<code>$readmemh</code>,<code>$fopen</code> or custom parsers. That makes simulation setups simpler (no file‑path issues), keeps testbenches file‑agnostic, and ensures cross‑tool portability and deterministic initialization.</li></ul><h4 id="technical-details"><strong>Technical Details</strong></h4><ul><li>The<strong>top module</strong> (<code>emnist.v</code>) follows an<strong>FSM-based approach</strong> with minimal to no overlap</li><li><strong>Softmax Approximation</strong>: Using<strong>Taylor series expansion</strong> for exponentiation</li><li><strong>Pipeline Strategy</strong>:<ul><li><strong>Currently</strong>: Using<strong>coarse-grained pipelining</strong>, meaning major computation blocks execute sequentially with some latency</li><li><strong>Next Steps</strong>: Implement<strong>fine-grained pipelining</strong>, where smaller operations are parallelized for higher throughput</li></ul></li></ul><hr><h4 id="to-do"><strong>To-Do</strong></h4><ul><li>Implement<strong>LUTs</strong> for efficient exponential computation in Softmax</li><li>Remove the<strong><code>real</code> datatype</strong> in the top module to ensure full synthesizability</li><li>Extend support for<strong>all ASCII characters</strong> (optional)</li><li>Enable<strong>OCR functionality</strong> to detect any character in a given image</li><li>Optimize for<strong>parallel processing</strong>,<strong>better pipelining</strong>, and<strong>hardware acceleration</strong></li></ul><hr><h4 id="demos"><strong>Demos</strong></h4><p>Here are some demo videos</p><h5 id="mnist-digit-recognition"><strong>Mnist Digit Recognition</strong></h5><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/QtkdWSq25zQ" frameborder="0" allowfullscreen=/></div></div><p><br><br/><ul><li><p>I developed a<strong>fully connected neural network</strong> from scratch in<strong>Google Colab</strong>, avoiding frameworks like<strong>TensorFlow</strong> and<strong>Keras</strong>. Instead, I relied on<strong>NumPy</strong> for numerical operations,<strong>Pandas</strong> for data handling, and<strong>Matplotlib</strong> for visualization. The model was trained on<strong>sample_data/mnist_train_small.csv</strong>, a dataset containing flattened<strong>784-pixel images</strong> of handwritten digits. Data preprocessing included<strong>normalizing pixel values</strong> (dividing by<strong>255</strong>) and splitting the dataset into a<strong>training set</strong> and a<strong>development set</strong>, with the first<strong>1000 samples</strong> reserved for validation. The dataset was shuffled before training to enhance randomness, and labels (digits 0-9) were stored separately</p></li><li><p>The network architecture consists of an<strong>input layer (784 neurons)</strong>, a<strong>hidden layer (128 neurons, ReLU activation)</strong>, and an<strong>output layer (10 neurons, softmax activation)</strong>. Model parameters (weights and biases) were initialized randomly and updated via<strong>gradient descent</strong> over<strong>500 iterations</strong> with a learning rate of<strong>0.1</strong>. Training followed the standard<strong>forward propagation</strong> for computing activations and<strong>backpropagation</strong> for updating parameters. Accuracy was recorded every<strong>10 iterations</strong>. To ensure compatibility with<strong>Verilog</strong>, all weights and biases were<strong>scaled by 10,000</strong> and stored as<strong>integer values</strong> in text files (<code>W1.txt</code>,<code>b1.txt</code>, etc.), eliminating the need for<strong>floating-point operations</strong> in hardware. These trained parameters were later used for inference on new images, verifying accuracy on the<strong>development set</strong> before deployment in Verilog for real-time classification</p></li><li><p>The trained model, based on<strong>sample_data/mnist_train_small.csv</strong>, achieved<strong>over 90% accuracy</strong>. It generates<code>W1</code>,<code>W2</code>,<code>b1</code>, and<code>b2</code> text files containing weight and bias values. These parameters are used in Verilog to predict digits from an<strong>input image</strong> stored in<code>input_vector.txt</code>, formatted as<strong>784 space-separated integers</strong>. The Verilog module reads this data, performs inference, and displays the predicted output using<code>$display</code>. The original CSV file was converted into a<strong>space-separated text format</strong>, where each row contains a digit followed by<strong>784 pixel values (785 total)</strong>. During inference, the first value (label) is discarded, ensuring the model classifies the input image without prior knowledge of its actual label</p></li><li><p>The<strong>Verilog implementation</strong> of the neural network consists of an<strong>input layer (784 neurons)</strong>, a<strong>hidden layer (128 neurons)</strong>, and an<strong>output layer (10 neurons)</strong>. It loads<strong>pre-trained weights and biases</strong> from<code>W1.txt</code>,<code>b1.txt</code>,<code>W2.txt</code>, and<code>b2.txt</code>, along with an<strong>input vector</strong> from<code>input_vector.txt</code>. Input values are<strong>normalized</strong> by dividing by<strong>255.0</strong>, while weights and biases are<strong>scaled by 10,000</strong> for fixed-point arithmetic. The hidden layer applies a<strong>fully connected transformation</strong> (<code>W1 * input + b1</code>) followed by<strong>ReLU activation</strong>, while the output layer computes another weighted sum (<code>W2 * hidden + b2</code>). Instead of applying softmax, the model identifies the predicted class by selecting the index of the<strong>highest output value</strong></p></li><li><p>The module ensures correct file reading before computation begins. Forward propagation is executed sequentially, with an initial delay for<strong>loading weights, biases, and input values</strong>. After processing activations in both layers, the output layer iterates through its neurons to determine the class with the highest activation. The classification result is displayed via<code>$display</code>. This hardware implementation<strong>bypasses complex activation functions</strong> like softmax while maintaining classification accuracy through direct maximum-value selection</p></li><li><p>In the latest iterations,<strong>Python scripts</strong> convert text-based weight and bias files into<strong>synthesizable Verilog memory blocks</strong>. These are stored in<strong>register modules</strong>, which are instantiated in the<strong>top-level module</strong>. The image input is handled in a similar way</p></li><li><p>Currently, the<strong>top module</strong> includes a few non-synthesizable constructs, such as<code>$display</code>,<code>$finish</code>, and the<strong><code>real</code> datatype</strong>. These were relocated to the testbench in later versions to improve synthesizability. Additionally, I am replacing<code>real</code> with a<strong>fixed-point representation (Q24.8)</strong> to make the design fully synthesizable. Future versions will output the classification result to a<strong>seven-segment display</strong> via<strong>case statements</strong>, replacing<code>$display</code></p></li><li><p>Moving forward, I am working on transitioning<strong>training from Python to Verilog</strong>, aiming to implement a fully<strong>synthesizable neural network</strong> for hardware-based learning and inference</p></li></ul><hr><h5 id="emnist-character-recognition"><strong>EMNIST Character Recognition</strong></h5><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/7YccFUtydM0" frameborder="0" allowfullscreen=/></div></div><p><br><br/><p>This model is trained on the<strong>EMNIST ByClass dataset</strong> (<a href="https://greg-cohen.com/datasets/emnist/" target="_blank">source</a>), which includes<strong>62 character classes</strong>—digits (<code>0-9</code>), uppercase letters (<code>A-Z</code>), and lowercase letters (<code>a-z</code>). The dataset undergoes preprocessing, where it is converted into a<strong>CSV format</strong>, normalized, reduced in dimensionality, and shuffled before training to improve generalization</p><h6 id="neural-network-architecture"><strong>Neural Network Architecture</strong></h6><p>The model consists of multiple layers:</p><ul><li><strong>Input Layer</strong>:<strong>784 neurons</strong> (28×28 grayscale pixel values)</li><li><strong>First Hidden Layer</strong>:<strong>256 neurons</strong> (<code>W1: 256×784</code>,<code>b1: 256×1</code>)</li><li><strong>Second Hidden Layer</strong>:<strong>128 neurons</strong> (<code>W2: 128×256</code>,<code>b2: 128×1</code>)</li><li><strong>Output Layer</strong>:<strong>62 neurons</strong> (<code>W3: 62×128</code>,<code>b3: 62×1</code>)</li></ul><h6 id="training-process"><strong>Training Process</strong></h6><p>The network is trained using<strong>forward propagation</strong>, where activations are computed at each layer through<strong>matrix multiplications</strong> and<strong>ReLU activation functions</strong> for hidden layers.<strong>Backpropagation</strong> is used to update weights based on the gradient of the loss function. The dataset is shuffled before each epoch to prevent overfitting. The model is trained over multiple epochs using a combination of<strong>Stochastic Gradient Descent (SGD)</strong> and the<strong>Adam optimizer</strong> to improve convergence</p><p>To ensure compatibility with hardware, weights and biases are<strong>scaled by 10,000</strong> and stored as integers in text files (<code>W1.txt</code>,<code>b1.txt</code>, etc.), since Verilog does not support floating-point arithmetic</p><h6 id="inference-in-verilog"><strong>Inference in Verilog</strong></h6><p>The inference process in Verilog follows a similar structure but accommodates additional layers and character classes. Input images are read from<code>input_vector.txt</code>, normalized, and processed through the neural network using preloaded weights and biases. The computation follows:</p><ul><li><code>hidden1 = ReLU(W1 * input + b1)</code></li><li><code>hidden2 = ReLU(W2 * hidden1 + b2)</code></li><li><code>output = W3 * hidden2 + b3</code></li></ul><p>The<strong>index of the maximum output value</strong> corresponds to the predicted character, which is mapped to<code>"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"</code> and displayed using<code>$display</code></p><hr><h6 id="pipeline-and-data-flow"><strong>Pipeline and Data Flow</strong></h6><p>The following scripts handle image processing, vectorization, and memory loading for Verilog inference:</p><ol><li><strong>Drawing &amp; Image Processing</strong>:<ul><li><code>draw.py</code>: Creates a square canvas in Tkinter for character input</li><li>After drawing, the script<strong>grayscales, inverts, compresses</strong> the image to<strong>28×28 resolution</strong>, and saves it as<code>drawing.jpg</code></li></ul></li><li><strong>Data Conversion &amp; Preprocessing</strong>:<ul><li><code>img2bin.py</code>: Converts<code>drawing.jpg</code> into a<strong>28×28</strong> grayscale pixel matrix (<code>mnist_single_no.txt</code>)</li><li><code>arr2row.py</code>: Flattens the<strong>2D array</strong> into a<strong>1D vector (784 values)</strong> and stores it in<code>input_vector.txt</code></li></ul></li><li><strong>Memory Module Generation</strong>:<ul><li><code>memloader_from_inp_vec.py</code>: Converts<code>input_vector.txt</code> into a<strong>synthesizable Verilog memory module</strong> (<code>image_memory.v</code>)</li><li><code>wtbs_loader.py</code>: Converts<code>W1</code>,<code>W2</code>,<code>W3</code>,<code>b1</code>,<code>b2</code>,<code>b3</code> into Verilog memory modules (<code>W1_memory.v</code>,<code>b1_memory.v</code>, etc.)</li></ul></li></ol><h6 id="final-hardware-implementation"><strong>Final Hardware Implementation</strong></h6><p>All these components are instantiated in the<strong>top module</strong> (<code>emnist_with_tb.v</code>), along with a<strong>testbench</strong> (<code>emnist_nn_tb.v</code>). The system successfully predicts handwritten characters in real-time</p><p>In the<strong>demo</strong>, I tested the characters<strong>&ldquo;H&rdquo;</strong>,<strong>&ldquo;f&rdquo;</strong>, and<strong>&ldquo;7&rdquo;</strong>, each representing different EMNIST subclasses (uppercase letters, lowercase letters, and numbers)</p><p>Additionally, I implemented a<strong>coarse-grained pipelined</strong> fully connected neural network using a<strong>Finite State Machine (FSM)</strong>, integrating a<strong>Softmax function approximation</strong> via<strong>Taylor series expansion</strong> to improve computational efficiency</p><h4 id="raw-demo-shots"><strong>Raw Demo Shots</strong></h4><table><thead><tr><th><strong>UpperCase Alphabet</strong></th><th><strong>LowerCase Alphabet</strong></th><th><strong>Single Digit Number</strong></th></tr></thead><tbody/></table><table><thead><tr><th><strong>Actual: R</strong></th><th><strong>Prediction: R</strong></th><th><strong>Actual: i</strong></th><th><strong>Prediction: i</strong></th><th><strong>Actual: 9</strong></th><th><strong>Prediction: 9</strong></th></tr></thead><tbody/></table><table><thead><tr><th><img title="" loading="lazy" decoding="async" class="img  " width="300" height="" src="/images/projects/improve/never/R.gif" alt="R" onerror="this.onerror='null';this.src=''"/></th><th><img title="" loading="lazy" decoding="async" class="img  " width="300" height="" src="/images/projects/improve/never/i.gif" alt="i" onerror="this.onerror='null';this.src=''"/></th><th><img title="" loading="lazy" decoding="async" class="img  " width="300" height="" src="/images/projects/improve/never/9.gif" alt="9" onerror="this.onerror='null';this.src=''"/></th></tr></thead><tbody/></table><hr><p><br><br/><h4 id="pre-processing-workflow"><strong>Pre-processing Workflow</strong></h4><table><thead><tr><th><strong>Original Drawing</strong></th><th><strong>Grayscale Image</strong></th><th><strong>Inverted Image</strong></th><th><strong>Text Matrix</strong></th></tr></thead><tbody><tr><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="300" src="/images/projects/improve/never/drawing_hu_d0025e211bebb1d4.webp" alt="Org Drawing" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/drawing_hu_aa607f95db2e4166.png'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="300" src="/images/projects/improve/never/drawing_gray_hu_1aa6c217779fa83f.webp" alt="Grayscale Image" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/drawing_gray_hu_1ec3127228ae2c9f.png'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="300" src="/images/projects/improve/never/drawing_inverted_hu_5998a1c57c3dc469.webp" alt="Inverted Image" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/drawing_inverted_hu_1771a2c107cd69a2.png'"/></td><td><img title="" loading="lazy" decoding="async" class="img  " width="300" height="299" src="/images/projects/improve/never/drawing_txt_hu_763c9a05a044497f.webp" alt="Text Matrix" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/drawing_txt_hu_e93f287e5d6bbf4a.png'"/></td></tr></tbody></table><h4 id="image-reconstruction-trials"><strong>Image Reconstruction Trials</strong></h4><p>Image reconstruction works by taking random noise as the initial seed image and passing it through layers formed with trained parameters, iteratively minimizing the loss. This way, we gradually obtain the average image for each digit. Currently, I&rsquo;m working on refining the process in Python with MNIST data, which will eventually be translated into Verilog. The process is taking a significant amount of time per image (even in Python), so you can imagine the challenge in Verilog. I’m still testing the Verilog implementation and aiming to optimize it to achieve better speed performance compared to the Python version.</p><img title="" loading="lazy" decoding="async" class="img  " width="900" height="436" src="/images/projects/improve/never/imgen_hu_956f3ec8d974c730.webp" alt="image-recon" onerror="this.onerror='null';this.src='\/images\/projects\/improve\/never\/imgen_hu_6c7dd18bc41a507d.png'"/>
]]></content:encoded></item><item><title/><link>https://mummanajagadeesh.github.io/projects/pidc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/projects/pidc/</guid><description>&lt;![CDATA[<h2 id="pidc---pid-controller-using-opamps"><a href="https://github.com/Mummanajagadeesh/PIDC_CTRL" target="_blank">PIDC - PID Controller using OpAmps</a></h2><table><thead><tr><th><strong>Name</strong></th><th>PIDC</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>Implemented a PID controller using operational amplifiers to regulate system response and maintain desired performance. The design leverages analog circuitry to achieve precise control over error correction and stability.</td></tr><tr><td><strong>Start</strong></td><td>Sep 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/PIDC_CTRL" target="_blank">PIDC🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Simulation , OpAmps</td></tr><tr><td><strong>Tools Used</strong></td><td>LtSpice</td></tr><tr><td><strong>Current Status</strong></td><td>On Hold</td></tr></tbody></table><hr><h4 id="pid-controller-theory-and-hardware-realization-using-op-amps"><strong>PID Controller: Theory and Hardware Realization Using Op-Amps</strong></h4><p>A<strong>PID (Proportional-Integral-Derivative) controller</strong> is a fundamental tool in control systems, widely used in industrial automation, robotics, temperature control, and motor speed regulation. It is designed to<strong>minimize error</strong> and<strong>improve stability</strong> by adjusting a system’s input based on the error between the desired setpoint and the actual output.</p>]]></description><content:encoded>&lt;![CDATA[<h2 id="pidc---pid-controller-using-opamps"><a href="https://github.com/Mummanajagadeesh/PIDC_CTRL" target="_blank">PIDC - PID Controller using OpAmps</a></h2><table><thead><tr><th><strong>Name</strong></th><th>PIDC</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>Implemented a PID controller using operational amplifiers to regulate system response and maintain desired performance. The design leverages analog circuitry to achieve precise control over error correction and stability.</td></tr><tr><td><strong>Start</strong></td><td>Sep 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/PIDC_CTRL" target="_blank">PIDC🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Simulation , OpAmps</td></tr><tr><td><strong>Tools Used</strong></td><td>LtSpice</td></tr><tr><td><strong>Current Status</strong></td><td>On Hold</td></tr></tbody></table><hr><h4 id="pid-controller-theory-and-hardware-realization-using-op-amps"><strong>PID Controller: Theory and Hardware Realization Using Op-Amps</strong></h4><p>A<strong>PID (Proportional-Integral-Derivative) controller</strong> is a fundamental tool in control systems, widely used in industrial automation, robotics, temperature control, and motor speed regulation. It is designed to<strong>minimize error</strong> and<strong>improve stability</strong> by adjusting a system’s input based on the error between the desired setpoint and the actual output.</p><h4 id="why-introduce-the-pid-controller"><strong>Why Introduce the PID Controller?</strong></h4><p>Before jumping straight into PID, it&rsquo;s useful to start with simpler controllers and gradually add complexity. This helps in understanding<strong>why</strong> each term (P, I, D) is necessary and how they contribute to system performance.</p><ul><li><strong>Proportional (P) Control</strong> – Reacts to the current error but does not eliminate steady-state error.</li><li><strong>Integral (I) Control</strong> – Eliminates steady-state error by integrating past errors.</li><li><strong>Derivative (D) Control</strong> – Predicts future errors by considering the rate of change.</li></ul><p>This structured approach builds an<strong>intuitive understanding</strong> of why each term is needed before implementing them in hardware.</p><h4 id="mathematical-formulation-of-pid"><strong>Mathematical Formulation of PID</strong></h4><p>The<strong>PID control law</strong> in the time domain is:</p><p>$$
u(t) = K_p e(t) + K_i \int e(t) dt + K_d \frac{d e(t)}{dt}
$$</p><p>where:</p><ul><li>(e(t) = r(t) y(t)) (error between the desired setpoint (r(t)) and actual output (y(t))).</li><li>(K_p) is the<strong>proportional gain</strong> (how much we react to the present error).</li><li>(K_i) is the<strong>integral gain</strong> (how much we consider past errors).</li><li>(K_d) is the<strong>derivative gain</strong> (how much we predict future errors).</li></ul><p>In the<strong>Laplace domain</strong>, the PID transfer function is:</p><p>$$
U(s) = \left( K_p + \frac{K_i}{s} + K_d s \right) E(s)
$$</p><p>which represents the combined effect of<strong>P, I, and D</strong> on the system.</p><h4 id="hardware-realization-of-pid-using-op-amps"><strong>Hardware Realization of PID Using Op-Amps</strong></h4><p>Op-amps are ideal for<strong>analog PID implementation</strong> because they can easily perform<strong>amplification, integration, and differentiation</strong> using simple resistor-capacitor networks.</p><h6 id="proportional-p-controller-using-op-amps"><strong>Proportional (P) Controller Using Op-Amps</strong></h6><p>A proportional controller applies a gain (K_p) to the input error signal. This can be implemented using a<strong>non-inverting op-amp configuration</strong>:</p><p><strong>Circuit:</strong><br>
Use an<strong>op-amp in non-inverting mode</strong> with a feedback resistor (R_f) and input resistor (R_1).</p><p><strong>Gain Equation:</strong><br>
$$
K_p = 1 + \frac{R_f}{R_1}
$$</p><p><strong>Transfer Function:</strong><br>
$$
V_{out}(s) = K_p V_{in}(s)
$$</p><p><strong>Effect:</strong> Improves response speed but does not eliminate steady-state error.</p><h6 id="integral-i-controller-using-op-amps"><strong>Integral (I) Controller Using Op-Amps</strong></h6><p>The integral action sums past error signals over time, eliminating steady-state error. This is implemented using an<strong>op-amp integrator</strong>.</p><p><strong>Circuit:</strong></p><ul><li>Replace the feedback resistor with a<strong>capacitor (C)</strong>.</li><li>A resistor (R) is placed in the input path.</li></ul><p><strong>Transfer Function:</strong><br>
$$
V_{out}(s) = \frac{K_i}{s} V_{in}(s)
$$</p><p>where (K_i = \frac{1}{RC}).</p><p><strong>Effect:</strong> Eliminates steady-state error but may cause slow response or instability.</p><h6 id="derivative-d-controller-using-op-amps"><strong>Derivative (D) Controller Using Op-Amps</strong></h6><p>A differentiator predicts future errors by computing the rate of change of the input signal. This is implemented using an<strong>op-amp differentiator</strong>.</p><p><strong>Circuit:</strong><br>
Use a<strong>capacitor (C) at the input</strong> and a<strong>resistor (R) in the feedback loop</strong>.</p><p><strong>Transfer Function:</strong><br>
$$
V_{out}(s) = K_d s V_{in}(s)
$$</p><p>where (K_d = R C).</p><p><strong>Effect:</strong> Improves stability and damping but is sensitive to noise.</p><h4 id="from-p-to-pi-to-pid-full-implementation"><strong>From P to PI to PID: Full Implementation</strong></h4><h6 id="pi-controller-proportional--integral"><strong>PI Controller (Proportional + Integral)</strong></h6><p>To combine<strong>P and I</strong>, sum the outputs of the proportional and integral circuits. The transfer function becomes:</p><p>$$
U(s) = K_p E(s) + \frac{K_i}{s} E(s)
$$</p><p><strong>Implementation:</strong><br>
Use a<strong>summing amplifier</strong> to combine the outputs of the P and I circuits.</p><h6 id="pid-controller-proportional--integral--derivative"><strong>PID Controller (Proportional + Integral + Derivative)</strong></h6><p>The full<strong>PID transfer function</strong>:</p><p>$$
U(s) = \left( K_p + \frac{K_i}{s} + K_d s \right) E(s)
$$</p><p><strong>Hardware Implementation Steps:</strong></p><ol><li><strong>Sum the outputs</strong> of the P, I, and D circuits using an<strong>op-amp summing amplifier</strong>.</li><li><strong>Adjust gains</strong> (K_p, K_i, K_d) by selecting appropriate resistor and capacitor values.</li><li><strong>Fine-tune</strong> the component values based on system response.</li></ol><h4 id="summary-of-pid-hardware-implementation"><strong>Summary of PID Hardware Implementation</strong></h4><table><thead><tr><th>Controller</th><th>Circuit Type</th><th>Transfer Function</th><th>Key Components</th></tr></thead><tbody><tr><td><strong>P</strong></td><td>Non-inverting amplifier</td><td>(K_p)</td><td>(R_f, R_1)</td></tr><tr><td><strong>I</strong></td><td>Integrator</td><td>(\frac{K_i}{s})</td><td>(R, C)</td></tr><tr><td><strong>D</strong></td><td>Differentiator</td><td>(K_d s)</td><td>(R, C)</td></tr><tr><td><strong>PI</strong></td><td>Summing amplifier of P and I</td><td>(K_p + \frac{K_i}{s})</td><td>Combination of P and I circuits</td></tr><tr><td><strong>PID</strong></td><td>Summing amplifier of P, I, and D</td><td>(K_p + \frac{K_i}{s} + K_d s)</td><td>Combination of P, I, and D circuits</td></tr></tbody></table><h4 id="conclusion-and-next-steps"><strong>Conclusion and Next Steps</strong></h4><p><strong>Why use Op-Amps?</strong></p><ul><li>Fast response time.</li><li>Continuous-time operation.</li><li>Lower power consumption compared to digital implementations.</li></ul><p><strong>Practical Considerations:</strong></p><ul><li>Noise filtering is required for the derivative term (D).</li><li>Proper tuning of (K_p, K_i, K_d) is needed for optimal performance.</li></ul><hr><h3 id="pi-controller-with-square-wave-input">PI Controller with Square Wave Input</h3><h4 id="circuit-diagram">Circuit Diagram</h4><img title="" loading="lazy" decoding="async" class="img  " width="900" height="518" src="/images/projects/pidc/PIDC_sq_hu_5a36f86b37e09eb6.webp" alt="Circuit Diagram" onerror="this.onerror='null';this.src='\/images\/projects\/pidc\/PIDC_sq_hu_96572113b3535b8d.png'"/><script>
window.addEventListener("load", (e) => {
const lightbox = GLightbox();
});</script><h4 id="setpoint">Setpoint</h4><img title="" loading="lazy" decoding="async" class="img  " width="900" height="213" src="/images/projects/pidc/sq_setpoint_hu_e85dacc33f0796e6.webp" alt="Setpoint" onerror="this.onerror='null';this.src='\/images\/projects\/pidc\/sq_setpoint_hu_6b7a6be26adb5d9.png'"/><h4 id="output-at-differential-amplifier">Output at Differential Amplifier</h4><img title="" loading="lazy" decoding="async" class="img  " width="900" height="215" src="/images/projects/pidc/sq_diff_out_hu_a716bceab7dedde3.webp" alt="Output at Diff Amp" onerror="this.onerror='null';this.src='\/images\/projects\/pidc\/sq_diff_out_hu_83f52599ed483c3b.png'"/><h4 id="after-integral">After Integral</h4><img title="" loading="lazy" decoding="async" class="img  " width="900" height="216" src="/images/projects/pidc/sq_int_hu_f80b3693eb5080af.webp" alt="After Integral" onerror="this.onerror='null';this.src='\/images\/projects\/pidc\/sq_int_hu_135e03878d070471.png'"/><h4 id="final-output">Final Output</h4><img title="" loading="lazy" decoding="async" class="img  " width="900" height="217" src="/images/projects/pidc/sq_final_hu_83bba444fc8a1b2f.webp" alt="Final Output" onerror="this.onerror='null';this.src='\/images\/projects\/pidc\/sq_final_hu_1ccdf07ee577b73b.png'"/><hr><h3 id="pid-controller-transient-analysis">PID Controller Transient Analysis</h3><h4 id="circuit">Circuit</h4><img title="" loading="lazy" decoding="async" class="img  " width="900" height="497" src="/images/projects/pidc/pidc_full_hu_392c88a2b2c9d077.webp" alt="Circuit" onerror="this.onerror='null';this.src='\/images\/projects\/pidc\/pidc_full_hu_8cb559604a486a69.png'"/><h4 id="response">Response</h4><img title="" loading="lazy" decoding="async" class="img  " width="900" height="216" src="/images/projects/pidc/pidc_tran_hu_cd373c1c1aa25e3c.webp" alt="Response" onerror="this.onerror='null';this.src='\/images\/projects\/pidc\/pidc_tran_hu_ed21459c6811b36c.png'"/>
]]></content:encoded></item><item><title/><link>https://mummanajagadeesh.github.io/projects/prosarm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/projects/prosarm/</guid><description>&lt;![CDATA[<h2 id="pr057h371c4rm"><a href="https://github.com/Mummanajagadeesh/PR057H371C4RM" target="_blank">PR057H371C4RM</a></h2><table><thead><tr><th><strong>Name</strong></th><th>PR057H371C4RM</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>A simple prosthetic arm that utilizes servo motors to create tension in strings, replicating the function of human tendons to achieve realistic finger motion.</td></tr><tr><td><strong>Start</strong></td><td>Ideation(2018), Implementation(Nov 2023)</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/PR057H371C4RM" target="_blank">PR057H371C4RM🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Mechanical Design, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Blender, Fusion 360, Tinkercad, Wokwi, VS Code, OpenCV, MediaPipe, Python, C++</td></tr><tr><td><strong>Current Status</strong></td><td>Ongoing (Passive)</td></tr><tr><td><strong>Progress</strong></td><td>- Mechanical model complete with updated tolerances.<br> - Finger tracking using MediaPipe is fully functional.<br> - Servo control code is operational.</td></tr><tr><td><strong>Next Steps</strong></td><td>- Analyze weight distribution.<br> - Reevaluate strength and agility for optimization.<br> - 3D print and assemble the prosthetic arm.<br> - Deploy the system on hardware.</td></tr></tbody></table><hr><h4 id="overview"><strong>Overview</strong></h4><p>PR057H371C4RM is a<strong>biomechanical prosthetic arm</strong> designed to replicate the<strong>natural movement and structure of a human hand</strong> as closely as possible. The project focuses on affordability, accessibility, and precision in design, making it a viable option for those who need a functional mechanical replacement for a lost limb.</p>]]></description><content:encoded>&lt;![CDATA[<h2 id="pr057h371c4rm"><a href="https://github.com/Mummanajagadeesh/PR057H371C4RM" target="_blank">PR057H371C4RM</a></h2><table><thead><tr><th><strong>Name</strong></th><th>PR057H371C4RM</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>A simple prosthetic arm that utilizes servo motors to create tension in strings, replicating the function of human tendons to achieve realistic finger motion.</td></tr><tr><td><strong>Start</strong></td><td>Ideation(2018), Implementation(Nov 2023)</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/PR057H371C4RM" target="_blank">PR057H371C4RM🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Mechanical Design, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Blender, Fusion 360, Tinkercad, Wokwi, VS Code, OpenCV, MediaPipe, Python, C++</td></tr><tr><td><strong>Current Status</strong></td><td>Ongoing (Passive)</td></tr><tr><td><strong>Progress</strong></td><td>- Mechanical model complete with updated tolerances.<br> - Finger tracking using MediaPipe is fully functional.<br> - Servo control code is operational.</td></tr><tr><td><strong>Next Steps</strong></td><td>- Analyze weight distribution.<br> - Reevaluate strength and agility for optimization.<br> - 3D print and assemble the prosthetic arm.<br> - Deploy the system on hardware.</td></tr></tbody></table><hr><h4 id="overview"><strong>Overview</strong></h4><p>PR057H371C4RM is a<strong>biomechanical prosthetic arm</strong> designed to replicate the<strong>natural movement and structure of a human hand</strong> as closely as possible. The project focuses on affordability, accessibility, and precision in design, making it a viable option for those who need a functional mechanical replacement for a lost limb.</p><p>Each<strong>finger</strong> has been modeled with accurate anatomical proportions to mimic human biomechanics. The design incorporates<strong>tendons (strings), ligaments (rubber cords), and joints (pivot points)</strong> to replicate<strong>natural flexion and extension</strong>. The<strong>rubber cord system</strong> ensures the fingers return to their original position after movement, while<strong>servo motors control flexion</strong>, allowing for precise and lifelike motion.</p><p>This project has been my<strong>dream</strong>, inspired by my passion for<strong>animatronics and robotics</strong>. I discovered<em>Will Cogley&rsquo;s</em> work on YouTube, which motivated me to learn<strong>Fusion 360</strong> to model the entire hand from scratch. Due to budget constraints, I have not yet been able to fully 3D print and assemble the final prototype, but the design and simulation have been extensively tested.</p><hr><h4 id="project-inspiration--resources"><strong>Project Inspiration &amp; Resources</strong></h4><ul><li><strong>3D Printing Service:</strong><a href="https://robu.in/product/3d-printing-service/" target="_blank">Robu</a></li><li><strong>CAD Files:</strong><ul><li><a href="https://grabcad.com/library/sg90-micro-servo-9g-tower-pro-1" target="_blank">SG90 Micro Servo</a></li><li><a href="https://grabcad.com/library/arduino-uno-r3-1" target="_blank">Arduino UNO R3</a></li></ul></li><li><strong>Project Inspiration:</strong><a href="https://www.youtube.com/@WillCogley" target="_blank">Will Cogley</a></li><li><strong>Bulk Export Add-ons:</strong><a href="https://github.com/Mummanajagadeesh/Project-Archiver" target="_blank">Project-Archiver</a></li></ul><hr><h4 id="demo-videos"><strong>Demo Videos</strong></h4><h5 id="latest-prototype"><strong>Latest Prototype</strong></h5><table><thead><tr><th>Motion Study</th><th>Assembly</th></tr></thead><tbody><tr><td><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/mATUY7Tn4Is" frameborder="0" allowfullscreen=/></div></div></td><td><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/IqjxZRdiDbM" frameborder="0" allowfullscreen=/></div></div></td></tr></tbody></table><h5 id="here-are-few-clicks">Here are few clicks</h5><p><img title="" loading="lazy" decoding="async" class="img img-75 " width="1920" height="1080" src="/images/projects/prosarm/F-view_hu_52ad98bdcd216c28.webp" alt="Front View" onerror="this.onerror='null';this.src='\/images\/projects\/prosarm\/F-view_hu_fec400fd9dc96dda.png'"/><script>
window.addEventListener("load", (e) => {
const lightbox = GLightbox();
});</script><br><img title="" loading="lazy" decoding="async" class="img img-75 " width="1920" height="1080" src="/images/projects/prosarm/full-arm-f_hu_bfccf3d42652305.webp" alt="Full Arm" onerror="this.onerror='null';this.src='\/images\/projects\/prosarm\/full-arm-f_hu_ba78469f1854223a.png'"/><br><img title="" loading="lazy" decoding="async" class="img img-75 " width="1920" height="1080" src="/images/projects/prosarm/full-workspace-all_hu_9de76de3980ffbee.webp" alt="Full Workspace" onerror="this.onerror='null';this.src='\/images\/projects\/prosarm\/full-workspace-all_hu_664937326571a59.png'"/></p><p><img title="" loading="lazy" decoding="async" class="img img-75 " width="1920" height="1080" src="/images/projects/prosarm/full-palm_hu_8acddcabd186486c.webp" alt="Full Palm" onerror="this.onerror='null';this.src='\/images\/projects\/prosarm\/full-palm_hu_e925d04b4cb5a19.png'"/><img title="" loading="lazy" decoding="async" class="img img-75 " width="1920" height="1080" src="/images/projects/prosarm/finger_hu_b0440099b3ef0e9c.webp" alt="Finger" onerror="this.onerror='null';this.src='\/images\/projects\/prosarm\/finger_hu_920723a1a601016d.png'"/></p><hr><h4 id="hand-anatomy--biomechanics-in-the-design"><strong>Hand Anatomy &amp; Biomechanics in the Design</strong></h4><p>The prosthetic hand is designed to closely resemble the<strong>biological structure of a human hand</strong>, including key anatomical components:</p><ol><li><p><strong>Phalanges (Finger Bones)</strong> – Each finger consists of<strong>three segments</strong>:</p><ul><li><strong>Distal Phalanx</strong> (tip of the finger)</li><li><strong>Middle Phalanx</strong></li><li><strong>Proximal Phalanx</strong> (connects to the palm)<br>
The<strong>thumb</strong> has only two phalanges (proximal and distal).</li></ul></li><li><p><strong>Metacarpals (Palm Bones)</strong> – The palm structure is designed to support all fingers and provide stability.</p></li><li><p><strong>Tendons (Strings/Twine)</strong> – Artificial tendons<strong>run through the palm and wrist</strong>, connecting to servo motors in the arm.</p></li><li><p><strong>Ligaments (Rubber Cords)</strong> – The rubber cords<strong>act like ligaments</strong>, maintaining the natural tension of the fingers.</p></li></ol><h5 id="finger-structure--naming-convention"><strong>Finger Structure &amp; Naming Convention</strong></h5><p>Each finger is labeled as follows:</p><p><strong>IX</strong> – Index<br><strong>MX</strong> – Middle<br><strong>RX</strong> – Ring<br><strong>PX</strong> – Pinky<br><strong>TX</strong> – Thumb</p><h6 id="segment-breakdown"><strong>Segment Breakdown</strong></h6><p>Each finger consists of multiple segments:</p><ul><li><strong>IX (Index)</strong> → I1 (Distal), I2 (Middle), I3 (Proximal)</li><li><strong>MX (Middle)</strong> → M1 (Distal), M2 (Middle), M3 (Proximal)</li><li><strong>RX (Ring)</strong> → R1 (Distal), R2 (Middle), R3 (Proximal)</li><li><strong>PX (Pinky)</strong> → P1 (Distal), P2 (Middle), P3 (Proximal)</li><li><strong>TX (Thumb)</strong> → T1 (Distal), T2 (Proximal)</li></ul><p><strong>Segment Definitions:</strong></p><ul><li><strong>Distal (1):</strong> Fingertip section</li><li><strong>Middle (2):</strong> Intermediate joint section</li><li><strong>Proximal (3):</strong> Base section connecting to the palm (except for the thumb, which has only two segments)</li></ul><hr><h4 id="finger-mechanism"><strong>Finger Mechanism</strong></h4><p>Each<strong>finger</strong> consists of<strong>three segments</strong> connected by pivot points, allowing for<strong>natural bending and extension</strong>.</p><ul><li><p><strong>Rubber Cord System:</strong></p><ul><li>A<strong>rubber cord runs twice through each finger</strong>, starting from the<strong>bottom</strong>, looping through the<strong>tip (making a U-turn)</strong>, and returning to the<strong>bottom</strong> again.</li><li>Both ends of the cord are<strong>secured at the back of the palm</strong>, ensuring controlled movement and restoring fingers to a neutral position.</li></ul></li><li><p><strong>String Control System:</strong></p><ul><li>A<strong>string runs through each finger</strong>, with a<strong>knot at the fingertip</strong> to create movement when pulled.</li><li>The<strong>strings pass through the palm, wrist, and arm</strong>, connecting to<strong>servo motors</strong> for actuation.</li></ul></li></ul><h5 id="how-it-works"><strong>How It Works</strong></h5><ol><li><strong>At rest</strong>, the rubber cords keep the fingers straight, aligning all segments.</li><li><strong>When a servo pulls a string</strong>, the corresponding finger bends.</li><li><strong>Once tension is released</strong>, the rubber cord returns the finger to its original position.</li><li><strong>Fingers can only move forward and back, preventing unnatural backward bending.</strong></li></ol><hr><h4 id="thumb-mechanism"><strong>Thumb Mechanism</strong></h4><p>The<strong>thumb</strong> requires two independent servos for realistic movement:</p><ol><li><strong>Palm Servo</strong> – Moves the thumb<strong>up and down</strong> (abduction/adduction).</li><li><strong>Arm Servo</strong> – Controls the<strong>folding motion</strong> (flexion/extension).</li></ol><p>This dual-motor setup allows the thumb to<strong>grip objects naturally</strong> when combined with finger movement.</p><hr><h4 id="palm--string-routing"><strong>Palm &amp; String Routing</strong></h4><ul><li><strong>Holes in the palm</strong> allow rubber cords to be secured at the back, where the fingers intersect.</li><li><strong>Channels through the palm</strong> guide the control strings from the fingertips through the wrist to the servos in the arm.</li></ul><hr><h4 id="servo-control--electronics"><strong>Servo Control &amp; Electronics</strong></h4><p>The system is powered by<strong>six SG90 servo motors</strong> controlled using an<strong>Arduino or similar microcontroller</strong>. The design allows for expansion with<strong>sensor-based or AI-driven control</strong>.</p><h5 id="simulation-links"><strong>Simulation Links</strong></h5><ul><li><p><strong>Tinkercad Circuit Simulation:</strong><a href="https://www.tinkercad.com/things/4HJGXiv97LI-servo-flex-mimic-hand" target="_blank">View Here</a></p><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="560" src="/images/projects/prosarm/tinkercad_hu_c6df5abeca91fc04.webp" alt="Tinkercad Model" onerror="this.onerror='null';this.src='\/images\/projects\/prosarm\/tinkercad_hu_e2012e237cb339f8.png'"/></li><li><p><strong>Wokwi Simulation:</strong><a href="https://wokwi.com/projects/400010151130264577" target="_blank">View Here</a></p></li></ul><hr><h4 id="future-work--goals"><strong>Future Work &amp; Goals</strong></h4><p>✅<strong>Improve Mechanical &amp; Electronic Design</strong> (70% Complete)<br>
✅<strong>Prototype &amp; Testing Phases</strong> (20% Complete)<br>
🔲<strong>Enhance User Control via Muscle Signals (BCI) or Computer Vision (MediaPipe for Hand Tracking)</strong></p><h5 id="next-steps"><strong>Next Steps</strong></h5><ul><li><strong>Brain-Computer Interface (BCI):</strong> Connect the prosthetic arm to human muscles using electromyography (EMG) sensors.</li><li><strong>Computer Vision Integration:</strong> Use<strong>MediaPipe hand tracking</strong> to detect hand gestures and translate them into servo motor commands.</li><li><strong>Further Refinements in Mechanical Design &amp; 3D Printing.</strong></li></ul><p>This project is constantly evolving, and I welcome feedback, collaboration, and new ideas! 🚀</p><hr>
]]></content:encoded></item><item><title/><link>https://mummanajagadeesh.github.io/projects/protocols/i2cv/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/projects/protocols/i2cv/</guid><description>&lt;![CDATA[<h2 id="i2c-protocol-verilog-implementation-using-fsm"><a href="https://github.com/Mummanajagadeesh/I2C-protocol-verilog" target="_blank">I2C Protocol Verilog Implementation using FSM</a></h2><table><thead><tr><th><strong>Name</strong></th><th>I2C Protocol</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>Verilog Implementation of I2C Protocol using Finite State Machine (FSM) design</td></tr><tr><td><strong>Start</strong></td><td>06 Nov 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/I2C-protocol-verilog" target="_blank">I2CV🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>HDL, Protocols, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Verilog, Icarus, Xilinx</td></tr><tr><td><strong>Current Status</strong></td><td>On Hold</td></tr></tbody></table><hr><p>This project implements the I2C protocol in Verilog with various versions and configurations. Below is a summary of each version:</p>]]></description><content:encoded>&lt;![CDATA[<h2 id="i2c-protocol-verilog-implementation-using-fsm"><a href="https://github.com/Mummanajagadeesh/I2C-protocol-verilog" target="_blank">I2C Protocol Verilog Implementation using FSM</a></h2><table><thead><tr><th><strong>Name</strong></th><th>I2C Protocol</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>Verilog Implementation of I2C Protocol using Finite State Machine (FSM) design</td></tr><tr><td><strong>Start</strong></td><td>06 Nov 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/I2C-protocol-verilog" target="_blank">I2CV🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>HDL, Protocols, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Verilog, Icarus, Xilinx</td></tr><tr><td><strong>Current Status</strong></td><td>On Hold</td></tr></tbody></table><hr><p>This project implements the I2C protocol in Verilog with various versions and configurations. Below is a summary of each version:</p><h4 id="v102---simple-master-slave-no-clock-stretching">v1.0.2 - Simple Master-Slave (No Clock Stretching)</h4><ul><li><strong>Description</strong>: This version implements a simple I2C protocol with one master and one slave device. It does not support clock stretching.</li><li><strong>Features</strong>:<ul><li>Basic I2C communication between a single master and slave.</li><li>No handling of clock stretching; both master and slave devices operate with the same clock frequency.</li><li>NACK will be raised indicating that the given address for slave is wrong.</li></ul></li></ul><h4 id="v203---clock-stretching-with-fixed-master-delay">v2.0.3 - Clock Stretching with Fixed Master Delay</h4><ul><li><strong>Description</strong>: This version adds support for clock stretching. The master introduces a fixed delay after sending each data frame, and the SCL line will be held low (clock stretching) while waiting for the slave.</li><li><strong>Features</strong>:<ul><li>Clock stretching is supported for managing communication timing.</li><li>The master introduces a fixed delay to simulate real-world clock stretching scenarios.</li></ul></li></ul><h4 id="v204---clock-stretching-with-configurable-master-delay">v2.0.4 - Clock Stretching with Configurable Master Delay</h4><ul><li><strong>Description</strong>: This version builds on v2.0.3 by adding a configurable delay from the testbench. The delay allows adjusting the time period that the SCL line waits, providing greater flexibility.</li><li><strong>Features</strong>:<ul><li>Configurable master delay, adjustable from the testbench.</li><li>SCL waiting period comparison between the master and slave devices.</li><li>Enhanced clock stretching handling with configurable delays.</li></ul></li></ul><h4 id="v301---multi-slave-single-master-configuration">v3.0.1 - Multi-Slave Single Master Configuration</h4><ul><li><strong>Description</strong>: This version introduces a configuration with a single master and multiple slave devices. The master can communicate with any of the slaves, supporting multi-slave communication.</li><li><strong>Features</strong>:<ul><li>One master can communicate with multiple slaves.</li><li>The master can address and select any slave for communication.</li><li>Basic multi-slave handling without clock stretching.</li></ul></li></ul><h4 id="v311---multi-slave-multi-master-configuration">v3.1.1 - Multi-Slave Multi-Master Configuration</h4><ul><li><strong>Description</strong>: The latest version supports a multi-master, multi-slave configuration, where both the master and slaves can initiate communication. It adds complexity to handle multiple devices that can take control of the bus.</li><li><strong>Features</strong>:<ul><li>Multiple masters can communicate with multiple slaves.</li><li>Advanced bus arbitration is implemented to handle multiple masters trying to access the bus at the same time.</li><li>Suitable for complex systems requiring both multiple masters and multiple slaves.</li></ul></li></ul><p>I2C combines the strengths of both UART and
SPI. It operates using just two wires, like asynchronous serial, yet
supports communication with up to 1,008 peripheral devices. Unlike SPI,
I2C accommodates multi-controller systems, allowing more than one
controller to communicate with all peripheral devices on the bus
(although the controllers must take turns using the bus lines).</p><p>I2C data rates fall between those of asynchronous serial and SPI, with
most devices communicating at 100 kHz or 400 kHz. While there is some
overhead&mdash;requiring one additional acknowledgment (ACK/NACK) bit for
every 8 bits of data transmitted&mdash;I2C remains efficient. Although
implementing I2C requires more complex hardware than SPI, it is still
simpler than asynchronous serial and can be easily realized in software.</p><figure id="fig:i2c_block_diagram"><span class="image placeholder" data-original-image-src="i2c_block_diagram.png" data-original-image-title="" width="80%"/><figcaption>Block Diagram of an I2C System</figcaption></figure><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="600" src="/images/projects/i2cv/i2c_block_diagram_hu_dd0b2b33ad6560e2.webp" alt="Block Diagram of an I2C System" onerror="this.onerror='null';this.src='\/images\/projects\/i2cv\/i2c_block_diagram_hu_b735bf1b608da6af.png'"/><script>
window.addEventListener("load", (e) => {
const lightbox = GLightbox();
});</script><h4 id="physical-layer">Physical layer</h4><h5 id="two-wire-communication">Two-Wire Communication</h5><p>An I2C system utilizes<strong>two shared communication lines</strong> for all
devices on the bus. These two lines facilitate<strong>bidirectional,
half-duplex communication</strong>. I2C supports multiple controllers and
multiple target devices, making it a flexible choice for various
applications. It is essential to use<strong>pull-up resistors</strong> on both of
these lines to ensure proper operation. Fig<a href="#fig:i2c_implementation">2.4</a>{reference-type=&ldquo;ref&rdquo;
reference=&ldquo;fig:i2c_implementation&rdquo;} shows a typical implementation of
the I2C physical layer.</p><figure id="fig:i2c_implementation"><span class="image placeholder" data-original-image-src="i2c_physical_layer.png" data-original-image-title="" width="80%"/><figcaption>Typical I2C Implementation</figcaption></figure><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="377" src="/images/projects/i2cv/i2c_physical_layer_hu_70caca21653a84fc.webp" alt="Typical I2C Implementation" onerror="this.onerror='null';this.src='\/images\/projects\/i2cv\/i2c_physical_layer_hu_6663d0aac3aa7b6c.png'"/><p>One of the main reasons that I2C is a widely adopted protocol is due to
its requirement of only<strong>two lines</strong> for communication. The first line,<strong>SCL</strong>, is the serial clock line, primarily controlled by the
controller device. SCL is responsible for synchronously clocking data in
or out of the target device. The second line,<strong>SDA</strong>, is the serial
data line, used to transmit data to or from the target devices. For
instance, a controller device can send configuration data and output
codes to a target<strong>digital-to-analog converter (DAC)</strong>, or a target<strong>analog-to-digital converter (ADC)</strong> can send conversion data back to
the controller device.</p><p>I2C operates as a<strong>half-duplex communication</strong> protocol, meaning that
only one controller or target device can send data on the bus at any
given time. In contrast, the<strong>Serial Peripheral Interface (SPI)</strong> is a<strong>full-duplex protocol</strong> that allows data to be sent and received
simultaneously, requiring four lines for communication: two data lines
for sending and receiving data, along with a serial clock and a unique
SPI chip select line to select the device for communication.</p><p>An I2C controller device initiates and terminates communication, which
eliminates potential issues related to<strong>bus contention</strong>. Communication
with a target device is established through a<strong>unique address</strong> on the
bus, allowing multiple controllers and multiple target devices to
coexist on the I2C bus.</p><p>The SDA and SCL lines have an<strong>open-drain connection</strong> to all devices
on the bus, necessitating a pull-up resistor connected to a common
voltage supply.</p><h5 id="open-drain-connection">Open-Drain Connection</h5><p>The<strong>open-drain connections</strong> are employed on both the SDA and SCL
lines and are linked to an NMOS transistor. This open-drain
configuration manages the I2C communication line by either pulling the
line low or allowing it to rise to a high state. The term "open-drain"
refers to the NMOS bus connection when the NMOS is turned<strong>OFF</strong>.
Figure<a href="#fig:open_drain_connection">2.5</a>{reference-type=&ldquo;ref&rdquo;
reference=&ldquo;fig:open_drain_connection&rdquo;} illustrates the open-drain
connection when the NMOS is turned<strong>ON</strong>.</p><figure id="fig:open_drain_connection"><span class="image placeholder" data-original-image-src="open_drain_connection.png" data-original-image-title="" width="80%"/><figcaption>Open-Drain Connection Pulls Line Low When NMOS is Turned
On</figcaption></figure><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="408" src="/images/projects/i2cv/open_drain_connection_hu_39f8dcb6fd6bd47c.webp" alt="Open-Drain Connection Pulls Line Low When NMOS is Turned On" onerror="this.onerror='null';this.src='\/images\/projects\/i2cv\/open_drain_connection_hu_e6298d9a910a313c.png'"/><p>To establish the voltage level of the SDA or SCL line, the NMOS
transistor is either switched<strong>ON</strong> or<strong>OFF</strong>. When the NMOS is<strong>ON</strong>, it allows current to flow through the resistor to ground,
effectively pulling the open-drain line low. This transition from high
to low is typically rapid, as the NMOS quickly discharges any
capacitance on the SDA or SCL lines.</p><p>When the NMOS turns<strong>OFF</strong>, the device ceases to pull current, and the
pull-up resistor subsequently raises the SDA or SCL line back to<strong>VDD</strong>. Figure<a href="#fig:open_drain_off">2.6</a>{reference-type=&ldquo;ref&rdquo;
reference=&ldquo;fig:open_drain_off&rdquo;} shows the open-drain line when the NMOS
is turned<strong>OFF</strong>, illustrating how the pull-up resistor brings the line
high.</p><figure id="fig:open_drain_off"><span class="image placeholder" data-original-image-src="open_drain_off.png" data-original-image-title="" width="80%"/><figcaption>Open-Drain Line with NMOS Turned Off</figcaption></figure><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="469" src="/images/projects/i2cv/open_drain_off_hu_f08f978ae02b9685.webp" alt="Open-Drain Line with NMOS Turned Off" onerror="this.onerror='null';this.src='\/images\/projects\/i2cv\/open_drain_off_hu_a52fad10d1e40530.png'"/><p>The transition of the open-drain line to a high state is slower because
the line is pulled up against the bus capacitance, rather than being
actively driven high.</p><h4 id="i2c-protocol">I2C Protocol</h4><p>Communication over<strong>I2C</strong> requires a specific signaling protocol to
ensure that devices on the bus recognize valid I2C transmissions. While
this process is more intricate than<strong>UART</strong> or<strong>SPI</strong>, most
I2C-compatible devices handle the finer protocol details internally,
allowing developers to focus primarily on data exchange.</p><p><strong>SDA and SCL Lines:</strong> The I2C bus operates with two main lines:<strong>SDA</strong>
(Serial Data Line) and<strong>SCL</strong> (Serial Clock Line). Data is transmitted
over the<strong>SDA</strong> line in sync with clock pulses on the<strong>SCL</strong> line.
Generally, data is placed on<strong>SDA</strong> when<strong>SCL</strong> is low, and devices
sample this data when<strong>SCL</strong> goes high. If needed, multiple internal<strong>registers</strong> may control data handling, especially in complex devices.</p><p><strong>Protocol Components:</strong></p><p>1.<strong>Start Condition:</strong> To initiate communication, the controller sets<strong>SCL</strong> high and then pulls<strong>SDA</strong> low. This signals all peripheral
devices on the bus that a transmission is starting. In cases where
multiple controllers attempt to start communication simultaneously, the
first device to pull<strong>SDA</strong> low gains control. If necessary, the
controller can issue repeated start conditions to maintain bus control
without releasing it.</p><p>2.<strong>Address Frame:</strong> Every I2C transmission begins with an<strong>address
frame</strong> to specify the target peripheral. This frame consists of a 7-bit
address, sent<strong>MSB</strong> (most significant bit) first, followed by a<strong>R/W
bit</strong> indicating the operation type (read or write).</p><p>After this, the 9th bit, known as the<strong>ACK/NACK bit</strong>, is used by the
receiving device to confirm reception. If the device pulls<strong>SDA</strong> low
before the 9th clock pulse (<strong>ACK</strong>), communication continues. If not
(<strong>NACK</strong>), it indicates either unrecognized data or an issue in
reception, prompting the controller to decide the next steps.</p><p>3.<strong>Data Frames:</strong> Following the address frame, one or more<strong>data
frames</strong> are sent over the<strong>SDA</strong> line. Each data frame is 8 bits, and
data is transferred from the controller to the peripheral or vice versa,
based on the<strong>R/W bit</strong> in the address frame.</p><p>Many peripheral devices have auto-incrementing<strong>internal registers</strong>,
enabling data to continue from consecutive registers without the need to
re-specify the register address.</p><p>4.<strong>Stop Condition:</strong> The controller ends communication by generating
a<strong>stop condition</strong>. This is done by transitioning<strong>SDA</strong> from low to
high after a high-to-low transition on<strong>SCL</strong>, with<strong>SCL</strong> held high
during the stop sequence. To avoid false stop conditions, the value on<strong>SDA</strong> should not change while<strong>SCL</strong> is high during regular data
transmission.</p><p>The<strong>I2C protocol</strong> divides communication into structured<strong>frames</strong>.
Each communication sequence begins with a<strong>START</strong> condition, initiated
by the controller, followed by an<strong>address frame</strong> and then one or more<strong>data frames</strong>. Every frame also includes an acknowledgment (ACK) bit,
signaling that the frame has been received successfully by the intended
device.<strong>Figure 3-3</strong> illustrates the structure of two I2C
communication frames, showing both address and data frames in detail.</p><figure id="fig:i2c_frames"><span class="image placeholder" data-original-image-src="i2c_frames.png" data-original-image-title="" width="80%"/><figcaption>I2C Address and Data Frames</figcaption></figure><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="214" src="/images/projects/i2cv/i2c_frames_hu_2f61972525b7bae2.webp" alt="I2C Address and Data Frames" onerror="this.onerror='null';this.src='\/images\/projects\/i2cv\/i2c_frames_hu_672abfe8cd224959.png'"/><p>In an I2C transaction, the controller first sends a<strong>START condition</strong>
by pulling the<strong>SDA</strong> line low, followed by the<strong>SCL</strong> line. This
sequence asserts control over the bus, preventing other devices from
interfering. Each target device on the I2C bus has a unique<strong>7-bit
address</strong>, allowing the controller to specify which target device it
intends to communicate with.</p><p>Once the address is set on<strong>SDA</strong> while<strong>SCL</strong> acts as the clock, the<strong>8th bit</strong> (R/W bit) indicates the intended operation type:<strong>read
(1)</strong> or<strong>write (0)</strong>. This initial address and R/W bit are followed by
an<strong>ACK bit</strong>, sent by the target device to confirm receipt. If the
target device receives the address successfully, it pulls<strong>SDA</strong> low
during the next<strong>SCL</strong> pulse, signaling an ACK. If no device
acknowledges, the line remains high, signaling a<strong>NACK</strong>.</p><p>After the address frame, one or more<strong>data frames</strong> follow. Each data
frame contains 8 bits of data, which are acknowledged (ACK) in the 9th
bit. If the data frame is a<strong>write</strong> operation, the target device pulls<strong>SDA</strong> low to confirm data receipt. For<strong>read</strong> operations, the
controller pulls<strong>SDA</strong> low to acknowledge receipt of the data. The
presence or absence of the ACK is essential for troubleshooting, as a
missing ACK may indicate an addressing error or transmission failure.</p><p>Finally, the communication ends with a<strong>STOP condition</strong>, where the
controller releases<strong>SCL</strong> first, followed by<strong>SDA</strong>. This action
releases the I2C bus for other devices to use, completing the
communication cycle.</p><p>This structured protocol allows for the transmission of multiple bytes
within one communication sequence. In cases where a target device has
multiple internal<strong>registers</strong>, a write operation can specify the
register to read or write data to, enhancing flexibility and enabling
complex data transactions.</p><h1 id="module-specifications">Module Specifications</h1><h4 id="master-module">MASTER MODULE</h4><h5 id="c0d3">C0d3</h5><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-verilog" data-lang="verilog"><span style="display:flex;"><span><span style="color:#66d9ef">`timescale</span><span style="color:#ae81ff">1</span>ns<span style="color:#f92672">/</span><span style="color:#ae81ff">1</span>ps</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// Main module declaration</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">module</span> i2c_master(</span></span><span style="display:flex;"><span><span style="color:#66d9ef">input</span><span style="color:#66d9ef">wire</span> clk,<span style="color:#75715e">// System clock</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">input</span><span style="color:#66d9ef">wire</span> rst,<span style="color:#75715e">// Reset signal</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">input</span><span style="color:#66d9ef">wire</span> [<span style="color:#ae81ff">6</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] addr,<span style="color:#75715e">// 7-bit I2C slave address</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">input</span><span style="color:#66d9ef">wire</span> [<span style="color:#ae81ff">7</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] data_in,<span style="color:#75715e">// Data to send to slave in write mode</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">input</span><span style="color:#66d9ef">wire</span> enable,<span style="color:#75715e">// Start signal for I2C communication</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">input</span><span style="color:#66d9ef">wire</span> rw,<span style="color:#75715e">// Read/Write control (0 for write, 1 for read)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">output</span><span style="color:#66d9ef">reg</span> [<span style="color:#ae81ff">7</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] data_out,<span style="color:#75715e">// Data received from slave in read mode</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">output</span><span style="color:#66d9ef">wire</span> ready,<span style="color:#75715e">// Indicates when the master is ready for a new transaction</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">inout</span> i2c_sda,<span style="color:#75715e">// I2C data line (SDA) - bidirectional</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">inout</span><span style="color:#66d9ef">wire</span> i2c_scl<span style="color:#75715e">// I2C clock line (SCL) - bidirectional</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/>);</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// Define states for I2C master FSM</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">localparam</span> IDLE<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;</span></span><span style="display:flex;"><span><span style="color:#66d9ef">localparam</span> START<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>;</span></span><span style="display:flex;"><span><span style="color:#66d9ef">localparam</span> ADDRESS<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>;</span></span><span style="display:flex;"><span><span style="color:#66d9ef">localparam</span> READ_ACK<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>;</span></span><span style="display:flex;"><span><span style="color:#66d9ef">localparam</span> WRITE_DATA<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>;</span></span><span style="display:flex;"><span><span style="color:#66d9ef">localparam</span> WRITE_ACK<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>;</span></span><span style="display:flex;"><span><span style="color:#66d9ef">localparam</span> READ_DATA<span style="color:#f92672">=</span><span style="color:#ae81ff">6</span>;</span></span><span style="display:flex;"><span><span style="color:#66d9ef">localparam</span> READ_ACK2<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span>;</span></span><span style="display:flex;"><span><span style="color:#66d9ef">localparam</span> STOP<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>;</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#66d9ef">localparam</span> DIVIDE_BY<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>;<span style="color:#75715e">// Clock divider to generate I2C clock from system clock</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/></span></span><span style="display:flex;"><span><span style="color:#66d9ef">reg</span> [<span style="color:#ae81ff">7</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] state;<span style="color:#75715e">// Current state of the FSM</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> [<span style="color:#ae81ff">7</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] saved_addr;<span style="color:#75715e">// Stores the 7-bit address and RW bit for the current transaction</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> [<span style="color:#ae81ff">7</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] saved_data;<span style="color:#75715e">// Data to be sent in write transactions</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> [<span style="color:#ae81ff">7</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] counter;<span style="color:#75715e">// Bit counter for data/address transmission</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> [<span style="color:#ae81ff">7</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] counter2<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Divider counter for generating i2c_clk</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> write_enable;<span style="color:#75715e">// Controls whether the master drives SDA line</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> sda_out;<span style="color:#75715e">// Data to output on SDA line when write_enable is 1</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> i2c_scl_enable<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Controls the state of the i2c_scl line (enabled or high)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> i2c_clk<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Internal I2C clock signal</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/></span></span><span style="display:flex;"><span><span style="color:#75715e">// Ready signal is high when the master is idle and not in reset</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">assign</span> ready<span style="color:#f92672">=</span> ((rst<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>)<span style="color:#f92672">&amp;&amp;</span> (state<span style="color:#f92672">==</span> IDLE))<span style="color:#f92672">?</span><span style="color:#ae81ff">1</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>;</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// I2C SCL signal: High when i2c_scl_enable is low; otherwise, driven by i2c_clk</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">assign</span> i2c_scl<span style="color:#f92672">=</span> (i2c_scl_enable<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>)<span style="color:#f92672">?</span><span style="color:#ae81ff">1</span><span style="color:#f92672">:</span> i2c_clk;</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// SDA line is driven by sda_out when write_enable is high; otherwise, it's in high-impedance</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">assign</span> i2c_sda<span style="color:#f92672">=</span> (write_enable<span style="color:#f92672">==</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">?</span> sda_out<span style="color:#f92672">:</span> 'bz;</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// I2C clock divider: Divides system clock to generate i2c_clk</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">always</span> @(<span style="color:#66d9ef">posedge</span> clk)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> (counter2<span style="color:#f92672">==</span> (DIVIDE_BY<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>)<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> i2c_clk<span style="color:#f92672">&lt;=</span><span style="color:#f92672">~</span>i2c_clk;<span style="color:#75715e">// Toggle i2c_clk when half period is reached</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> counter2<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Reset the divider counter</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span><span style="color:#66d9ef">else</span><span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> counter2<span style="color:#f92672">&lt;=</span> counter2<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Increment the divider counter</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// Enable/disable I2C clock based on current state</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">always</span> @(<span style="color:#66d9ef">negedge</span> i2c_clk<span style="color:#66d9ef">or</span><span style="color:#66d9ef">posedge</span> rst)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> (rst<span style="color:#f92672">==</span><span style="color:#ae81ff">1</span>)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> i2c_scl_enable<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Disable SCL on reset</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span><span style="color:#66d9ef">else</span><span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> ((state<span style="color:#f92672">==</span> IDLE)<span style="color:#f92672">||</span> (state<span style="color:#f92672">==</span> START)<span style="color:#f92672">||</span> (state<span style="color:#f92672">==</span> STOP))<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> i2c_scl_enable<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// SCL is disabled in IDLE, START, and STOP states</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span><span style="color:#66d9ef">else</span><span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> i2c_scl_enable<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Enable SCL in other states</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// State machine for controlling the I2C master operation</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">always</span> @(<span style="color:#66d9ef">posedge</span> i2c_clk<span style="color:#66d9ef">or</span><span style="color:#66d9ef">posedge</span> rst)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> (rst<span style="color:#f92672">==</span><span style="color:#ae81ff">1</span>)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> state<span style="color:#f92672">&lt;=</span> IDLE;<span style="color:#75715e">// Reset state to IDLE on reset</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span><span style="color:#66d9ef">else</span><span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">case</span> (state)</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> IDLE:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> (enable)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> state<span style="color:#f92672">&lt;=</span> START;<span style="color:#75715e">// Start I2C transaction when enable is high</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> saved_addr<span style="color:#f92672">&lt;=</span> {addr, rw};<span style="color:#75715e">// Save the 7-bit address and RW bit</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> saved_data<span style="color:#f92672">&lt;=</span> data_in;<span style="color:#75715e">// Save the data to be sent (in write mode)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> START:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> counter<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">7</span>;<span style="color:#75715e">// Initialize bit counter to 7 for 8-bit transmission</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> state<span style="color:#f92672">&lt;=</span> ADDRESS;<span style="color:#75715e">// Move to ADDRESS state</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> ADDRESS:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> (counter<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> state<span style="color:#f92672">&lt;=</span> READ_ACK;<span style="color:#75715e">// Move to ACK check after sending address and RW bit</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span><span style="color:#66d9ef">else</span><span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> counter<span style="color:#f92672">&lt;=</span> counter<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Transmit address bits, count down</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> READ_ACK:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> (i2c_sda<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>)<span style="color:#66d9ef">begin</span><span style="color:#75715e">// ACK received (SDA pulled low by slave)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> counter<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">7</span>;<span style="color:#75715e">// Reset bit counter</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">if</span> (saved_addr[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>) state<span style="color:#f92672">&lt;=</span> WRITE_DATA;<span style="color:#75715e">// If RW=0, go to write mode</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">else</span> state<span style="color:#f92672">&lt;=</span> READ_DATA;<span style="color:#75715e">// If RW=1, go to read mode</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span><span style="color:#66d9ef">else</span><span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> state<span style="color:#f92672">&lt;=</span> STOP;<span style="color:#75715e">// NACK received, move to STOP state</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> WRITE_DATA:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> (counter<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> state<span style="color:#f92672">&lt;=</span> READ_ACK2;<span style="color:#75715e">// Move to second ACK check after data transmission</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span><span style="color:#66d9ef">else</span><span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> counter<span style="color:#f92672">&lt;=</span> counter<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Transmit data bits, count down</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> READ_ACK2:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> ((i2c_sda<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>)<span style="color:#f92672">&amp;&amp;</span> (enable<span style="color:#f92672">==</span><span style="color:#ae81ff">1</span>)) state<span style="color:#f92672">&lt;=</span> IDLE;<span style="color:#75715e">// Return to IDLE on ACK</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">else</span> state<span style="color:#f92672">&lt;=</span> STOP;<span style="color:#75715e">// If NACK received or enable low, go to STOP</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> READ_DATA:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> data_out[counter]<span style="color:#f92672">&lt;=</span> i2c_sda;<span style="color:#75715e">// Capture data bit from SDA line</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">if</span> (counter<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>) state<span style="color:#f92672">&lt;=</span> WRITE_ACK;<span style="color:#75715e">// After last bit, go to WRITE_ACK</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">else</span> counter<span style="color:#f92672">&lt;=</span> counter<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Count down for each bit received</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> WRITE_ACK:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> state<span style="color:#f92672">&lt;=</span> STOP;<span style="color:#75715e">// Go to STOP after sending ACK</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> STOP:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> state<span style="color:#f92672">&lt;=</span> IDLE;<span style="color:#75715e">// Go back to IDLE after STOP condition</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">endcase</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// SDA output logic based on the current state</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">always</span> @(<span style="color:#66d9ef">negedge</span> i2c_clk<span style="color:#66d9ef">or</span><span style="color:#66d9ef">posedge</span> rst)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> (rst<span style="color:#f92672">==</span><span style="color:#ae81ff">1</span>)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> write_enable<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Drive SDA high on reset</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> sda_out<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">1</span>;</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span><span style="color:#66d9ef">else</span><span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">case</span> (state)</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> START:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> write_enable<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Enable SDA for start condition</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> sda_out<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Pull SDA low for start condition</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> ADDRESS:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> sda_out<span style="color:#f92672">&lt;=</span> saved_addr[counter];<span style="color:#75715e">// Send each bit of the address and RW bit</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> READ_ACK:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> write_enable<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Release SDA to allow slave to drive ACK/NACK</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> WRITE_DATA:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> write_enable<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Enable SDA for data transmission</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> sda_out<span style="color:#f92672">&lt;=</span> saved_data[counter];<span style="color:#75715e">// Output each bit of data to SDA</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> WRITE_ACK:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> write_enable<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Enable SDA for ACK transmission</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> sda_out<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Send ACK by pulling SDA low</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> READ_DATA:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> write_enable<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Release SDA to read data from slave</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> STOP:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> write_enable<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Enable SDA for stop condition</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> sda_out<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Release SDA to indicate stop</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">endcase</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#66d9ef">endmodule</span></span></span></code></pre></div><h5 id="explanation">Explanation</h5><p>The provided code is a Verilog implementation of an I2C Master Module.
This module enables communication with I2C-compatible devices through
the I2C protocol by implementing the necessary operations to generate
I2C signals and manage data transfer. Let&rsquo;s break down each section of
the code:</p><h6 id="module-declaration">Module Declaration</h6><p>The code begins with the module declaration:</p><blockquote><pre><code>module i2c_master(
input wire clk, // System clock
input wire rst, // Synchronous reset
input wire [6:0] addr, // 7-bit I2C address
input wire [7:0] data_in, // Data to be transmitted
input wire enable, // Enable signal to start I2C transaction
input wire rw, // Read/Write control (0 = Write, 1 = Read)
output reg [7:0] data_out, // Data received from I2C
output wire ready, // Ready signal when module is idle
inout i2c_sda, // I2C data line (SDA)
inout wire i2c_scl // I2C clock line (SCL)
);</code></pre></blockquote><p>This module contains inputs for the system clock (<code>clk</code>), reset (<code>rst</code>),
I2C address (<code>addr</code>), data to be sent (<code>data_in</code>), an enable signal
(<code>enable</code>), and a Read/Write control (<code>rw</code>). It also provides outputs
for data received (<code>data_out</code>), a ready status signal (<code>ready</code>), and
bidirectional I2C lines,<code>i2c_sda</code> and<code>i2c_scl</code>.</p><h6 id="state-machine-definition">State Machine Definition</h6><p>The code defines several states representing stages in the I2C
transaction:</p><blockquote><pre><code>localparam IDLE = 0;
localparam START = 1;
localparam ADDRESS = 2;
localparam READ_ACK = 3;
localparam WRITE_DATA = 4;
localparam WRITE_ACK = 5;
localparam READ_DATA = 6;
localparam READ_ACK2 = 7;
localparam STOP = 8;</code></pre></blockquote><p>Each<code>localparam</code> corresponds to a state in the Finite State Machine
(FSM), controlling the I2C protocol flow, including start, address
transmission, acknowledgment (ACK) reception, data transfer, and stop
condition generation.</p><h6 id="clock-divider">Clock Divider</h6><p>To generate a slower clock for the I2C operations, a clock divider is
implemented:</p><blockquote><pre><code>always @(posedge clk) begin
if (counter2 == (DIVIDE_BY / 2) - 1) begin
i2c_clk &lt;= ~i2c_clk;
counter2 &lt;= 0;
end else counter2 &lt;= counter2 + 1;
end</code></pre></blockquote><p>This block toggles<code>i2c_clk</code> at a lower frequency than the system clock,<code>clk</code>, using a counter<code>counter2</code> with a division factor defined by<code>DIVIDE_BY</code>.</p><h6 id="sda-and-scl-control">SDA and SCL Control</h6><p>To control the<code>i2c_sda</code> and<code>i2c_scl</code> lines based on the module&rsquo;s
state:</p><blockquote><pre><code>assign ready = ((rst == 0) &amp;&amp; (state == IDLE)) ? 1 : 0;
assign i2c_scl = (i2c_scl_enable == 0) ? 1 : i2c_clk;
assign i2c_sda = (write_enable == 1) ? sda_out : 'bz;</code></pre></blockquote><ul><li><p><code>ready</code> is high when the reset is inactive and the state is<code>IDLE</code>.</p></li><li><p><code>i2c_scl</code> is either high (idle state) or follows the divided<code>i2c_clk</code> signal.</p></li><li><p><code>i2c_sda</code> outputs the value of<code>sda_out</code> when<code>write_enable</code> is
active. When<code>write_enable</code> is inactive,<code>i2c_sda</code> goes to
high-impedance (<code>’bz</code>) for reading data.</p></li></ul><h6 id="finite-state-machine-fsm">Finite State Machine (FSM)</h6><p>The FSM controls the I2C communication process, progressing through
states based on the I2C protocol requirements:</p><blockquote><pre><code>always @(posedge i2c_clk or posedge rst) begin
if (rst == 1) begin
state &lt;= IDLE;
end else begin
case (state)
IDLE: begin
if (enable) begin
state &lt;= START;
saved_addr &lt;= {addr, rw};
saved_data &lt;= data_in;
end
end
START: begin
counter &lt;= 7;
state &lt;= ADDRESS;
end
...
STOP: begin
state &lt;= IDLE;
end
endcase
end
end</code></pre></blockquote><p>Each state corresponds to an I2C operation:</p><ul><li><p><code>IDLE</code>: Waits for<code>enable</code> signal to initiate communication.</p></li><li><p><code>START</code>: Prepares a start condition by asserting<code>sda_out</code> low.</p></li><li><p><code>ADDRESS</code>: Sends the address and R/W bit.</p></li><li><p><code>READ_ACK</code> and<code>READ_ACK2</code>: Verifies acknowledgment (ACK) from the
slave.</p></li><li><p><code>WRITE_DATA</code> and<code>WRITE_ACK</code>: Transfers data to the slave and waits
for ACK.</p></li><li><p><code>READ_DATA</code>: Receives data from the slave.</p></li><li><p><code>STOP</code>: Generates a stop condition and returns to<code>IDLE</code>.</p></li></ul><h6 id="sda-output-logic">SDA Output Logic</h6><p>The logic for controlling the<code>i2c_sda</code> line, depending on the FSM
state, is implemented as follows:</p><blockquote><pre><code>always @(negedge i2c_clk or posedge rst) begin
if (rst == 1) begin
write_enable &lt;= 1;
sda_out &lt;= 1;
end else begin
case (state)
START: begin
write_enable &lt;= 1;
sda_out &lt;= 0;
end
ADDRESS: begin
sda_out &lt;= saved_addr[counter];
end
...
STOP: begin
write_enable &lt;= 1;
sda_out &lt;= 1;
end
endcase
end
end</code></pre></blockquote><ul><li><p>In the<code>START</code> state,<code>sda_out</code> goes low to generate a start
condition.</p></li><li><p>In the<code>ADDRESS</code> and<code>WRITE_DATA</code> states,<code>sda_out</code> sends the bits
of<code>saved_addr</code> or<code>saved_data</code>.</p></li><li><p>In<code>STOP</code>,<code>sda_out</code> goes high to signify the end of the
transmission.</p></li></ul><p>This Verilog module effectively implements an I2C Master communication
sequence by controlling the<code>i2c_sda</code> and<code>i2c_scl</code> lines according to
the I2C protocol.</p><h4 id="slave-module">SLAVE MODULE</h4><h5 id="c0d3-1">C0d3</h5><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-verilog" data-lang="verilog"><span style="display:flex;"><span><span style="color:#66d9ef">module</span> i2c_slave(</span></span><span style="display:flex;"><span><span style="color:#66d9ef">input</span> [<span style="color:#ae81ff">6</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] addr_in,<span style="color:#75715e">// Slave address to respond to (dynamic address input)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">inout</span> sda,<span style="color:#75715e">// I2C data line (SDA) - bidirectional</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">inout</span> scl<span style="color:#75715e">// I2C clock line (SCL)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/>);</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// Define states for the I2C slave FSM</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">localparam</span> READ_ADDR<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// State for reading the address from the master</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">localparam</span> SEND_ACK<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// State for sending ACK after receiving a matching address</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">localparam</span> READ_DATA<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>;<span style="color:#75715e">// State for reading data from the master</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">localparam</span> WRITE_DATA<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>;<span style="color:#75715e">// State for sending data to the master</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">localparam</span> SEND_ACK2<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>;<span style="color:#75715e">// State for sending ACK after receiving data from the master</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/></span></span><span style="display:flex;"><span><span style="color:#66d9ef">reg</span> [<span style="color:#ae81ff">7</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] addr;<span style="color:#75715e">// Register to store the address received from the master</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> [<span style="color:#ae81ff">7</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] counter;<span style="color:#75715e">// Bit counter for data/address transmission</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> [<span style="color:#ae81ff">7</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Current state of the FSM</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> [<span style="color:#ae81ff">7</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] data_in<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Register to store data received from the master</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> [<span style="color:#ae81ff">7</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] data_out<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span><span style="color:#ae81ff">'b11001100</span>;<span style="color:#75715e">// Data to be sent to the master in read mode</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> sda_out<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Data to drive onto SDA when write_enable is high</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> sda_in<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Register to capture SDA input data</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> start<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Flag to indicate the start condition (SDA goes low while SCL is high)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> write_enable<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Controls whether the slave drives the SDA line</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/></span></span><span style="display:flex;"><span><span style="color:#75715e">// Tri-state SDA line: driven by sda_out when write_enable is high, otherwise high-impedance</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">assign</span> sda<span style="color:#f92672">=</span> (write_enable<span style="color:#f92672">==</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">?</span> sda_out<span style="color:#f92672">:</span> 'bz;</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// Detect start condition on SDA falling edge when SCL is high</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">always</span> @(<span style="color:#66d9ef">negedge</span> sda)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> ((start<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>)<span style="color:#f92672">&amp;&amp;</span> (scl<span style="color:#f92672">==</span><span style="color:#ae81ff">1</span>))<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> start<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Set start flag</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> counter<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">7</span>;<span style="color:#75715e">// Initialize counter to read 8 bits (address or data)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// Detect stop condition on SDA rising edge when SCL is high</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">always</span> @(<span style="color:#66d9ef">posedge</span> sda)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> ((start<span style="color:#f92672">==</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">&amp;&amp;</span> (scl<span style="color:#f92672">==</span><span style="color:#ae81ff">1</span>))<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> state<span style="color:#f92672">&lt;=</span> READ_ADDR;<span style="color:#75715e">// Go to READ_ADDR state to read the address from master</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> start<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Clear start flag</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> write_enable<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Release SDA line</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// State machine for I2C slave behavior, triggered on rising edge of SCL</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">always</span> @(<span style="color:#66d9ef">posedge</span> scl)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> (start<span style="color:#f92672">==</span><span style="color:#ae81ff">1</span>)<span style="color:#66d9ef">begin</span><span style="color:#75715e">// Only proceed if start condition was detected</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">case</span>(state)</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> READ_ADDR:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> addr[counter]<span style="color:#f92672">&lt;=</span> sda;<span style="color:#75715e">// Capture address bit from SDA</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">if</span>(counter<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> state<span style="color:#f92672">&lt;=</span> SEND_ACK;<span style="color:#75715e">// Move to SEND_ACK after receiving full address</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span><span style="color:#66d9ef">else</span><span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> counter<span style="color:#f92672">&lt;=</span> counter<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Count down to receive 8 bits</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> SEND_ACK:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// Check if received address matches slave address (addr_in)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">if</span>(addr[<span style="color:#ae81ff">7</span><span style="color:#f92672">:</span><span style="color:#ae81ff">1</span>]<span style="color:#f92672">==</span> addr_in)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> counter<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">7</span>;<span style="color:#75715e">// Reset bit counter for next data frame</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#75715e">// Determine next state based on R/W bit (addr[0])</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">if</span>(addr[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> state<span style="color:#f92672">&lt;=</span> READ_DATA;<span style="color:#75715e">// If R/W=0, master wants to write, go to READ_DATA</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span><span style="color:#66d9ef">else</span><span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> state<span style="color:#f92672">&lt;=</span> WRITE_DATA;<span style="color:#75715e">// If R/W=1, master wants to read, go to WRITE_DATA</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span><span style="color:#66d9ef">else</span><span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> state<span style="color:#f92672">&lt;=</span> READ_ADDR;<span style="color:#75715e">// Address mismatch, go back to READ_ADDR</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> READ_DATA:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> data_in[counter]<span style="color:#f92672">&lt;=</span> sda;<span style="color:#75715e">// Capture data bit from SDA</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">if</span>(counter<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> state<span style="color:#f92672">&lt;=</span> SEND_ACK2;<span style="color:#75715e">// Move to SEND_ACK2 after receiving full byte</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span><span style="color:#66d9ef">else</span><span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> counter<span style="color:#f92672">&lt;=</span> counter<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Count down to receive 8 bits</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> SEND_ACK2:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> state<span style="color:#f92672">&lt;=</span> READ_ADDR;<span style="color:#75715e">// Go back to READ_ADDR to listen for next address</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> WRITE_DATA:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// Transmit data_out to master one bit at a time</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">if</span>(counter<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> state<span style="color:#f92672">&lt;=</span> READ_ADDR;<span style="color:#75715e">// After last bit, go back to READ_ADDR</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span><span style="color:#66d9ef">else</span><span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> counter<span style="color:#f92672">&lt;=</span> counter<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Count down for each bit sent</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#66d9ef">endcase</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// Control SDA output behavior on falling edge of SCL, depending on the state</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">always</span> @(<span style="color:#66d9ef">negedge</span> scl)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">case</span>(state)</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> READ_ADDR:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> write_enable<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Release SDA while reading address</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> SEND_ACK:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> sda_out<span style="color:#f92672">&lt;=</span> (addr[<span style="color:#ae81ff">7</span><span style="color:#f92672">:</span><span style="color:#ae81ff">1</span>]<span style="color:#f92672">==</span> addr_in)<span style="color:#f92672">?</span><span style="color:#ae81ff">0</span><span style="color:#f92672">:</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Send ACK (low) if address matches, else NACK (high)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> write_enable<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Enable SDA to drive ACK/NACK</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> READ_DATA:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> write_enable<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Release SDA while reading data</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> WRITE_DATA:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> sda_out<span style="color:#f92672">&lt;=</span> data_out[counter];<span style="color:#75715e">// Send each bit of data_out on SDA</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> write_enable<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Enable SDA to drive data</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> SEND_ACK2:<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> sda_out<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Send ACK (low) after receiving data</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> write_enable<span style="color:#f92672">&lt;=</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Enable SDA to drive ACK</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">endcase</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">endmodule</span></span></span></code></pre></div><h5 id="explanation-1">Explanation</h5><p>The Verilog code presented is for an I2C Slave Module that implements
the core logic for an I2C slave device capable of receiving and
transmitting data over the I2C protocol. Let&rsquo;s break down the components
of the code and their functionality:</p><h6 id="module-declaration-1">Module Declaration</h6><p>The module begins with the declaration of inputs and outputs:</p><blockquote><pre><code>module i2c_slave(
input [6:0] addr_in, // Dynamic address input for I2C slave
inout sda, // I2C data line (SDA)
inout scl // I2C clock line (SCL)
);</code></pre></blockquote><p>The inputs include a 7-bit address (<code>addr_in</code>) for the slave device,
along with the bidirectional<code>sda</code> and<code>scl</code> lines for data and clock
signals respectively.</p><h6 id="state-machine-definition-1">State Machine Definition</h6><p>The I2C protocol relies on a finite state machine (FSM) to control the
data transfer sequence. The FSM is represented by five states:</p><blockquote><pre><code>localparam READ_ADDR = 0;
localparam SEND_ACK = 1;
localparam READ_DATA = 2;
localparam WRITE_DATA = 3;
localparam SEND_ACK2 = 4;</code></pre></blockquote><p>Each state corresponds to a particular phase of the I2C communication: -<code>READ_ADDR</code>: Reads the I2C address and R/W bit. -<code>SEND_ACK</code>: Sends
acknowledgment (ACK) if the address matches. -<code>READ_DATA</code>: Receives
data from the master. -<code>WRITE_DATA</code>: Sends data to the master. -<code>SEND_ACK2</code>: Sends a second ACK after data reception.</p><h6 id="internal-registers-and-signals">Internal Registers and Signals</h6><p>Several internal registers and signals are declared to support the
functionality of the I2C slave: -<code>addr</code> holds the slave address and R/W
bit. -<code>counter</code> is used to count bits during transmission. -<code>state</code>
holds the current FSM state. -<code>data_in</code> and<code>data_out</code> store the
incoming and outgoing data, respectively. -<code>sda_out</code> and<code>sda_in</code>
control the data line (SDA). -<code>start</code> flags the detection of the I2C
start condition. -<code>write_enable</code> controls whether the slave can drive
the SDA line.</p><h6 id="sda-line-control">SDA Line Control</h6><p>The assignment of the<code>sda</code> line is conditional on the<code>write_enable</code>
signal:</p><blockquote><pre><code>assign sda = (write_enable == 1) ? sda_out : 'bz;</code></pre></blockquote><p>This means that the slave drives the<code>sda</code> line when<code>write_enable</code> is
active, otherwise, the line is in high-impedance state (&lsquo;bz).</p><h6 id="start-and-stop-condition-detection">Start and Stop Condition Detection</h6><p>The start condition is detected when there is a falling edge on the<code>sda</code> line while the<code>scl</code> line is high, and the stop condition is
detected when there is a rising edge on the<code>sda</code> line while<code>scl</code> is
high. These conditions trigger transitions in the FSM.</p><blockquote><pre><code>always @(negedge sda) begin
if ((start == 0) &amp;&amp; (scl == 1)) begin
start &lt;= 1; // Set start flag
counter &lt;= 7; // Initialize bit counter
end
end
always @(posedge sda) begin
if ((start == 1) &amp;&amp; (scl == 1)) begin
state &lt;= READ_ADDR; // Transition to READ_ADDR state
start &lt;= 0; // Reset start flag
write_enable &lt;= 0; // Disable write
end
end</code></pre></blockquote><p>These blocks capture the start and stop conditions and manage the FSM
transitions accordingly.</p><h6 id="fsm-logic-for-data-transfer">FSM Logic for Data Transfer</h6><p>The FSM operates on the rising edge of the<code>scl</code> signal, progressing
through various states based on the detected conditions:</p><blockquote><pre><code>always @(posedge scl) begin
if (start == 1) begin
case(state)
READ_ADDR: begin
addr[counter] &lt;= sda;
if(counter == 0) state &lt;= SEND_ACK;
else counter &lt;= counter - 1;
end
SEND_ACK: begin
if(addr[7:1] == addr_in) begin
counter &lt;= 7;
if(addr[0] == 0) begin
state &lt;= READ_DATA; // If write mode, move to READ_DATA
end else state &lt;= WRITE_DATA; // Else move to WRITE_DATA
end else state &lt;= READ_ADDR;
end
...
endcase
end
end</code></pre></blockquote><p>The state transitions depend on whether the address matches, the R/W
bit, and whether data is being read or written. The<code>SEND_ACK</code> state
sends an acknowledgment if the address is correct, while the<code>READ_DATA</code>
and<code>WRITE_DATA</code> states handle data reception and transmission
respectively.</p><h6 id="sda-output-logic-1">SDA Output Logic</h6><p>The logic for controlling the<code>sda</code> output during the FSM states is
defined in the following block:</p><blockquote><pre><code>always @(negedge scl) begin
case(state)
READ_ADDR: begin
write_enable &lt;= 0; // Disable writing during address read
end
SEND_ACK: begin
sda_out &lt;= (addr[7:1] == addr_in) ? 0 : 1; // Send ACK (0) or NACK (1)
write_enable &lt;= 1;
end
READ_DATA: begin
write_enable &lt;= 0; // Disable writing during data read
end
WRITE_DATA: begin
sda_out &lt;= data_out[counter]; // Output data bit by bit
write_enable &lt;= 1;
end
SEND_ACK2: begin
sda_out &lt;= 0; // Send ACK (0) after data reception
write_enable &lt;= 1;
end
endcase
end</code></pre></blockquote><p>Each state manipulates the<code>sda_out</code> signal to either send an
acknowledgment (ACK) or transmit the data bit by bit. The<code>SEND_ACK</code>
state checks the address match and sends either an ACK or NACK. The<code>WRITE_DATA</code> state sends the data, while the<code>SEND_ACK2</code> state sends an
ACK after data reception.</p><h6 id="summary">Summary</h6><p>This Verilog code implements a simple I2C Slave module that can handle
basic I2C communication. It includes start/stop condition detection,
address matching, data reception, and data transmission using an FSM.
The module can receive data from the I2C master, send data to it, and
properly acknowledge the master at each step in the communication
process.</p><h4 id="top-level-module">TOP-LEVEL MODULE</h4><h5 id="c0d3-2">C0d3</h5><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-verilog" data-lang="verilog"><span style="display:flex;"><span><span style="color:#66d9ef">`timescale</span><span style="color:#ae81ff">1</span>ns<span style="color:#f92672">/</span><span style="color:#ae81ff">1</span>ps</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// Top module to integrate i2c_master and i2c_slave</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// Top module to integrate i2c_master and i2c_slave</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">module</span> top(</span></span><span style="display:flex;"><span><span style="color:#66d9ef">input</span><span style="color:#66d9ef">wire</span> clk,<span style="color:#75715e">// System clock</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">input</span><span style="color:#66d9ef">wire</span> rst,<span style="color:#75715e">// Reset signal</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">input</span><span style="color:#66d9ef">wire</span> [<span style="color:#ae81ff">6</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] addr,<span style="color:#75715e">// 7-bit I2C address for the master to communicate with</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">input</span><span style="color:#66d9ef">wire</span> [<span style="color:#ae81ff">7</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] data_in,<span style="color:#75715e">// Data to be sent from the master to the slave</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">input</span><span style="color:#66d9ef">wire</span> enable,<span style="color:#75715e">// Enable signal to initiate I2C communication</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">input</span><span style="color:#66d9ef">wire</span> rw,<span style="color:#75715e">// Read/Write signal (0 = Write, 1 = Read)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">output</span><span style="color:#66d9ef">wire</span> [<span style="color:#ae81ff">7</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] data_out,<span style="color:#75715e">// Data received by the master from the slave</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">output</span><span style="color:#66d9ef">wire</span> ready,<span style="color:#75715e">// Signal indicating the master is ready for a new operation</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">inout</span><span style="color:#66d9ef">wire</span> i2c_sda,<span style="color:#75715e">// I2C data line (SDA) - bidirectional</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">inout</span><span style="color:#66d9ef">wire</span> i2c_scl<span style="color:#75715e">// I2C clock line (SCL)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/>);</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// Internal register to store the address the slave will respond to.</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#75715e">// This is the fixed address of the slave in this example.</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> [<span style="color:#ae81ff">6</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] slave_address<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span><span style="color:#ae81ff">'b0101010</span>;<span style="color:#75715e">// Example default slave address</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/></span></span><span style="display:flex;"><span><span style="color:#75715e">// Instantiate the I2C slave module</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> i2c_slave slave_inst (</span></span><span style="display:flex;"><span> .addr_in(slave_address),<span style="color:#75715e">// Provide the fixed slave address to the slave instance</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .sda(i2c_sda),<span style="color:#75715e">// Connect the slave's SDA line to the top-level SDA</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .scl(i2c_scl)<span style="color:#75715e">// Connect the slave's SCL line to the top-level SCL</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> );</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// Instantiate the I2C master module</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> i2c_master master_inst (</span></span><span style="display:flex;"><span> .clk(clk),<span style="color:#75715e">// Connect the system clock to the master</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .rst(rst),<span style="color:#75715e">// Connect the reset signal to the master</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .addr(addr),<span style="color:#75715e">// Provide the I2C address the master should communicate with</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .data_in(data_in),<span style="color:#75715e">// Data to be sent to the slave (if writing)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .enable(enable),<span style="color:#75715e">// Enable signal to start the I2C transaction</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .rw(rw),<span style="color:#75715e">// Read/Write signal (0 = Write, 1 = Read)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .data_out(data_out),<span style="color:#75715e">// Data received from the slave (if reading)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .ready(ready),<span style="color:#75715e">// Master ready signal indicating it's idle or ready for a new transaction</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .i2c_sda(i2c_sda),<span style="color:#75715e">// Connect the master's SDA line to the top-level SDA</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .i2c_scl(i2c_scl)<span style="color:#75715e">// Connect the master's SCL line to the top-level SCL</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> );</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#66d9ef">endmodule</span></span></span></code></pre></div><h5 id="explanation-2">Explanation</h5><ul><li><p>The<code>top</code> module connects an I2C master and slave module on shared<code>i2c_sda</code> and<code>i2c_scl</code> lines.</p></li><li><p>The<code>slave_address</code> register holds a predefined address used by the
slave.</p></li><li><p>The<code>i2c_slave</code> and<code>i2c_master</code> modules are instantiated and
connected to share the I2C lines and control signals.</p></li></ul><h4 id="testbench-module">TESTBENCH MODULE</h4><h5 id="c0d3-3">C0d3</h5><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-verilog" data-lang="verilog"><span style="display:flex;"><span><span style="color:#66d9ef">`timescale</span><span style="color:#ae81ff">1</span>ns<span style="color:#f92672">/</span><span style="color:#ae81ff">1</span>ps</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#66d9ef">module</span> i2c_controller_tb();</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// Inputs</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> clk;<span style="color:#75715e">// System clock</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> rst;<span style="color:#75715e">// Reset signal</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> [<span style="color:#ae81ff">6</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] addr;<span style="color:#75715e">// Address for the master to communicate with</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> [<span style="color:#ae81ff">7</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] data_in;<span style="color:#75715e">// Data to be sent from the master to the slave</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> enable;<span style="color:#75715e">// Enable signal to start communication</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> rw;<span style="color:#75715e">// Read/Write control (0 = Write, 1 = Read)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/></span></span><span style="display:flex;"><span><span style="color:#75715e">// Outputs</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">wire</span> [<span style="color:#ae81ff">7</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] data_out;<span style="color:#75715e">// Data received by the master from the slave</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">wire</span> ready;<span style="color:#75715e">// Ready signal indicating the master is ready for a new operation</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/></span></span><span style="display:flex;"><span><span style="color:#75715e">// Bidirectional wires</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">wire</span> i2c_sda;<span style="color:#75715e">// I2C data line (SDA) - shared between master and slave</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">wire</span> i2c_scl;<span style="color:#75715e">// I2C clock line (SCL) - shared between master and slave</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/></span></span><span style="display:flex;"><span><span style="color:#75715e">// Instantiate the Top Module (Device Under Test - DUT)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> top uut (</span></span><span style="display:flex;"><span> .clk(clk),<span style="color:#75715e">// Connect system clock to DUT</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .rst(rst),<span style="color:#75715e">// Connect reset signal to DUT</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .addr(addr),<span style="color:#75715e">// Connect address input to DUT</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .data_in(data_in),<span style="color:#75715e">// Connect data to be sent by master to DUT</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .enable(enable),<span style="color:#75715e">// Connect enable signal to DUT</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .rw(rw),<span style="color:#75715e">// Connect read/write control to DUT</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .data_out(data_out),<span style="color:#75715e">// Receive data read by master from DUT</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .ready(ready),<span style="color:#75715e">// Receive ready signal from DUT</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .i2c_sda(i2c_sda),<span style="color:#75715e">// Connect bidirectional SDA line</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> .i2c_scl(i2c_scl)<span style="color:#75715e">// Connect bidirectional SCL line</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> );</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// Clock generation</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">initial</span><span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> clk<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;</span></span><span style="display:flex;"><span><span style="color:#66d9ef">forever</span> #<span style="color:#ae81ff">1</span> clk<span style="color:#f92672">=</span><span style="color:#f92672">~</span>clk;<span style="color:#75715e">// Toggle clock every 1 ns to generate a 2 ns period clock (500 MHz)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// Test sequence to simulate I2C operations</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">initial</span><span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// Set up VCD file for waveform dumping</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> $dumpfile(<span style="color:#e6db74">"i2c_controller_tb.vcd"</span>);<span style="color:#75715e">// Name of the VCD file for waveform output</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> $dumpvars(<span style="color:#ae81ff">0</span>, i2c_controller_tb);<span style="color:#75715e">// Dump all variables in this module for waveform analysis</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/></span></span><span style="display:flex;"><span><span style="color:#75715e">// Initialize Inputs</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> rst<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Assert reset to initialize the system</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> enable<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Initially disable communication</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> addr<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span><span style="color:#ae81ff">'b0000000</span>;<span style="color:#75715e">// Set an initial address (not used immediately)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> data_in<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span><span style="color:#ae81ff">'b0</span>;<span style="color:#75715e">// Set initial data (not used immediately)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> rw<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Set initial operation to write (0 = Write, 1 = Read)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/></span></span><span style="display:flex;"><span><span style="color:#75715e">// Wait for reset to complete</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> #<span style="color:#ae81ff">10</span>;</span></span><span style="display:flex;"><span> rst<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Deassert reset after 10 ns to start normal operation</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/></span></span><span style="display:flex;"><span><span style="color:#75715e">// Test Case 1: Write operation with matching address (Expect ACK from slave)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> addr<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span><span style="color:#ae81ff">'b0101010</span>;<span style="color:#75715e">// Set address to match the slave address</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> data_in<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span><span style="color:#ae81ff">'b10101010</span>;<span style="color:#75715e">// Data to be sent to the slave</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> rw<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Set operation to write</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> enable<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Assert enable to start the I2C communication</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> #<span style="color:#ae81ff">20</span> enable<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Deassert enable after 20 ns to complete the command</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/></span></span><span style="display:flex;"><span><span style="color:#75715e">// Wait and observe response (slave should ACK the address and receive data)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> #<span style="color:#ae81ff">100</span>;</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// Test Case 2: Write operation with non-matching address (Expect NACK from slave)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> addr<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span><span style="color:#ae81ff">'b1111111</span>;<span style="color:#75715e">// Set address to a non-matching address for the slave</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> data_in<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span><span style="color:#ae81ff">'b11001100</span>;<span style="color:#75715e">// Different data to be sent to the slave</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> rw<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Set operation to write</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> enable<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Assert enable to start the I2C communication</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> #<span style="color:#ae81ff">20</span> enable<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Deassert enable after 20 ns</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/></span></span><span style="display:flex;"><span><span style="color:#75715e">// Wait and observe response (slave should NACK the address since it does not match)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> #<span style="color:#ae81ff">100</span>;</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#75715e">// Test Case 3: Read operation with matching address (Expect ACK from slave and read data)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> addr<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span><span style="color:#ae81ff">'b0101010</span>;<span style="color:#75715e">// Set address to match the slave address</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> rw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Set operation to read</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> enable<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>;<span style="color:#75715e">// Assert enable to start the I2C communication</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> #<span style="color:#ae81ff">20</span> enable<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;<span style="color:#75715e">// Deassert enable after 20 ns</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/></span></span><span style="display:flex;"><span><span style="color:#75715e">// Wait and observe response (slave should ACK the address and send data to master)</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> #<span style="color:#ae81ff">100</span>;</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span> #<span style="color:#ae81ff">200</span></span></span><span style="display:flex;"><span> $finish;<span style="color:#75715e">// End the simulation after 200 ns</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span><span style="color:#66d9ef">endmodule</span></span></span></code></pre></div><h5 id="explanation-3">Explanation</h5><ul><li><p><code>i2c_controller_tb</code>: Testbench module for the<code>top</code> module
integrating the master-slave I2C communication.</p></li><li><p>A clock signal is generated using a continuous<code>initial</code> block.</p></li><li><p>Test cases:</p><ul><li><p><strong>Test Case 1</strong>: Matches the slave address, expecting an ACK.</p></li><li><p><strong>Test Case 2</strong>: Uses a non-matching address, expecting a NACK.</p></li><li><p><strong>Test Case 3</strong>: Matches the address and tests a read operation.</p></li></ul></li><li><p>At the end of the test cases, the simulation finishes with<code>$finish</code>.</p></li></ul><h1 id="results">Results</h1><h4 id="test-case-1-address-match-with-write-operation">Test Case 1: Address Match with Write Operation</h4><figure><span class="image placeholder" data-original-image-src="TEST-CASE-1.jpg" data-original-image-title="" width="\textwidth"/><figcaption>Test Case 1</figcaption></figure><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="563" src="/images/projects/i2cv/TEST-CASE-2_hu_8b68e82ef4a99b68.webp" alt="Test Case 2" onerror="this.onerror='null';this.src='\/images\/projects\/i2cv\/TEST-CASE-2_hu_dc7648274cbac9ed.jpg'"/><p>The waveform in this test shows the master sending an address 7&rsquo;b0101010, which matches the slave’s configured address. Since the address matches, the slave acknowledges the communication by sending an ACK (acknowledgment) signal. After receiving the ACK, the master initiates a write operation, transmitting the data 8&rsquo;b10101010 to the slave. The enable signal is set to initiate communication and deasserted after a delay, allowing the transmission to complete. The presence of the ACK in this waveform confirms that the address was successfully matched, allowing data transfer.</p><figure><span class="image placeholder" data-original-image-src="TEST-CASE-2.jpg" data-original-image-title="" width="\textwidth"/><figcaption>Test Case 2</figcaption></figure><h4 id="test-case-2-address-mismatch-with-write-operation-expect-nack">Test Case 2: Address Mismatch with Write Operation (Expect NACK)</h4><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="563" src="/images/projects/i2cv/TEST-CASE-2_hu_8b68e82ef4a99b68.webp" alt="Test Case 3" onerror="this.onerror='null';this.src='\/images\/projects\/i2cv\/TEST-CASE-2_hu_dc7648274cbac9ed.jpg'"/><p>In this test case, the master sends a non-matching address 7&rsquo;b1111111, which does not correspond to the slave’s preset address. The waveform shows the absence of an acknowledgment signal (NACK) from the slave, indicating that the address verification failed and the data transfer cannot proceed. This NACK response is expected behavior when the slave does not recognize the transmitted address. The enable signal initiates the communication, but with no matching address and no ACK received, data transfer is not established.</p><figure><span class="image placeholder" data-original-image-src="TEST-CASE-3.jpg" data-original-image-title="" width="\textwidth"/><figcaption>Test Case 3</figcaption></figure><h4 id="test-case-3-address-match-with-read-operation">Test Case 3: Address Match with Read Operation</h4><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="563" src="/images/projects/i2cv/TEST-CASE-3_hu_562df21e0477b755.webp" alt="Test Case 3" onerror="this.onerror='null';this.src='\/images\/projects\/i2cv\/TEST-CASE-3_hu_c6ae172ab9572db3.jpg'"/><p>The final waveform demonstrates a read operation with the master sending a matching address 7&rsquo;b0101010. Upon recognizing the address, the slave responds with an ACK signal, confirming the communication link. Following the acknowledgment, the master initiates a read operation, and the slave provides the preloaded data 8&rsquo;b11001100 to the master. The enable signal triggers the start of communication and is deasserted after a delay, allowing the master to read the data. This successful transmission and receipt of data verify correct read functionality with an address match.</p><h3 id="challenges-and-risk-analysis">Challenges and Risk Analysis</h3><h4 id="potential-issues-and-solutions">Potential Issues and Solutions</h4><p>In the development of the I2C communication project, I encountered several challenges, starting with the design of basic Verilog modules for both master and slave entities capable of fundamental read and write operations. After an initial review of the I2C protocol from various online resources and Verilog syntax, I designed a preliminary testbench module to simulate and validate the basic functionality.</p><p>As the project progressed, I introduced additional functionality, including ACK and NACK flags, to handle incorrect slave addresses and refined the finite state machine (FSM) logic for more reliable state transitions. To make the slave module independent of global configurations, I designed a top-level module that instantiated both the master and slave modules and added an extra layer of address validation.</p><p>For synchronization, I implemented clock stretching between address and data frames. This addition worked effectively during address transmission but revealed timing issues during data frame read operations. After extensive troubleshooting, I identified limitations in the initial approach and modified the design accordingly.</p><p>I also explored a multi-master, multi-slave configuration, assigning slave addresses in the format<code>10101XY</code> (with<code>XY</code> values as 00, 01, 10, and 11 for slaves 1 through 4) and a master select line in the testbench to choose the active master. Preloaded data in each slave was structured as<code>110011XY</code> to streamline read operations. However, unresolved synchronization issues in the multi-master setup led me to scale back to a single-master, single-slave model, excluding clock stretching and multiple nodes. The final design focuses on single-point communication, with code attempts for the multi-master setup included in the project appendix on GitHub.</p><h4 id="risk-management">Risk Management</h4><p>Several potential risks emerged during the design and integration phases:</p><ul><li><p><strong>Design Complexity</strong>: The complexity of the I2C protocol and multi-node configuration posed unforeseen design challenges. A modular testing approach mitigated these risks by allowing iterative refinements.</p></li><li><p><strong>Timing Issues</strong>: Timing mismatches, particularly in multi-master configurations, impacted protocol accuracy. While resolved in the single-master model, this remains an area for future improvement.</p></li><li><p><strong>FSM Complexity</strong>: Introducing ACK/NACK handling increased the complexity of the FSM, raising the potential for state transition errors. Comprehensive simulation and debugging minimized these risks.</p></li></ul><p>In this project, I implemented the clock stretching functionality and addressed timing issues in data frame read operations. Starting with basic I2C modules, I managed the synchronization aspect by incorporating clock stretching between address and data frames, a mechanism crucial for addressing timing issues in the protocol. Despite progress, some discrepancies remain in the data frame during read operations, which are planned for future enhancements.</p><p>Additionally, I expanded the basic modules by introducing acknowledgment (ACK) and negative acknowledgment (NACK) flags to handle erroneous addresses. I refined the FSM logic to improve state transitions, address handling, and protocol management complexities. While the initial goal was a multi-master configuration with selectable slave nodes, unresolved timing issues led to a focus on a single-master, single-slave model, effectively capturing the core aspects of I2C communication in the final implementation.</p><h3 id="future-work-and-improvements">Future Work and Improvements</h3><h4 id="suggested-enhancements">Suggested Enhancements</h4><p>Future enhancements to this project could include adding more registers
to each slave, allowing for more sophisticated data handling. Additional
registers would enable more extensive data storage and retrieval options
in each slave device, making the project closer to real-world I2C
applications.</p><h4 id="alternative-designs">Alternative Designs</h4><p>Exploring alternative FSM architectures could improve the efficiency and
stability of the I2C protocol, especially for multi-master
configurations. Further, advanced data synchronization techniques,
possibly through modified clock stretching or data frame timing
adjustments, could address the current timing issues. Replacing the
current point-to-point master-slave setup with a robust multi-node
configuration, if resolved, could significantly enhance the protocol&rsquo;s
scalability.</p><h3 id="appendices">Appendices</h3><h4 id="verilog-code-listings">Verilog Code Listings</h4><p>The complete Verilog code for the I2C Master module, including support
for multi-master/slave configuration and clock stretching, is available
in the following GitHub repository:</p><p><strong>Repository:</strong><a href="https://github.com/Mummanajagadeesh/I2C-protocol-verilog" target="_blank">https://github.com/Mummanajagadeesh/I2C-protocol-verilog</a></p><h4 id="references">References</h4><ol><li><p>Texas Instruments,<em>A Basic Guide to I2C</em>, Available at:<a href="https://www.ti.com/lit/pdf/sbaa565" target="_blank">https://www.ti.com/lit/pdf/sbaa565</a></p></li><li><p>Prodigy Technoinnovations,<em>I2C Protocol</em>, Available at:<a href="https://www.prodigytechno.com/i2c-protocol" target="_blank">https://www.prodigytechno.com/i2c-protocol</a></p></li><li><p>SparkFun,<em>I2C Tutorial</em>, Available at:<a href="https://learn.sparkfun.com/tutorials/i2c/all" target="_blank">https://learn.sparkfun.com/tutorials/i2c/all</a></p></li><li><p>Class Lectures,<em>Verilog Code Syntax</em></p></li></ol><blockquote><p>Most images in this document are adapted from the above
resources. All images are copyrighted by their respective owners; no
ownership rights are claimed.</p></blockquote>
]]></content:encoded></item><item><title/><link>https://mummanajagadeesh.github.io/projects/rubec/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/projects/rubec/</guid><description>&lt;![CDATA[<h2 id="ru83c-rubik"><a href="https://github.com/Mummanajagadeesh/RU83C/" target="_blank">RU83C: Rubik&rsquo;s Cube Solving Robot</a></h2><table><thead><tr><th><strong>Name</strong></th><th>RU83C</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>Rubik&rsquo;s Cube-solving robot using Kociemba algorithm, featuring computer vision for state detection, mechanical design for cube manipulation, and electronics for execution.</td></tr><tr><td><strong>Start</strong></td><td>Ideation(July 2023), Implementation(Aug 2023)</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/RU83C/" target="_blank">RU83C🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Algorithms, Programming, Game Dev</td></tr><tr><td><strong>Tools Used</strong></td><td>Blender, Unity3D, Python, C#, VS Code, OpenCV, Fusion 360, ArduinoIDE</td></tr><tr><td><strong>Current Status</strong></td><td>Ongoing (Passive)</td></tr><tr><td><strong>Progress</strong></td><td>- Unity3D implementation is done.<br> - Mechanical Design is started in Fusion 360<br> - CV part code is ready, color ranges yet to be tuned</td></tr><tr><td><strong>Next Steps</strong></td><td>- CV (Computer Vision): Responsible for recognizing the scrambled state of the cube via a camera.<br> - Mechanical Design: Focused on the creation of the holder and gripping mechanisms to manipulate the cube.<br> - Electronics: Controls and coordinates the robot’s movements based on the computed solution.</td></tr></tbody></table><hr><h4 id="overview">Overview</h4><p>RU83C is a Rubik&rsquo;s Cube-solving robot that leverages computer vision (CV), mechanical design, and electronics to solve a scrambled Rubik&rsquo;s Cube using the Kociemba algorithm. The robot features a camera that captures the current state of the scrambled cube, processes the image using CV, and calculates the solution. A mechanical holder grips the cube securely, and the robot executes the necessary moves to solve it.</p>]]></description><content:encoded>&lt;![CDATA[<h2 id="ru83c-rubik"><a href="https://github.com/Mummanajagadeesh/RU83C/" target="_blank">RU83C: Rubik&rsquo;s Cube Solving Robot</a></h2><table><thead><tr><th><strong>Name</strong></th><th>RU83C</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>Rubik&rsquo;s Cube-solving robot using Kociemba algorithm, featuring computer vision for state detection, mechanical design for cube manipulation, and electronics for execution.</td></tr><tr><td><strong>Start</strong></td><td>Ideation(July 2023), Implementation(Aug 2023)</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/RU83C/" target="_blank">RU83C🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Algorithms, Programming, Game Dev</td></tr><tr><td><strong>Tools Used</strong></td><td>Blender, Unity3D, Python, C#, VS Code, OpenCV, Fusion 360, ArduinoIDE</td></tr><tr><td><strong>Current Status</strong></td><td>Ongoing (Passive)</td></tr><tr><td><strong>Progress</strong></td><td>- Unity3D implementation is done.<br> - Mechanical Design is started in Fusion 360<br> - CV part code is ready, color ranges yet to be tuned</td></tr><tr><td><strong>Next Steps</strong></td><td>- CV (Computer Vision): Responsible for recognizing the scrambled state of the cube via a camera.<br> - Mechanical Design: Focused on the creation of the holder and gripping mechanisms to manipulate the cube.<br> - Electronics: Controls and coordinates the robot’s movements based on the computed solution.</td></tr></tbody></table><hr><h4 id="overview">Overview</h4><p>RU83C is a Rubik&rsquo;s Cube-solving robot that leverages computer vision (CV), mechanical design, and electronics to solve a scrambled Rubik&rsquo;s Cube using the Kociemba algorithm. The robot features a camera that captures the current state of the scrambled cube, processes the image using CV, and calculates the solution. A mechanical holder grips the cube securely, and the robot executes the necessary moves to solve it.</p><h4 id="features">Features</h4><p><strong>Computer Vision</strong>: A camera-based system to detect the current state of the Rubik&rsquo;s Cube, using image processing techniques to translate the visual input into a digital cube representation.</p><p><strong>Mechanical Design</strong>: A custom-built holder with an efficient gripping system to securely hold and rotate the cube while executing the solution.</p><p><strong>Kociemba Algorithm</strong>: The Kociemba algorithm is employed to calculate the optimal solution for the scrambled state.</p><p><strong>Electronics</strong>: The system uses electronics to control the motors and mechanical components, translating the calculated moves into physical actions.</p><h4 id="structure">Structure</h4><p>The project is divided into several key parts:</p><ol><li><p>CV (Computer Vision): Responsible for recognizing the scrambled state of the cube via a camera.</p></li><li><p>Mechanical Design: Focused on the creation of the holder and gripping mechanisms to manipulate the cube.</p></li><li><p>Algorithm: Implementation of the Kociemba algorithm to calculate the solution to any scrambled state.</p></li><li><p>Electronics: Controls and coordinates the robot’s movements based on the computed solution.</p></li></ol><h4 id="base-version">Base Version</h4><p>A base version of this project was previously developed in simulation, where users could manually or automatically scramble a virtual cube in Unity. The solution for the scrambled cube was calculated and displayed in the virtual environment. This was an intermediate step, and the current version represents the full hardware implementation of the Rubik&rsquo;s Cube-solving robot.</p><p>BASE VERSION:<a href="https://github.com/Mummanajagadeesh/V-RU81K5CU83" target="_blank">here</a> &ndash;implemented in simulation using Unity3D(C#)</p><h2 id="base-version-1">BASE VERSION</h2><h3 id="v-ru81k5cu83---virtual-rubik"><a href="https://github.com/Mummanajagadeesh/V-RU81K5CU83" target="_blank">V-RU81K5CU83 - Virtual Rubik&rsquo;s Cube Using Kociemba Solver</a></h3><p>A virtual Rubik&rsquo;s Cube implemented using the<strong>Kociemba algorithm</strong> for solving scrambled states. This project offers an interactive 3D simulation of a Rubik&rsquo;s Cube that you can scramble, manipulate manually, and solve. The solution is computed using the Kociemba two-phase algorithm, widely known for its efficiency in solving Rubik&rsquo;s cubes with minimal moves.</p><p>Deployment: This project is deployed using a WebGL server with Node.js alongside GitHub Pages for hosting the live demo.</p><p><strong>INSPIRATION:</strong><a href="https://www.megalomobile.com/" target="_blank">@Megalomobile</a></p><p><strong><a href="https://mummanajagadeesh.github.io/v-cube-host/" target="_blank">Click here for the Live Demo</a> |<a href="https://v-cube-host.vercel.app/" target="_blank">Vercel</a></strong></p><h4 id="features-1">Features</h4><ul><li><strong>Interactive 3D Cube</strong>: Manipulate the cube by dragging and rotating different layers in 3D space.</li><li><strong>Scramble &amp; Solve</strong>: Automatically scramble the cube or solve any configuration using the Kociemba algorithm.</li><li><strong>Unity/Blender Simulation</strong>: Visual simulations and demos showcasing the solution steps.</li></ul><h4 id="demo-videos">Demo Videos</h4><table><thead><tr><th>Unity 3D Demo</th><th>Blender Solution Demo</th></tr></thead><tbody><tr><td><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/L4s2YYyi-70" frameborder="0" allowfullscreen=/></div></div></td><td><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/pQN5wu2dtTQ" frameborder="0" allowfullscreen=/></div></div></td></tr></tbody></table><h4 id="project-overview">Project Overview</h4><p>This project leverages the pre-existing<strong>Kociemba two-phase algorithm</strong> for Rubik&rsquo;s Cube solving, and our main contribution has been the seamless integration of this algorithm into a Unity-based simulation. The role of the solver is crucial, as it computes the solution when a scrambled cube is presented.</p><p>We designed the system to fetch cube states directly from the user interface, where users can scramble or manipulate the cube. Upon clicking the &ldquo;Solve&rdquo; button, the Kociemba algorithm works under the hood to generate an optimal solution, which is then passed to the Unity engine. The Unity environment simulates this solution visually, step by step, allowing users to see the cube&rsquo;s transformation in real-time.</p><p>The<strong>Blender solution demo</strong> featured here is purely a visual mockup, simulating how the cube might appear when following a solved sequence. It serves as a demonstration for visual feedback but has no direct involvement in the solving process or algorithm implementation. This is an intermediate step showcasing the solution process and will be further integrated into Unity for enhanced user interaction.</p><h2 id="kociembas-algorithm-for-solving-a-rubiks-cube"><strong>Kociemba’s Algorithm for Solving a Rubik’s Cube</strong></h2><p>Kociemba’s Algorithm is a two-phase algorithm used to efficiently solve a 3×3 Rubik’s Cube in a minimal number of moves. It is an advanced method that improves upon simpler approaches, such as layer-by-layer (LBL) solving, by reducing the number of moves required to solve the cube.</p><p>Kociemba’s Algorithm is often used in optimal cube solvers, and it serves as the basis for algorithms like Herbert Kociemba’s Cube Explorer and the well-known<strong>Two-Phase Algorithm</strong>.</p><hr><h4 id="overview-of-the-algorithm"><strong>Overview of the Algorithm</strong></h4><p>The algorithm consists of two main phases:</p><ol><li><strong>Phase 1</strong>: Reducing the cube to a subset of solvable states known as the<strong>&ldquo;G1 group&rdquo;</strong>.</li><li><strong>Phase 2</strong>: Solving the cube optimally from this subset.</li></ol><p>By breaking the problem into these two phases, Kociemba’s Algorithm achieves solutions that typically require around 20 moves (or fewer), which is significantly shorter than beginner methods.</p><hr><h4 id="mathematical-background"><strong>Mathematical Background</strong></h4><p>The algorithm relies on<strong>group theory</strong>, particularly the concept of<strong>cosets</strong> and<strong>group reductions</strong>. In simple terms, it works by reducing the number of legal states the cube can be in while maintaining solvability.</p><p>The Rubik’s Cube has a<strong>state space</strong> of<strong>43,252,003,274,489,856,000 (43 quintillion) possible arrangements</strong>. Finding an optimal solution in this space is extremely complex, but by using the concept of<strong>group reduction</strong>, the problem is split into two more manageable parts.</p><hr><h4 id="phase-1-reduction-to-g1-group"><strong>PHASE 1: Reduction to G1 Group</strong></h4><p>In this phase, the cube is transformed into a<strong>restricted subset</strong> of states where:</p><ul><li>All edge orientations are correct (edges are in the correct flipped orientation).</li><li>All corner orientations are correct.</li><li>The positioning of the U/D (Up/Down) face edges falls into a specific allowed pattern.</li></ul><p>This phase<strong>does not solve</strong> the cube completely but simplifies it to a structured form that is easier to solve in the second phase.</p><h6 id="mathematical-properties-of-g1-group"><strong>Mathematical Properties of G1 Group</strong></h6><p>The group G1 is a subset of all possible cube states that satisfies the following conditions:</p><ol><li><strong>Edge Orientation is Solved</strong>: Each edge must be in its correct orientation.</li><li><strong>Corner Orientation is Solved</strong>: Each corner must be in its correct orientation.</li><li><strong>UD-Slice Edges Must Stay in the UD-Slice</strong>: The four edges belonging to the U (Up) and D (Down) face centers must remain within that slice.</li></ol><p>The key insight is that reducing the cube to this state significantly reduces the number of possible positions, making the final solving phase much easier.</p><h6 id="how-this-phase-works"><strong>How This Phase Works</strong></h6><ul><li>Kociemba’s algorithm uses a<strong>pruning table</strong> that precomputes the minimum number of moves required to reach the G1 state from any given configuration.</li><li>Using<strong>bidirectional search</strong>, the algorithm efficiently finds the shortest path to reach G1.</li><li>Typically, this phase takes<strong>at most 12 moves</strong>.</li></ul><hr><h4 id="phase-2-solving-the-cube-from-g1"><strong>PHASE 2: Solving the Cube from G1</strong></h4><p>Once the cube is in the<strong>G1 subset</strong>, the next step is to solve it completely while maintaining the constraints of G1.</p><h6 id="mathematical-properties-of-phase-2"><strong>Mathematical Properties of Phase 2</strong></h6><p>In this phase, we solve the cube while ensuring:</p><ol><li><strong>The E-Slice (Middle Layer Edges) is Correct</strong>: The four middle layer edges must be correctly positioned.</li><li><strong>Corner Permutation is Correct</strong>: The corners must be arranged correctly.</li><li><strong>Edge Permutation is Correct</strong>: All edges must be positioned correctly.</li></ol><p>At this point, the cube is already simplified, so a<strong>brute-force or optimized search</strong> (using a<strong>pruning table</strong>) is performed to find the shortest solution sequence.</p><ul><li>This phase usually takes around<strong>10 moves</strong> in most cases.</li><li>Since the cube is already in a reduced state,<strong>only a limited number of moves are needed</strong> to reach the solved state.</li></ul><hr><h4 id="key-techniques-used-in-kociembas-algorithm"><strong>Key Techniques Used in Kociemba’s Algorithm</strong></h4><h6 id="pruning-tables"><strong>Pruning Tables</strong></h6><ul><li>The algorithm precomputes<strong>lookup tables</strong> that store the shortest paths for solving different cube states.</li><li>These tables help the algorithm efficiently determine the best next move without unnecessary computations.</li></ul><h6 id="heuristic-search"><strong>Heuristic Search</strong></h6><ul><li>Kociemba’s Algorithm uses<em><em>A</em> search</em>* (or similar algorithms) to minimize the number of moves required to reach a solved state.</li><li>It evaluates different move sequences and picks the shortest one.</li></ul><h6 id="group-theory-based-reduction"><strong>Group Theory-Based Reduction</strong></h6><ul><li>The algorithm does not directly solve the cube but instead<strong>reduces it</strong> step by step to smaller solvable subsets.</li><li>By using<strong>cosets</strong> and<strong>subgroups</strong>, the problem is broken down into manageable parts.</li></ul><h6 id="move-pruning--bidirectional-search"><strong>Move Pruning &amp; Bidirectional Search</strong></h6><ul><li>The algorithm avoids unnecessary moves by pruning branches that lead to longer solutions.</li><li>It uses<strong>bidirectional search</strong> (searching both forward and backward) to quickly find the optimal solution.</li></ul><hr><h4 id="why-is-kociembas-algorithm-efficient"><strong>Why is Kociemba’s Algorithm Efficient?</strong></h4><p><strong>It significantly reduces the search space</strong>: Instead of searching through 43 quintillion possible states, it first reduces the cube to G1, making the search much easier.<strong>It finds near-optimal solutions</strong>: While not always the absolute shortest solution, Kociemba’s Algorithm typically finds solutions within 20 moves, close to the<strong>God’s Number</strong> (20 moves).<strong>It is practical for human and computer solvers</strong>: Many advanced cube solvers and speedcubing programs use this approach for efficient solving.</p><hr><h4 id="comparison-to-other-solving-methods"><strong>Comparison to Other Solving Methods</strong></h4><table><thead><tr><th>Algorithm</th><th>Average Move Count</th><th>Approach</th></tr></thead><tbody><tr><td>Layer-by-Layer (Beginner Method)</td><td>100+ moves</td><td>Step-by-step solving</td></tr><tr><td>CFOP (Fridrich Method)</td><td>50-60 moves</td><td>Speedcubing method</td></tr><tr><td>Kociemba’s Algorithm</td><td>~20 moves</td><td>Two-phase optimal solving</td></tr><tr><td>Thistlethwaite’s Algorithm</td><td>40-50 moves</td><td>Four-phase group theory approach</td></tr><tr><td>God’s Algorithm</td><td>20 moves (optimal)</td><td>Brute-force minimum solution</td></tr></tbody></table><p>Kociemba’s Algorithm is<strong>not always optimal</strong>, but it is extremely<strong>efficient</strong> and can be computed<strong>very quickly</strong> compared to brute-force approaches.</p><hr><h4 id="applications-of-kociembas-algorithm"><strong>Applications of Kociemba’s Algorithm</strong></h4><ul><li><strong>Speedcubing</strong>: Used in optimal cube-solving software.</li><li><strong>Computer Solvers</strong>: Implemented in AI-based solvers and robotic Rubik’s Cube solvers.</li><li><strong>Mathematical Research</strong>: Used to study<strong>group theory and combinatorial optimization</strong>.</li><li><strong>God’s Number Research</strong>: Helps find efficient solutions near the<strong>20-move optimal bound</strong>.</li></ul><hr><h4 id="controls">Controls</h4><ul><li><strong>Right Click + Drag</strong>: Rotate the entire cube in 3D space.</li><li><strong>Left Click + Drag on Layers</strong>: Rotate specific layers of the Rubik&rsquo;s Cube.</li></ul><h4 id="buttons">Buttons</h4><ul><li><strong>SCRAMBLE</strong>: Randomly scrambles the Rubik&rsquo;s Cube.</li><li><strong>SOLVE</strong>: Solves the scrambled cube using the Kociemba solver. (Note: Initial solve may take some time to compute.)</li></ul><h5 style="text-align: center;">Layout</h5><img title="" loading="lazy" decoding="async" class="img  " width="1201" height="802" src="/images/projects/rubec/layout_hu_2debb8c2264ac096.webp" alt="Unity Layout" onerror="this.onerror='null';this.src='\/images\/projects\/rubec\/layout_hu_a6cf289f63060703.png'"/><script>
window.addEventListener("load", (e) => {
const lightbox = GLightbox();
});</script><h4 id="installation--setup">Installation &amp; Setup</h4><p>To run the project locally, follow these steps:</p><ol><li><p>Clone the repository:</p><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/Mummanajagadeesh/v-rubiks-cube.git</span></span><span style="display:flex;"><span>cd v-rubiks-cube</span></span></code></pre></div></li><li><p>Open the project in your preferred IDE or deploy it on a web hosting service.</p></li><li><p>To modify or extend the project, ensure you have:</p><ul><li>A working knowledge of 3D engines such as Unity or Blender.</li><li>Familiarity with the Kociemba algorithm and basic Rubik’s Cube concepts.</li></ul></li></ol><h4 id="how-the-solver-works">How the Solver Works</h4><p>This project uses the<strong>Kociemba two-phase algorithm</strong>, which is an optimized approach for solving the Rubik&rsquo;s Cube in under 20 moves. The first phase reduces the problem to a manageable subset of configurations, and the second phase finds an optimal solution from that subset.</p><h4 id="performance-notes">Performance Notes</h4><ul><li>The first time you run the solver, it may take longer due to the initial calculation of move tables.</li><li>Subsequent solves will be faster as the tables are cached.</li></ul><h4 id="future-goals">Future Goals</h4><p>We have exciting plans for the future of this project:</p><ul><li><strong>Hardware Integration</strong>: The next step is to bring this solution into the physical world. Using computer vision (CV) techniques, we aim to read the Rubik&rsquo;s Cube faces via a camera and translate the detected state into a solvable format.</li><li><strong>3D Design</strong>: We are currently working on the 3D design of the hardware, which is under construction. This will involve a mechanism where gripper-like structures hold and manipulate the cube for solving.</li><li><strong>Optimization</strong>: Another goal is to reduce the time required for solving, making the process as fast and efficient as possible.</li><li><strong>Physical Cube Solving</strong>: Ultimately, the project will be able to solve a real Rubik&rsquo;s Cube using robotic structures that simulate the virtual environment.</li></ul><p>We are open to contributions from anyone interested in participating in this exciting journey. If you&rsquo;d like to help us push the project forward, feel free to reach out!</p><p>Enjoy solving the cube! 🎲</p>
]]></content:encoded></item><item><title/><link>https://mummanajagadeesh.github.io/projects/shopping-cart-bot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/projects/shopping-cart-bot/</guid><description>&lt;![CDATA[<h2 id="shopping-cart-bot"><a href="https://github.com/Mummanajagadeesh/shopping-cart-bot-rig" target="_blank">Shopping Cart Bot</a></h2><table><thead><tr><th><strong>Name</strong></th><th>Shopping Cart Bot</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>The Shopping Cart Bot is a robotics project designed to automate the shopping experience by utilizing computer vision and autonomous navigation. The bot follows a person, detects and classifies items placed in the cart, and categorizes them based on predefined labels such as food, electronics, and clothing. Additionally, it integrates barcode recognition, label detection, and a payment system to streamline the checkout process.</td></tr><tr><td><strong>Start</strong></td><td>30 Sep 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/shopping-cart-bot-rig" target="_blank">Shopping Cart Bot🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Computer Vision, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Webots, ROS2, GeminiAPI, PyQt, Python, OpenCV</td></tr><tr><td><strong>Current Status</strong></td><td>On Hold</td></tr></tbody></table><p>The<strong>Shopping Cart Bot</strong> is a robotics project designed to automate the shopping experience by utilizing computer vision and autonomous navigation. The bot follows a person, detects and classifies items placed in the cart, and categorizes them based on predefined labels such as food, electronics, and clothing. Additionally, it integrates barcode recognition, label detection, and a payment system to streamline the checkout process.</p>]]></description><content:encoded>&lt;![CDATA[<h2 id="shopping-cart-bot"><a href="https://github.com/Mummanajagadeesh/shopping-cart-bot-rig" target="_blank">Shopping Cart Bot</a></h2><table><thead><tr><th><strong>Name</strong></th><th>Shopping Cart Bot</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>The Shopping Cart Bot is a robotics project designed to automate the shopping experience by utilizing computer vision and autonomous navigation. The bot follows a person, detects and classifies items placed in the cart, and categorizes them based on predefined labels such as food, electronics, and clothing. Additionally, it integrates barcode recognition, label detection, and a payment system to streamline the checkout process.</td></tr><tr><td><strong>Start</strong></td><td>30 Sep 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/shopping-cart-bot-rig" target="_blank">Shopping Cart Bot🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>Computer Vision, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Webots, ROS2, GeminiAPI, PyQt, Python, OpenCV</td></tr><tr><td><strong>Current Status</strong></td><td>On Hold</td></tr></tbody></table><p>The<strong>Shopping Cart Bot</strong> is a robotics project designed to automate the shopping experience by utilizing computer vision and autonomous navigation. The bot follows a person, detects and classifies items placed in the cart, and categorizes them based on predefined labels such as food, electronics, and clothing. Additionally, it integrates barcode recognition, label detection, and a payment system to streamline the checkout process.</p><h4 id="problem-statement">Problem Statement</h4><p>The project aims to address the following challenges:</p><ul><li><strong>Object Detection and Classification:</strong> Implement computer vision techniques to identify and categorize products.</li><li><strong>Person Following:</strong> Develop a system that enables the bot to follow a shopper autonomously.</li><li><strong>Barcode and Label Recognition:</strong> Use image processing and OCR to extract information from labels and barcodes.</li><li><strong>Discount Application and Payment System:</strong> Calculate the final bill with category-wise discounts and generate a QR code for payment.</li></ul><h4 id="motivation">Motivation</h4><p>This project was developed as part of the<strong>Round 3 induction project for my college robotics club</strong>. The objective was to explore the integration of robotics and computer vision in real-world applications, enhancing the retail shopping experience.</p><h4 id="hardware-design">Hardware Design</h4><ul><li><strong>Frame:</strong> Designed using aluminum extrusion in<strong>Fusion 360</strong>.</li><li><strong>Wheels:</strong> Mecanum wheels were chosen due to the<strong>restricted motion in malls</strong> (tight spaces, high foot traffic, and the need for omnidirectional movement).</li></ul><h4 id="person-following-implementation">Person Following Implementation</h4><h5 id="simulation-environment"><strong>Simulation Environment</strong></h5><ul><li>Implemented in<strong>Webots</strong> simulation.</li><li>The world was divided into separate tracks for the person to walk and collect items.</li><li>The cart was equipped with a<strong>camera and GPS</strong>, mounted on a mecanum-wheeled base.</li><li>The person was equipped with a<strong>GPS sensor</strong> embedded in their body slot, transmitting signals via a dedicated channel.</li><li>The bot’s camera continuously monitored the person, following them wherever they moved.</li><li>The<strong>GPS signal</strong> served as an additional reference for tracking the person’s location.</li></ul><style>
.youtube-container {
width: 100%;
}
.youtube-container .youtube-embed {
position: relative;
width: 100%;
padding-bottom: 56.25%;
height: 0;
}
.youtube-container .youtube-embed iframe {
position: absolute;
width: 100%;
height: 100%;
top: 0;
left: 0;
}
table {
width: 100%;
table-layout: fixed;
}
td {
padding: 10px;
vertical-align: top;
}</style><div class="youtube-container"><div class="youtube-embed"><iframe src="https://www.youtube.com/embed/7x07H0RtYMg" frameborder="0" allowfullscreen=/></div></div><h5 id="unimplemented-features-due-to-time-constraints"><strong>Unimplemented Features Due to Time Constraints</strong></h5><ul><li>Handling the scenario where the person moves out of the camera frame, using GPS as a fallback.</li><li>Detecting when the bot is stuck without any input stimulus.</li></ul><h4 id="object-detection-and-classification">Object Detection and Classification</h4><ul><li><p><strong>YOLOv5 Model:</strong> Trained using a<strong>Roboflow dataset</strong> to detect and categorize shopping items.</p></li><li><p><strong>Label Detection Pipeline:</strong></p><ul><li>Convert the image from<strong>RGB to grayscale</strong>.</li><li>Apply<strong>Canny edge detection</strong>.</li><li>Identify the<strong>largest contour</strong> and create a<strong>bounding box</strong> around the label.</li><li>Extract the<strong>bounded region</strong> from the original image.</li><li>Perform<strong>OCR (Optical Character Recognition)</strong> using an API to extract text.</li><li>Utilize<strong>Gemini API</strong> for text correction and cost retrieval.</li></ul><video width="1000" autoplay= loop= muted= controls= class="embed-responsive "><source src="/images/projects/scbot/shpbotrecog.mp4" type="video/mp4"/>
Your browser does not support the video tag.</video></li></ul><blockquote><p>Implemented a ROS2 node to enable communication between mecanum base motors and code via a UDP server.</p></blockquote><h4 id="payment-system">Payment System</h4><ul><li><p>The total cost is calculated based on item quantity, category-wise discounts, and final pricing.</p></li><li><p>A<strong>UPI URL</strong> is used to generate a<strong>QR code</strong>.</p></li><li><p>Scanning the QR code prompts the user to pay the exact calculated amount.</p><video width="500" autoplay= loop= muted= controls= class="embed-responsive "><source src="/images/projects/scbot/upiqr.mp4" type="video/mp4"/>
Your browser does not support the video tag.</video></li></ul><p>The<strong>Shopping Cart Bot</strong> successfully integrates multiple robotics and computer vision techniques to automate the shopping experience. Despite some unimplemented features due to time constraints, the project demonstrates a proof of concept for autonomous retail assistance.</p><hr><h5 id="future-improvements">Future Improvements</h5><ul><li>Implement fallback mechanisms for lost person tracking.</li><li>Enhance navigation strategies for obstacle avoidance.</li><li>Develop a fully integrated hardware prototype for real-world testing.</li></ul>
]]></content:encoded></item><item><title/><link>https://mummanajagadeesh.github.io/projects/tlcv/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/projects/tlcv/</guid><description>&lt;![CDATA[<h2 id="traffic-light-controller-"><a href="https://github.com/Mummanajagadeesh/TrafficLightController-verilog" target="_blank">TRAFFIC LIGHT CONTROLLER 🚦</a></h2><table><thead><tr><th><strong>Name</strong></th><th>TLC using Verilog</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>Verilog Implementation of Traffic Light Controller</td></tr><tr><td><strong>Start</strong></td><td>05 Apr 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/TrafficLightController-verilog" target="_blank">TLCV🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>HDL, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Verilog, Icarus, Xilinx</td></tr><tr><td><strong>Current Status</strong></td><td>On Hold</td></tr></tbody></table><hr><blockquote><p>The original version, which utilizes FSM, is currently on hold. Here is the base version, which does not use FSM and instead relies directly on Boolean logic expressions. Below is the explanation for the base version.</p>]]></description><content:encoded>&lt;![CDATA[<h2 id="traffic-light-controller-"><a href="https://github.com/Mummanajagadeesh/TrafficLightController-verilog" target="_blank">TRAFFIC LIGHT CONTROLLER 🚦</a></h2><table><thead><tr><th><strong>Name</strong></th><th>TLC using Verilog</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>Verilog Implementation of Traffic Light Controller</td></tr><tr><td><strong>Start</strong></td><td>05 Apr 2024</td></tr><tr><td><strong>Repository</strong></td><td><a href="https://github.com/Mummanajagadeesh/TrafficLightController-verilog" target="_blank">TLCV🔗</a></td></tr><tr><td><strong>Type</strong></td><td>Individual</td></tr><tr><td><strong>Level</strong></td><td>Beginner</td></tr><tr><td><strong>Skills</strong></td><td>HDL, Programming</td></tr><tr><td><strong>Tools Used</strong></td><td>Verilog, Icarus, Xilinx</td></tr><tr><td><strong>Current Status</strong></td><td>On Hold</td></tr></tbody></table><hr><blockquote><p>The original version, which utilizes FSM, is currently on hold. Here is the base version, which does not use FSM and instead relies directly on Boolean logic expressions. Below is the explanation for the base version.</p></blockquote><h2 id="base-version">BASE VERSION</h2><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="737" src="/images/projects/tlc/lane-picture_hu_925fb90806d9713a.webp" alt="Original Image 1" onerror="this.onerror='null';this.src='\/images\/projects\/tlc\/lane-picture_hu_9ba873098c1d9360.png'"/><script>
window.addEventListener("load", (e) => {
const lightbox = GLightbox();
});</script><p>Consider the problem of controlling a traffic light at the intersection of two equally busy streets, A Street and B Street. Our traffic light controller takes two inputs – CarA (which is high when there is a car just before the intersection on A Street – in either direction), and CarB (which is high when there is a car just before the intersection on B street). The controller needs to generate six outputs – RedA, YellowA, GreenA, RedB, YellowB, and GreenB – which drive the respective traffic lights for A Street and B Street. In the figure above, CarA will be high, since there is a car (the rectangle) on A Street, and CarB will be low, since there is no car on B Street. Also in the Figure RedA is high since A Street has a red light, and GreenB is high since B Street has a green light. All other outputs are low. We can think of the traffic light controller as a black box that takes two inputs (and a clock) and generates six outputs as shown below.</p><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="689" src="/images/projects/tlc/tlcblock_hu_2f90ae0f3fc55180.webp" alt="Original Image 1" onerror="this.onerror='null';this.src='\/images\/projects\/tlc\/tlcblock_hu_22144728c1d7f863.png'"/><ol><li>When the light is green on A Street and a car is waiting on B Street, give A Street a yellow light for one clock cycle and then give A Street a red light and B Street a green light for at least two cycles.</li><li>When the light is green on A Street and there is no car on B Street, leave the light green on A Street.</li><li>When the is green on B Street (and we’ve finished the two cycles from step 1) and a car is waiting on A Street, give B Street a yellow light for one clock cycle and then give B Street a red light and A Street a green light for at least two cycles.</li><li>When the light is green on B Street and there is no car on A Street, leave the light green on B Street.</li><li>When you press the reset switch, after no more than six cycles, the light should be initially green on A Street and red on B Street and the controller should be ready for operation.</li></ol><h4 id="working">WORKING:</h4><p>We can translate these five rules into the following state diagram. For clarity, we omit the transitions that take all states to state AG2 (A Green 2nd cycle) when reset is true.</p><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="817" src="/images/projects/tlc/state-diagram_hu_df13040bc5fed73c.webp" alt="Original Image 1" onerror="this.onerror='null';this.src='\/images\/projects\/tlc\/state-diagram_hu_76a78ecb3ae166ee.png'"/><p>Each circle in the state diagram represents a state. The name of the state is in the circle and the state of the six output lines (in the order listed above) is shown below that state. The transitions between the states are labeled with the signals that make these transitions occur. Most of the edges have no label which indicates that the transition always occurs (unless reset is asserted).
When our finite-state machine (FSM) is in state AG2, A Street has a green light and B Street has a red light. The transition from AG2 back to itself indicates that as long as there is no car on B Street we keep the A Street light green. The transition to AY (for A Yellow) indicates that if there is a car on B Street, we make the A Street light yellow on the next cycle. AY always transitions to BG1 (for B Green 1st cycle) where the A Street light becomes red and the B Street light becomes green. BG1 always transitions to BG2 where the FSM waits for a car on A Street before sequencing through BY and AG1 back to AG2.</p><p>From this state diagram we can write the following state table:</p><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="563" src="/images/projects/tlc/state-table_hu_c3080a61e7cf5725.webp" alt="Original Image 1" onerror="this.onerror='null';this.src='\/images\/projects\/tlc\/state-table_hu_1d92661e9e32e9d4.png'"/><table><tr><th>State</th><th colspan="3">Inputs</th><th>nextState</th><th colspan="2">Outputs</th></tr><tr><th/><th>reset</th><th>CarA</th><th>CarB</th><th/><th>A lights</th><th>B lights</th></tr><tr><td>*</td><td>1</td><td>*</td><td>*</td><td>AG2</td><td>Green</td><td>Red</td></tr><tr><td>AG2</td><td>0</td><td>0</td><td>*</td><td>AG2</td><td>Green</td><td>Red</td></tr><tr><td>AG2</td><td>0</td><td>*</td><td>1</td><td>AY</td><td>Green</td><td>Red</td></tr><tr><td>AY</td><td>0</td><td>*</td><td>*</td><td>BG1</td><td>Yellow</td><td>Red</td></tr><tr><td>BG1</td><td>0</td><td>*</td><td>*</td><td>BG2</td><td>Red</td><td>Green</td></tr><tr><td>BG2</td><td>0</td><td>*</td><td>1</td><td>BY</td><td>Red</td><td>Green</td></tr><tr><td>BG2</td><td>0</td><td>0</td><td>1</td><td>BY</td><td>Red</td><td>Green</td></tr><tr><td>BY</td><td>0</td><td>*</td><td>*</td><td>AG1</td><td>Red</td><td>Yellow</td></tr><tr><td>AG1</td><td>0</td><td>*</td><td>*</td><td>AG2</td><td>Green</td><td>Red</td></tr></table><h4 id="circuit-diagram-implemented">CIRCUIT DIAGRAM IMPLEMENTED:</h4><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="372" src="/images/projects/tlc/circuit-diagram_hu_9648e124c9d715e5.webp" alt="Original Image 1" onerror="this.onerror='null';this.src='\/images\/projects\/tlc\/circuit-diagram_hu_50f7fea08c48e074.jpg'"/><h5 id="inputs-for-d-flip-flops">INPUTS FOR D FLIP FLOPS:</h5><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-verilog" data-lang="verilog"><span style="display:flex;"><span>D1<span style="color:#f92672">=</span> Q3<span style="color:#f92672">+</span> (<span style="color:#f92672">~</span>CB)<span style="color:#960050;background-color:#1e0010">•</span>(Q1)<span style="color:#f92672">+</span> reset</span></span><span style="display:flex;"><span>D2<span style="color:#f92672">=</span> Q1<span style="color:#960050;background-color:#1e0010">•</span>CB</span></span><span style="display:flex;"><span>D3<span style="color:#f92672">=</span> Q5</span></span><span style="display:flex;"><span>D4<span style="color:#f92672">=</span> (Q6<span style="color:#f92672">+</span> (Q4<span style="color:#960050;background-color:#1e0010">•</span>(<span style="color:#f92672">~</span>CA))<span style="color:#f92672">+</span> reset</span></span><span style="display:flex;"><span>D5<span style="color:#f92672">=</span> Q4<span style="color:#960050;background-color:#1e0010">•</span>CB</span></span><span style="display:flex;"><span>D6<span style="color:#f92672">=</span> Q2</span></span></code></pre></div><h5 id="overall-ouputs">OVERALL OUPUTS:</h5><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-verilog" data-lang="verilog"><span style="display:flex;"><span>RedA<span style="color:#f92672">=</span> Q5<span style="color:#f92672">+</span> reset<span style="color:#f92672">+</span> (<span style="color:#f92672">~</span>reset)<span style="color:#960050;background-color:#1e0010">•</span>(Q4<span style="color:#f92672">+</span> Q6)</span></span><span style="display:flex;"><span>YellowA<span style="color:#f92672">=</span> Q2<span style="color:#960050;background-color:#1e0010">•</span> (<span style="color:#f92672">~</span>reset)</span></span><span style="display:flex;"><span>GreenA<span style="color:#f92672">=</span> (<span style="color:#f92672">~</span>reset)<span style="color:#960050;background-color:#1e0010">•</span>(Q1<span style="color:#f92672">+</span> Q3)</span></span><span style="display:flex;"><span/></span><span style="display:flex;"><span>RedB<span style="color:#f92672">=</span> Q2<span style="color:#f92672">+</span> reset<span style="color:#f92672">+</span> (<span style="color:#f92672">~</span>reset)<span style="color:#960050;background-color:#1e0010">•</span>(Q1<span style="color:#f92672">+</span> Q3)</span></span><span style="display:flex;"><span>YellowB<span style="color:#f92672">=</span> Q5<span style="color:#960050;background-color:#1e0010">•</span>(<span style="color:#f92672">~</span>reset)</span></span><span style="display:flex;"><span>GreenB<span style="color:#f92672">=</span> (<span style="color:#f92672">~</span>reset)<span style="color:#960050;background-color:#1e0010">•</span>(Q5<span style="color:#f92672">+</span> Q6)</span></span></code></pre></div><h4 id="design-code">DESIGN CODE:</h4><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-verilog" data-lang="verilog"><span style="display:flex;"><span><span style="color:#66d9ef">module</span> TLC(clk, reset, carA, carB, lightsA, lightsB) ;</span></span><span style="display:flex;"><span><span style="color:#66d9ef">input</span> clk ;<span style="color:#75715e">// clock</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">input</span> reset ;<span style="color:#75715e">// reset</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">input</span> carA ;<span style="color:#75715e">// a car is waiting on A Street</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">input</span> carB ;<span style="color:#75715e">// a car is waiting on B Street</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">output</span>[<span style="color:#ae81ff">2</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] lightsA ;<span style="color:#75715e">// Red, Yellow, Green lights for A Street</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">output</span>[<span style="color:#ae81ff">2</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] lightsB ;<span style="color:#75715e">// Red, Yellow, Green lights for B Street</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> ag2, ay, ag1, bg2, by, bg1 ;<span style="color:#75715e">// state bits</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">wire</span> nag2, nay, nag1, nbg2, nby, nbg1 ;<span style="color:#75715e">// next state bits</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">wire</span>[<span style="color:#ae81ff">5</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] state ;<span style="color:#75715e">// for observation only</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">assign</span> state<span style="color:#f92672">=</span> {ag2, ay, ag1, bg2, by, bg1} ;</span></span><span style="display:flex;"><span><span style="color:#75715e">// state equations</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">assign</span></span></span><span style="display:flex;"><span>nag2<span style="color:#f92672">=</span> ag1<span style="color:#f92672">|</span>(ag2<span style="color:#f92672">&amp;</span><span style="color:#f92672">~</span>carB)<span style="color:#f92672">|</span>reset ,<span style="color:#75715e">// D1 = Q3 + (~CB)•(Q1) + reset</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/>nay<span style="color:#f92672">=</span> ag2<span style="color:#f92672">&amp;</span> carB ,<span style="color:#75715e">// D2 = Q1•CB</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/>nbg1<span style="color:#f92672">=</span> ay ,<span style="color:#75715e">// D3 = Q5</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/>nbg2<span style="color:#f92672">=</span> (bg1<span style="color:#f92672">|</span>(bg2<span style="color:#f92672">&amp;</span><span style="color:#f92672">~</span>carA))<span style="color:#f92672">&amp;~</span>reset,<span style="color:#75715e">// D4 = (Q6 + (Q4•(~CA)) + reset</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/>nby<span style="color:#f92672">=</span> bg2<span style="color:#f92672">&amp;</span> carA ,<span style="color:#75715e">// D5 = Q4•CA</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/>nag1<span style="color:#f92672">=</span> by ;<span style="color:#75715e">// D6 = Q2</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// flip flops</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">always</span> @(<span style="color:#66d9ef">posedge</span> clk)</span></span><span style="display:flex;"><span>{ag2, ay, ag1, bg2, by, bg1}<span style="color:#f92672">=</span> {nag2, nay, nag1, nbg2, nby, nbg1} ;</span></span><span style="display:flex;"><span><span style="color:#75715e">// output equations</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">assign</span></span></span><span style="display:flex;"><span>lightsA[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">=</span> by<span style="color:#f92672">|</span> lightsB[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">|</span> reset ,<span style="color:#75715e">// red</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/>lightsA[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">=</span> ay<span style="color:#f92672">&amp;</span><span style="color:#f92672">~</span>reset ,<span style="color:#75715e">// yellow</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/>lightsA[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">=</span> (ag1<span style="color:#f92672">|</span> ag2)<span style="color:#f92672">&amp;</span><span style="color:#f92672">~</span>reset,<span style="color:#75715e">// green</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/>lightsB[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">=</span> ay<span style="color:#f92672">|</span> lightsA[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">|</span> reset,<span style="color:#75715e">// red</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/>lightsB[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">=</span> by<span style="color:#f92672">&amp;</span><span style="color:#f92672">~</span>reset,<span style="color:#75715e">// yellow</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/>lightsB[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">=</span> (bg1<span style="color:#f92672">|</span> bg2)<span style="color:#f92672">&amp;</span><span style="color:#f92672">~</span>reset ;<span style="color:#75715e">// green</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">endmodule</span></span></span></code></pre></div><h4 id="testbench-code">TESTBENCH CODE:</h4><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-verilog" data-lang="verilog"><span style="display:flex;"><span><span style="color:#66d9ef">`timescale</span><span style="color:#ae81ff">1</span>ns<span style="color:#f92672">/</span><span style="color:#ae81ff">1</span>ns</span></span><span style="display:flex;"><span><span style="color:#66d9ef">module</span> TLC_tb;</span></span><span style="display:flex;"><span><span style="color:#75715e">// Parameters</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">parameter</span> CLK_PERIOD<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>;<span style="color:#75715e">// Clock period in ns</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// Inputs</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">reg</span> clk;</span></span><span style="display:flex;"><span><span style="color:#66d9ef">reg</span> reset;</span></span><span style="display:flex;"><span><span style="color:#66d9ef">reg</span> carA;</span></span><span style="display:flex;"><span><span style="color:#66d9ef">reg</span> carB;</span></span><span style="display:flex;"><span><span style="color:#75715e">// Outputs</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">wire</span> [<span style="color:#ae81ff">2</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] lightsA;</span></span><span style="display:flex;"><span><span style="color:#66d9ef">wire</span> [<span style="color:#ae81ff">2</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] lightsB;</span></span><span style="display:flex;"><span><span style="color:#66d9ef">wire</span> [<span style="color:#ae81ff">5</span><span style="color:#f92672">:</span><span style="color:#ae81ff">0</span>] state;</span></span><span style="display:flex;"><span><span style="color:#75715e">// Instantiate the TLC module</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/>TLC uut (</span></span><span style="display:flex;"><span> .clk(clk),</span></span><span style="display:flex;"><span> .reset(reset),</span></span><span style="display:flex;"><span> .carA(carA),</span></span><span style="display:flex;"><span> .carB(carB),</span></span><span style="display:flex;"><span> .lightsA(lightsA),</span></span><span style="display:flex;"><span> .lightsB(lightsB)</span></span><span style="display:flex;"><span>);</span></span><span style="display:flex;"><span><span style="color:#75715e">// Connect the state output of TLC to state wire in testbench</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">assign</span> state<span style="color:#f92672">=</span> uut.state;</span></span><span style="display:flex;"><span><span style="color:#75715e">// Clock generation</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">always</span> #((CLK_PERIOD)<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>) clk<span style="color:#f92672">=</span><span style="color:#f92672">~</span>clk;</span></span><span style="display:flex;"><span><span style="color:#75715e">// Initial values</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">initial</span><span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> clk<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;</span></span><span style="display:flex;"><span> reset<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>;</span></span><span style="display:flex;"><span> carA<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;</span></span><span style="display:flex;"><span> carB<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>;</span></span><span style="display:flex;"><span> #<span style="color:#ae81ff">10</span>;<span style="color:#75715e">// Wait for a few cycles</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#75715e">// Test cases</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> reset<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>; carA<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; carB<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; #<span style="color:#ae81ff">10</span>;<span style="color:#75715e">// 1</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> reset<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>; carA<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; carB<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; #<span style="color:#ae81ff">10</span>;<span style="color:#75715e">// 2</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> reset<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>; carA<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; carB<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; #<span style="color:#ae81ff">10</span>;<span style="color:#75715e">// 3</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> reset<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>; carA<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; carB<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; #<span style="color:#ae81ff">10</span>;<span style="color:#75715e">// 4</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> reset<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; carA<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; carB<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; #<span style="color:#ae81ff">10</span>;<span style="color:#75715e">// 5</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> reset<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; carA<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; carB<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>; #<span style="color:#ae81ff">10</span>;<span style="color:#75715e">// 6</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> reset<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; carA<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; carB<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>; #<span style="color:#ae81ff">10</span>;<span style="color:#75715e">// 7</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> reset<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; carA<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; carB<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>; #<span style="color:#ae81ff">10</span>;<span style="color:#75715e">// 8</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> reset<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; carA<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; carB<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>; #<span style="color:#ae81ff">10</span>;<span style="color:#75715e">// 9</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> reset<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; carA<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; carB<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; #<span style="color:#ae81ff">10</span>;<span style="color:#75715e">// 10</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> reset<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; carA<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>; carB<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; #<span style="color:#ae81ff">10</span>;<span style="color:#75715e">// 11</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> reset<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; carA<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>; carB<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; #<span style="color:#ae81ff">10</span>;<span style="color:#75715e">// 12</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/> $finish;<span style="color:#75715e">// End simulation</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// Display outputs</span></span></span><span style="display:flex;"><span><span style="color:#75715e"/><span style="color:#66d9ef">always</span> @(<span style="color:#66d9ef">posedge</span> clk)<span style="color:#66d9ef">begin</span></span></span><span style="display:flex;"><span> $display(<span style="color:#e6db74">"reset = %b carA = %b carB = %b : lightsA = %b lightsB = %b state =%b%b%b%b%b%b"</span>,</span></span><span style="display:flex;"><span> reset, carA, carB, lightsA, lightsB, state[<span style="color:#ae81ff">5</span>], state[<span style="color:#ae81ff">4</span>], state[<span style="color:#ae81ff">3</span>], state[<span style="color:#ae81ff">2</span>], state[<span style="color:#ae81ff">1</span>], state[<span style="color:#ae81ff">0</span>]);</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span></span></span><span style="display:flex;"><span><span style="color:#66d9ef">endmodule</span></span></span></code></pre></div><h5 id="code-explanation">CODE EXPLANATION:</h5><h6 id="design-code-1">DESIGN CODE:</h6><p>Functionality:
The Verilog module TLC defines a traffic light controller with inputs for clock (clk), reset (reset), and car presence on streets A and B (carA, carB). It generates outputs (lightsA, lightsB) to control the traffic lights and uses registers (ag2, ay, ag1, bg2, by, bg1) and wires (nag2, nay, nag1, nbg2, nby, nbg1) to manage internal state transitions and next state calculations (state).</p><p>Operation:
The module updates its internal state (ag2, ay, ag1, bg2, by, bg1) based on clock edges and input conditions, calculates next state bits (nag2, nay, nag1, nbg2, nby, nbg1) using state equations, and determines the output state of traffic lights (lightsA, lightsB) based on the current state and reset conditions using output equations.</p><h6 id="testbench-code-1">TESTBENCH CODE:</h6><p>Purpose:
This Verilog testbench (TLC_tb) is designed to simulate the behavior of the TLC module by providing input stimuli (clk, reset, carA, carB) and observing the corresponding outputs (lightsA, lightsB) and internal state (state). It helps validate the functionality and correctness of the TLC module under various test cases.</p><p>Operation:
The testbench initializes the inputs, generates a clock signal (clk), applies test cases with different input combinations, and displays the resulting outputs and state changes using $display. The testbench also connects the state wire to observe the internal state of the TLC module during simulation.</p><h4 id="ouput">OUPUT:</h4><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-verilog" data-lang="verilog"><span style="display:flex;"><span>reset<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> carA<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> carB<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span><span style="color:#f92672">:</span> lightsA<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span> lightsB<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span> state<span style="color:#f92672">=</span><span style="color:#ae81ff">100000</span></span></span><span style="display:flex;"><span>reset<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> carA<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> carB<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span><span style="color:#f92672">:</span> lightsA<span style="color:#f92672">=</span><span style="color:#ae81ff">001</span> lightsB<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span> state<span style="color:#f92672">=</span><span style="color:#ae81ff">100000</span></span></span><span style="display:flex;"><span>reset<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> carA<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> carB<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span><span style="color:#f92672">:</span> lightsA<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span> lightsB<span style="color:#f92672">=</span><span style="color:#ae81ff">001</span> state<span style="color:#f92672">=</span><span style="color:#ae81ff">000001</span></span></span><span style="display:flex;"><span>reset<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> carA<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> carB<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span><span style="color:#f92672">:</span> lightsA<span style="color:#f92672">=</span><span style="color:#ae81ff">001</span> lightsB<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span> state<span style="color:#f92672">=</span><span style="color:#ae81ff">001000</span></span></span><span style="display:flex;"><span>reset<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> carA<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> carB<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span><span style="color:#f92672">:</span> lightsA<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span> lightsB<span style="color:#f92672">=</span><span style="color:#ae81ff">001</span> state<span style="color:#f92672">=</span><span style="color:#ae81ff">000001</span></span></span><span style="display:flex;"><span>reset<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> carA<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> carB<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span><span style="color:#f92672">:</span> lightsA<span style="color:#f92672">=</span><span style="color:#ae81ff">001</span> lightsB<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span> state<span style="color:#f92672">=</span><span style="color:#ae81ff">001000</span></span></span><span style="display:flex;"><span>reset<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> carA<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> carB<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span><span style="color:#f92672">:</span> lightsA<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span> lightsB<span style="color:#f92672">=</span><span style="color:#ae81ff">001</span> state<span style="color:#f92672">=</span><span style="color:#ae81ff">000001</span></span></span><span style="display:flex;"><span>reset<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> carA<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> carB<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span><span style="color:#f92672">:</span> lightsA<span style="color:#f92672">=</span><span style="color:#ae81ff">001</span> lightsB<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span> state<span style="color:#f92672">=</span><span style="color:#ae81ff">001000</span></span></span><span style="display:flex;"><span>reset<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> carA<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> carB<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span><span style="color:#f92672">:</span> lightsA<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span> lightsB<span style="color:#f92672">=</span><span style="color:#ae81ff">010</span> state<span style="color:#f92672">=</span><span style="color:#ae81ff">000010</span></span></span><span style="display:flex;"><span>reset<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> carA<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> carB<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span><span style="color:#f92672">:</span> lightsA<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span> lightsB<span style="color:#f92672">=</span><span style="color:#ae81ff">001</span> state<span style="color:#f92672">=</span><span style="color:#ae81ff">000001</span></span></span><span style="display:flex;"><span>reset<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> carA<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> carB<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span><span style="color:#f92672">:</span> lightsA<span style="color:#f92672">=</span><span style="color:#ae81ff">001</span> lightsB<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span> state<span style="color:#f92672">=</span><span style="color:#ae81ff">001000</span></span></span><span style="display:flex;"><span>reset<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> carA<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> carB<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span><span style="color:#f92672">:</span> lightsA<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span> lightsB<span style="color:#f92672">=</span><span style="color:#ae81ff">001</span> state<span style="color:#f92672">=</span><span style="color:#ae81ff">000001</span></span></span></code></pre></div><img title="" loading="lazy" decoding="async" class="img  " width="1000" height="112" src="/images/projects/tlc/waveform_hu_92c820405e567607.webp" alt="Original Image 1" onerror="this.onerror='null';this.src='\/images\/projects\/tlc\/waveform_hu_7345d80a131048a5.png'"/><p><strong>Output Columns Explanation</strong>:</p><ol><li><code>reset</code>: Indicates whether the controller is in reset mode (<code>1</code> = reset active,<code>0</code> = normal operation).</li><li><code>carA</code>: Indicates whether there is a car waiting on Street A (<code>1</code> = car present,<code>0</code> = no car).</li><li><code>carB</code>: Indicates whether there is a car waiting on Street B (<code>1</code> = car present,<code>0</code> = no car).</li><li><code>lightsA</code> and<code>lightsB</code>: 3-bit outputs indicating the light status for streets A and B, respectively:<ul><li><code>100</code>: Red</li><li><code>010</code>: Yellow</li><li><code>001</code>: Green</li></ul></li><li><code>state</code>: 6-bit internal state of the controller, where each bit corresponds to one of the states (<code>ag2</code>,<code>ay</code>,<code>ag1</code>,<code>bg2</code>,<code>by</code>,<code>bg1</code>).</li></ol><p><strong>Key Observations from the Output</strong>:</p><ol><li><p><strong>Reset Behavior</strong>:</p><ul><li>When<code>reset = 1</code>, the controller initializes to the default state (<code>AG2</code>), where Street A has a green light (<code>lightsA = 100</code>) and Street B has a red light (<code>lightsB = 100</code>).</li></ul></li><li><p><strong>State Transitions</strong>:</p><ul><li><strong>Initial State</strong> (<code>AG2</code>): Street A has a green light, and Street B has a red light. The controller remains in this state as long as<code>carB = 0</code>.</li><li><strong>Transition to AY</strong>: When<code>carB = 1</code>, the controller transitions to<code>AY</code> (A Yellow), giving Street A a yellow light while preparing to switch to B Street.</li><li><strong>Transition to BG1/BG2</strong>: After<code>AY</code>, the controller transitions to<code>BG1</code> and<code>BG2</code>, giving Street B the green light and Street A the red light. The controller waits for at least two cycles on Street B (<code>BG1</code> and<code>BG2</code>) before checking for<code>carA</code>.</li><li><strong>Transition to BY</strong>: When<code>carA = 1</code>, the controller transitions to<code>BY</code> (B Yellow), switching from Street B back to Street A.</li></ul></li><li><p><strong>Input-Driven State Changes</strong>:</p><ul><li>The controller responds dynamically to<code>carA</code> and<code>carB</code>, switching states and light colors based on the presence of cars. For example:<ul><li><code>carA = 0, carB = 1</code>: Street B gets priority (transition from<code>AG2</code> →<code>AY</code> →<code>BG1</code>).</li><li><code>carA = 1, carB = 0</code>: Street A gets priority (transition from<code>BG2</code> →<code>BY</code> →<code>AG1</code>).</li></ul></li></ul></li><li><p><strong>Output Consistency</strong>:</p><ul><li>The outputs (<code>lightsA</code> and<code>lightsB</code>) match the expected light status for each state in the state table:<ul><li><code>lightsA = 100</code> (Red for A) corresponds to<code>BG1</code>/<code>BG2</code> states.</li><li><code>lightsB = 001</code> (Green for B) corresponds to<code>BG1</code>/<code>BG2</code> states.</li><li><code>lightsA = 001</code> (Green for A) corresponds to<code>AG1</code>/<code>AG2</code> states.</li></ul></li></ul></li></ol><p><strong>Waveform Analysis</strong>:
The waveform visualizes the same transitions observed in the output table. Key points:</p><ul><li>The state bits change in accordance with the state transitions defined by the FSM.</li><li>The<code>lightsA</code> and<code>lightsB</code> signals follow the expected traffic light rules.</li><li><code>Reset</code> enforces the initial state (<code>AG2</code>), confirming the correctness of initialization.</li></ul>
]]></content:encoded></item><item><title>:~$ whoami</title><link>https://mummanajagadeesh.github.io/about_1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/about_1/</guid><description>&lt;![CDATA[<p>I’m from Visakhapatnam, Andhra Pradesh, and currently a<strong>sophomore at NIT Calicut</strong>, studying<strong>Electronics and Communication Engineering</strong>.</p>]]></description><content:encoded>&lt;![CDATA[<p>I’m from Visakhapatnam, Andhra Pradesh, and currently a<strong>sophomore at NIT Calicut</strong>, studying<strong>Electronics and Communication Engineering</strong>.</p><p>As an electronics student, I have a strong interest in digital and mixed-signal VLSI design, as well as hardware optimization. I also enjoy working with microcontrollers and coding for various electronics projects in my personal time. My broader interests include robotics, AI, and the hardware that powers these systems.</p><p>I am actively seeking internship opportunities in electronics design roles. To enhance my skills, I am continuously revising course material, gaining hands-on experience with design tools and technologies, and familiarizing myself with full design flows and simulation processes.</p><p>Currently, I am focused on electronics design and systems development for real-world applications at my college&rsquo;s Mechatronics Lab.</p><h5 id="my-blog">My Blog</h5><p><a href="https://mummanajagadeesh.github.io/blog/hello-world/" target="_blank">Do checkout the introduction post :)</a></p><p>This blog was initiated as a platform to systematically document my technical projects. It functions both as a personal archive and as a means to share insights with others who may be exploring related areas.</p><p>Within this space, I chronicle my progress, challenges encountered, and the underlying technical aspects of my work. Whether it involves deconstructing complex problems, presenting completed projects, or reflecting on the development process, the blog serves as a tool to organize my thoughts and contribute meaningfully to collective learning.</p><p>Through careful documentation, I aim to reinforce my own understanding while offering resources that may prove helpful to others pursuing similar interests.</p><p>// The photo is to be updated on the left-hand side, which is currently empty //</p>
]]></content:encoded></item><item><title>About ME</title><link>https://mummanajagadeesh.github.io/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/about/</guid><description>&lt;![CDATA[<p>I’m from Visakhapatnam, Andhra Pradesh, and currently a<strong>sophomore at NIT Calicut</strong>, studying<strong>Electronics and Communication Engineering</strong>.</p>]]></description><content:encoded>&lt;![CDATA[<p>I’m from Visakhapatnam, Andhra Pradesh, and currently a<strong>sophomore at NIT Calicut</strong>, studying<strong>Electronics and Communication Engineering</strong>.</p><p>As an electronics student, I have a strong interest in digital and mixed-signal VLSI design, as well as hardware architecture and optimization. I also enjoy working with microcontrollers and coding for various electronics projects in my personal time. My broader interests include robotics, AI, and the hardware that powers these systems.</p><p>I am actively seeking internship opportunities in electronics design roles. To enhance my skills, I am continuously revising course material, gaining hands-on experience with design tools and technologies, and familiarizing myself with full design flows and simulation processes.</p><p>Currently, I am focused on electronics design and systems development for real-world applications at my college&rsquo;s Mechatronics Lab.</p><h5 id="my-blog">My Blog</h5><p><a href="https://mummanajagadeesh.github.io/blog/hello-world/" target="_blank">Do checkout the introduction post :)</a></p><p>This blog was initiated as a platform to systematically document my technical projects. It functions both as a personal archive and as a means to share insights with others who may be exploring related areas.</p><p>Within this space, I chronicle my progress, challenges encountered, and the underlying technical aspects of my work. Whether it involves deconstructing complex problems, presenting completed projects, or reflecting on the development process, the blog serves as a tool to organize my thoughts and contribute meaningfully to collective learning.</p><p>Through careful documentation, I aim to reinforce my own understanding while offering resources that may prove helpful to others pursuing similar interests.</p><p>// The photo is to be updated on the left-hand side, which is currently empty //</p>
]]></content:encoded></item><item><title>Contact</title><link>https://mummanajagadeesh.github.io/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/contact/</guid><description>&lt;![CDATA[]]></description><content:encoded>&lt;![CDATA[]]></content:encoded></item><item><title>Courses</title><link>https://mummanajagadeesh.github.io/about/courses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/about/courses/</guid><description>&lt;![CDATA[<h6 id="edx"><strong>EDX</strong></h6><ul><li><strong>CS50P 2022</strong> Harvard | Jan'24</li><li><strong>CS50X 2024</strong> Harvard | Sep'24</li></ul><h6 id="coursera"><strong>Coursera</strong></h6><ul><li><strong>Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)</strong> Hebrew University</li><li><strong>Robotics Specialisation</strong> University of Pennsylvania</li></ul>]]></description><content:encoded>&lt;![CDATA[<h6 id="edx"><strong>EDX</strong></h6><ul><li><strong>CS50P 2022</strong> Harvard | Jan'24</li><li><strong>CS50X 2024</strong> Harvard | Sep'24</li></ul><h6 id="coursera"><strong>Coursera</strong></h6><ul><li><strong>Build a Modern Computer from First Principles: From Nand to Tetris (Project-Centered Course)</strong> Hebrew University</li><li><strong>Robotics Specialisation</strong> University of Pennsylvania</li></ul>
]]></content:encoded></item><item><title>GPBOT Sub Projects</title><link>https://mummanajagadeesh.github.io/projects/gpbot/subprojects/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/projects/gpbot/subprojects/</guid><description>&lt;![CDATA[]]></description><content:encoded>&lt;![CDATA[]]></content:encoded></item><item><title>ImProVe Sub Projects</title><link>https://mummanajagadeesh.github.io/projects/improve/subprojects/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/projects/improve/subprojects/</guid><description>&lt;![CDATA[<h5 id="verilog-hdl-toolkit-for-image-processing-and-pattern-recognition">Verilog HDL Toolkit for Image Processing and Pattern Recognition</h5><p><strong>Duration:</strong> Individual, Ongoing<br><strong>Tools:</strong> Verilog (Icarus Verilog, Xilinx Vivado) | Python (OpenCV, NumPy, Tkinter) | Scripting (TCL. Perl)</p>]]></description><content:encoded>&lt;![CDATA[<h5 id="verilog-hdl-toolkit-for-image-processing-and-pattern-recognition">Verilog HDL Toolkit for Image Processing and Pattern Recognition</h5><p><strong>Duration:</strong> Individual, Ongoing<br><strong>Tools:</strong> Verilog (Icarus Verilog, Xilinx Vivado) | Python (OpenCV, NumPy, Tkinter) | Scripting (TCL. Perl)</p><ul><li><strong>Designed image processing algorithms (e.g., edge detection, geometric &amp; color transforms, noise reduction) in Verilog, utilizing hardware optimized math techniques to maximize computational efficiency; fine-tuned for low-latency preprocessing in embedded vision SoCs</strong></li><li><strong>Implemented 3-layer MLP (784-256-128-62) for Extended-MNIST Character Recognition (62 classes, ∼124k samples) using FSM-controlled neural network in Verilog; achieved >90% training accuracy with &lt;5s inference latency (in simulation); developed full end-to-end preprocessing and inference workflow</strong></li><li><strong>Automated model inference and performance metric evaluation via Tcl/Perl scripts (py &amp; iverilog commands execution); real-time Tkinter GUI for test user input</strong></li><li><strong>Working on real life applications including label detection, document scanning, stereo depth map generation, and neural network inference for MNIST/EMNIST datasets</strong></li></ul><p><br><br><br/><h5 id="tabular-summary-of-all-technical-details--nn-inference-workflow-">Tabular Summary of all technical details ( NN Inference Workflow )</h5><table><thead><tr><th><strong>Parameter</strong></th><th><strong>Value / Detail</strong></th><th><strong>Technical Highlights</strong></th></tr></thead><tbody><tr><td><strong>Accuracy</strong></td><td>>90%</td><td>High classification precision on both MNIST (10 classes) and EMNIST (62 classes).</td></tr><tr><td><strong>Inference Latency</strong></td><td>&lt;5 sec per prediction</td><td>Achieved via a tightly controlled FSM-driven sequential evaluation in simulation.</td></tr><tr><td><strong>Pipelining</strong></td><td>Coarse-grained</td><td>Finite State Machine (FSM) orchestrates layer-by-layer processing without fine-grained, parallel overlap.</td></tr><tr><td><strong>Data Type</strong></td><td>IEEE 754<code>real</code></td><td>Simulation relies on IEEE 754 floating-point (non-synthesizable) for high-precision computation during development.</td></tr><tr><td><strong>Python Libraries</strong></td><td>NumPy exclusively</td><td>Utilizes NumPy for all training routines and parameter extraction, without relying on higher-level ML frameworks.</td></tr><tr><td><strong>Dataset</strong></td><td>EMNIST ByClass ≈124k (≈2K per class, 62 classes); MNIST – TSD (Google Colab)</td><td>Separate datasets: EMNIST ByClass, with 62 classes (digits and both uppercase and lowercase letters), and MNIST (handwritten digits 0-9), trained independently for alphanumeric character recognition tasks.</td></tr><tr><td><strong>Training Optimizers</strong></td><td>Adam (1500 iterations) + SGD with Momentum (500 iterations)</td><td>Hybrid optimization: Adam for rapid convergence initially, then SGD with Momentum for fine-tuning and refinement.</td></tr><tr><td><strong>Neural Architecture (MNIST)</strong></td><td>784 (input) – 128 (hidden) – 10 (output)</td><td>A compact feed-forward architecture optimized for digit recognition.</td></tr><tr><td><strong>Neural Architecture (EMNIST)</strong></td><td>784 (input) – 256 (hidden) – 128 (hidden) – 62 (output)</td><td>Expanded design to support 62 classes (digits, uppercase, lowercase) within the same inference infrastructure.</td></tr><tr><td><strong>Weight &amp; Bias Scaling</strong></td><td>×10,000</td><td>Parameters are scaled to simulate fixed-point arithmetic, ensuring compatibility during inference in Verilog.</td></tr><tr><td><strong>Input Image Format</strong></td><td>28×28 grayscale</td><td>Images are captured via a Tkinter drawing interface, then processed to 28×28 grayscale before vectorization.</td></tr><tr><td><strong>Preprocessing Pipeline</strong></td><td>Custom Python scripts for initial image conversion + Verilog modules</td><td>Workflow includes image conversion (<code>img2bin.py</code>), image processing (resize, contrast, gray, roi, padding, invert, rotate, flip, flatten), and file preparation for memory module generation.</td></tr><tr><td><strong>Memory Module Generation</strong></td><td>Python scripts (wtbs_loader.py, memloader_from_inp_vec.py)</td><td>Automated conversion of weight, bias, and image vector files into synthesizable Verilog memory modules.</td></tr><tr><td><strong>Simulation Environment</strong></td><td>Icarus Verilog</td><td>Compilation and simulation are conducted using Icarus Verilog with dedicated testbenches for module validation.</td></tr><tr><td><strong>Automation &amp; Workflow</strong></td><td>Fully automated, cross-platform</td><td>Integration via Makefiles (Linux/macOS), batch scripts (Windows), plus Perl and TCL scripts to streamline build, test, and simulation phases.</td></tr><tr><td><strong>Verification &amp; Testbenches</strong></td><td>Implemented</td><td>Robust testbenches facilitate both unit-level and end-to-end verification of the Verilog inference modules.</td></tr><tr><td><strong>System Integration</strong></td><td>End-to-End pipeline</td><td>Seamless integration connects Python for image input/output with Verilog for core logic simulation, creating a unified hardware prototyping workflow</td></tr><tr><td><strong>Test User Interaction</strong></td><td>Tkinter-based interface</td><td>Direct interactive drawing capability allows real-time testing of handwritten inputs through the complete conversion and inference flow.</td></tr></tbody></table>
]]></content:encoded></item><item><title>Other Achievements [Secondary School]</title><link>https://mummanajagadeesh.github.io/about/achievements/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/about/achievements/</guid><description>&lt;![CDATA[<h6 id="karate-black-belt-"><strong>Karate Black Belt [DAN Ⅰ] Holder</strong></h6><ul><li>Achieved a<strong>Black Belt (DAN Ⅰ)</strong> in Karate</li></ul><h6 id="gold-medalist--district-level-karate-tournament"><strong>Gold Medalist – District Level Karate Tournament</strong></h6><ul><li>Secured<strong>1st place</strong> in a district-level Karate tournament</li></ul><h6 id="sports-kid-of-the-year-"><strong>Sports Kid of the Year [2018-19]</strong></h6><ul><li>Honored as the<strong>Sports Kid of the Year 2018</strong></li></ul><h6 id="gold-medalist--essay-writing"><strong>Gold Medalist – Essay Writing</strong></h6><ul><li>Won<strong>1st place</strong> in an<strong>essay writing competition</strong></li></ul><h6 id="silver-medalist--vedic-math"><strong>Silver Medalist – Vedic Math</strong></h6><ul><li>Achieved<strong>2nd place</strong> in a<strong>Vedic Mathematics competition</strong></li></ul>
]]></description><content:encoded>&lt;![CDATA[<h6 id="karate-black-belt-"><strong>Karate Black Belt [DAN Ⅰ] Holder</strong></h6><ul><li>Achieved a<strong>Black Belt (DAN Ⅰ)</strong> in Karate</li></ul><h6 id="gold-medalist--district-level-karate-tournament"><strong>Gold Medalist – District Level Karate Tournament</strong></h6><ul><li>Secured<strong>1st place</strong> in a district-level Karate tournament</li></ul><h6 id="sports-kid-of-the-year-"><strong>Sports Kid of the Year [2018-19]</strong></h6><ul><li>Honored as the<strong>Sports Kid of the Year 2018</strong></li></ul><h6 id="gold-medalist--essay-writing"><strong>Gold Medalist – Essay Writing</strong></h6><ul><li>Won<strong>1st place</strong> in an<strong>essay writing competition</strong></li></ul><h6 id="silver-medalist--vedic-math"><strong>Silver Medalist – Vedic Math</strong></h6><ul><li>Achieved<strong>2nd place</strong> in a<strong>Vedic Mathematics competition</strong></li></ul>
]]></content:encoded></item><item><title>Personal Accomplishments and Competitions</title><link>https://mummanajagadeesh.github.io/about/accomplishments/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/about/accomplishments/</guid><description>&lt;![CDATA[<h6 id="isro-iroc-u-robotics-challenge--round-1-qualified-team-dec"><strong>ISRO IROC-U Robotics Challenge – Round 1 Qualified</strong><em>(Team, Dec &lsquo;24)</em></h6><ul><li>Designing an<strong>autonomous drone</strong> for operations on the<strong>Martian surface</strong> as part of the<strong>ISRO IROC-U Robotics Challenge</strong></li><li>Successfully<strong>qualified for Round 1</strong>;<strong>Round 2 is ongoing</strong></li></ul><h6 id="circuit-conclave--nitc-tathva--1st-place-team-oct"><strong>Circuit Conclave | NITC Tathva – 1st Place</strong><em>(Team, Oct &lsquo;24)</em></h6><ul><li>Secured<strong>1st place</strong> in<strong>Circuit Conclave</strong>, an electronics design competition focused on innovative circuit solutions</li></ul><h6 id="disarmamine--nitc-tathva--3rd-place-team-oct"><strong>Disarmamine | NITC Tathva – 3rd Place</strong><em>(Team, Oct &lsquo;24)</em></h6><ul><li>Achieved<strong>3rd place</strong> in<strong>Disarmamine</strong>, showcasing expertise in electronics and problem-solving</li></ul><h6 id="robotrix24--nit-surathkal--1st-round-qualified-individual-dec"><strong>Robotrix'24 | NIT Surathkal – 1st Round Qualified</strong><em>(Individual, Dec &lsquo;24)</em></h6><ul><li>Successfully cleared<strong>Round 1</strong> of<strong>Robotrix'24</strong> and participated in the<strong>24-hour robotics simulation hackathon (Round 2)</strong> using<strong>CoppeliaSim</strong></li></ul><h6 id="digital-circuit-design-challenge--nit-trichy--1st-round-qualified-team-feb"><strong>Digital Circuit Design Challenge | NIT Trichy – 1st Round Qualified</strong><em>(Team, Feb &lsquo;25)</em></h6><ul><li>Cleared<strong>Round 1</strong> in<strong>Digital Circuit Design Challenge</strong>, a challenge focusing on digital electronics and logic design</li></ul><h6 id="flipkart-grid-robotics-challenge--1st-round-qualified-individual-aug"><strong>Flipkart GRID Robotics Challenge – 1st Round Qualified</strong><em>(Individual, Aug &lsquo;24)</em></h6><ul><li>Advanced past<strong>Round 1</strong> in the<strong>Flipkart GRID Robotics Challenge</strong>, an AI-driven robotics competition</li></ul><hr><hr><h6 id="aws-aiml-nano-degree-scholarship--udacity--amazon-mar"><strong>AWS AI/ML Nano Degree Scholarship | Udacity &amp; Amazon</strong><em>(Mar &lsquo;24)</em></h6><ul><li>Completed<strong>two AI/ML courses</strong> and cleared<strong>evaluation test</strong> as part of the<strong>AWS AI/ML Scholarship Program</strong></li><li>Trained an<strong>autonomous racing car</strong> using<strong>reinforcement learning (RL)</strong> to compete against a lap, making it to the<strong>leaderboards</strong></li><li>Became<strong>eligible to apply</strong> for the<strong>AWS AI/ML Nano Degree Scholarship</strong></li></ul><hr><hr><h6 id="c2s-chip-to-startup--team-feb"><strong>C2S Chip to Startup &ldquo;Digital Hackathon&rdquo; – Participated</strong><em>(Team, Feb &lsquo;25)</em></h6><ul><li>Competed in<strong>Digital Hackathon</strong> with a focus on<strong>digital electronics</strong>, organized under<strong>C2S Chip to Startup</strong> by<strong>Ministry of Electronics &amp; Information Technology (MeitY)</strong></li></ul><h6 id="nokia-fpga-hackathon-team-mar"><strong>Nokia FPGA Hackathon</strong><em>(Team, Mar &lsquo;25)</em></h6><h6 id="bajaj-auto-ohm-challenge--participated-team-feb"><strong>Bajaj Auto Ohm Challenge – Participated</strong><em>(Team, Feb &lsquo;25)</em></h6><ul><li>Took part in the<strong>Bajaj Auto Ohm Challenge</strong>, an engineering competition emphasizing<strong>electrical and electronic innovations</strong></li></ul><h6 id="bharatiya-antariksh-hackathon-bah--team-july"><strong>Bharatiya Antariksh Hackathon (BAH &lsquo;24) – Participated</strong><em>(Team, July &lsquo;24)</em></h6><ul><li>Developed an<strong>AI/ML-based system</strong> for<strong>automatic detection of craters and boulders</strong> from<strong>Orbiter High-Resolution Camera (OHRC) images</strong></li><li>Focused on<strong>enhancing planetary exploration</strong> through automation and computer vision</li></ul>]]></description><content:encoded>&lt;![CDATA[<h6 id="isro-iroc-u-robotics-challenge--round-1-qualified-team-dec"><strong>ISRO IROC-U Robotics Challenge – Round 1 Qualified</strong><em>(Team, Dec &lsquo;24)</em></h6><ul><li>Designing an<strong>autonomous drone</strong> for operations on the<strong>Martian surface</strong> as part of the<strong>ISRO IROC-U Robotics Challenge</strong></li><li>Successfully<strong>qualified for Round 1</strong>;<strong>Round 2 is ongoing</strong></li></ul><h6 id="circuit-conclave--nitc-tathva--1st-place-team-oct"><strong>Circuit Conclave | NITC Tathva – 1st Place</strong><em>(Team, Oct &lsquo;24)</em></h6><ul><li>Secured<strong>1st place</strong> in<strong>Circuit Conclave</strong>, an electronics design competition focused on innovative circuit solutions</li></ul><h6 id="disarmamine--nitc-tathva--3rd-place-team-oct"><strong>Disarmamine | NITC Tathva – 3rd Place</strong><em>(Team, Oct &lsquo;24)</em></h6><ul><li>Achieved<strong>3rd place</strong> in<strong>Disarmamine</strong>, showcasing expertise in electronics and problem-solving</li></ul><h6 id="robotrix24--nit-surathkal--1st-round-qualified-individual-dec"><strong>Robotrix'24 | NIT Surathkal – 1st Round Qualified</strong><em>(Individual, Dec &lsquo;24)</em></h6><ul><li>Successfully cleared<strong>Round 1</strong> of<strong>Robotrix'24</strong> and participated in the<strong>24-hour robotics simulation hackathon (Round 2)</strong> using<strong>CoppeliaSim</strong></li></ul><h6 id="digital-circuit-design-challenge--nit-trichy--1st-round-qualified-team-feb"><strong>Digital Circuit Design Challenge | NIT Trichy – 1st Round Qualified</strong><em>(Team, Feb &lsquo;25)</em></h6><ul><li>Cleared<strong>Round 1</strong> in<strong>Digital Circuit Design Challenge</strong>, a challenge focusing on digital electronics and logic design</li></ul><h6 id="flipkart-grid-robotics-challenge--1st-round-qualified-individual-aug"><strong>Flipkart GRID Robotics Challenge – 1st Round Qualified</strong><em>(Individual, Aug &lsquo;24)</em></h6><ul><li>Advanced past<strong>Round 1</strong> in the<strong>Flipkart GRID Robotics Challenge</strong>, an AI-driven robotics competition</li></ul><hr><hr><h6 id="aws-aiml-nano-degree-scholarship--udacity--amazon-mar"><strong>AWS AI/ML Nano Degree Scholarship | Udacity &amp; Amazon</strong><em>(Mar &lsquo;24)</em></h6><ul><li>Completed<strong>two AI/ML courses</strong> and cleared<strong>evaluation test</strong> as part of the<strong>AWS AI/ML Scholarship Program</strong></li><li>Trained an<strong>autonomous racing car</strong> using<strong>reinforcement learning (RL)</strong> to compete against a lap, making it to the<strong>leaderboards</strong></li><li>Became<strong>eligible to apply</strong> for the<strong>AWS AI/ML Nano Degree Scholarship</strong></li></ul><hr><hr><h6 id="c2s-chip-to-startup--team-feb"><strong>C2S Chip to Startup &ldquo;Digital Hackathon&rdquo; – Participated</strong><em>(Team, Feb &lsquo;25)</em></h6><ul><li>Competed in<strong>Digital Hackathon</strong> with a focus on<strong>digital electronics</strong>, organized under<strong>C2S Chip to Startup</strong> by<strong>Ministry of Electronics &amp; Information Technology (MeitY)</strong></li></ul><h6 id="nokia-fpga-hackathon-team-mar"><strong>Nokia FPGA Hackathon</strong><em>(Team, Mar &lsquo;25)</em></h6><h6 id="bajaj-auto-ohm-challenge--participated-team-feb"><strong>Bajaj Auto Ohm Challenge – Participated</strong><em>(Team, Feb &lsquo;25)</em></h6><ul><li>Took part in the<strong>Bajaj Auto Ohm Challenge</strong>, an engineering competition emphasizing<strong>electrical and electronic innovations</strong></li></ul><h6 id="bharatiya-antariksh-hackathon-bah--team-july"><strong>Bharatiya Antariksh Hackathon (BAH &lsquo;24) – Participated</strong><em>(Team, July &lsquo;24)</em></h6><ul><li>Developed an<strong>AI/ML-based system</strong> for<strong>automatic detection of craters and boulders</strong> from<strong>Orbiter High-Resolution Camera (OHRC) images</strong></li><li>Focused on<strong>enhancing planetary exploration</strong> through automation and computer vision</li></ul>
]]></content:encoded></item><item><title>Positions Held</title><link>https://mummanajagadeesh.github.io/about/positions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/about/positions/</guid><description>&lt;![CDATA[<h6 id="ecea-executive-assistant--event-management--techincal-team--oct24---present"><strong>ECEA Executive Assistant | Event Management | Techincal Team , Oct'24 - Present</strong></h6><ul><li><strong>ECEA - Electronics And Communication Engineering Association @ NITC</strong></li><li>Arranged workshops and seminars in ECE department blocks</li><li>Helped freshers connect with faculty</li><li>Built relations with alumni and learned a lot during the process</li></ul><h6 id="tathva24-junior-executive--tech-conclave-committee-oct24"><strong>Tathva'24 Junior Executive | Tech Conclave Committee, Oct'24</strong></h6><ul><li><strong>Tathva - South India&rsquo;s Largest Techno-management Fest @ NITC</strong></li><li>Reaching out to YouTubers and influencers via cold emails to invite them to the event; successfully secured one guest attendee</li><li>Contributed to content writing and poster ideation for event promotion</li><li>Managed crowd coordination and logistics on the event day</li></ul><h6 id="rignitc-tech-member--nov24---forever"><strong>RIGNITC Tech Member | Nov'24 - Forever</strong></h6><ul><li><strong>RIGNITC - Robotics Interest Group @ NITC</strong></li><li>Worked on several real-world interdisciplinary projects as part of robotics enthusiast teams</li><li>Assisted in explaining lab visits to students from various schools</li><li>Helped in organizing Origo'25, the annual tech workshop by team RIG</li></ul><hr><hr><h6 id="scientific-volunteer--sep23"><strong>Scientific Volunteer | Sep'23</strong></h6><ul><li><strong>Institute for Plasma Research (IPR × NITC) | Plasma Exhibition</strong></li><li>Trained to explain plasma science, its applications, and nuclear fusion to exhibition visitors</li><li>Demonstrated and provided in-depth explanations of<strong>5+ plasma exhibits</strong> to over<strong>100 students</strong>, detailing their functions and construction while addressing queries</li></ul><h6 id="asteroid-hunter--may"><strong>Asteroid Hunter | May &lsquo;24 - Aug &lsquo;24</strong></h6><ul><li><strong>International Astronomical Search Collaboration (IASC × NASA × Saptarshi India | STAC)</strong></li><li>Trained in using<strong>Astrometrica</strong> software to analyze astronomical data for asteroid detection</li><li>Identified<strong>10+ potential asteroid</strong> signatures as part of<strong>NASA’s Citizen Science</strong> initiative</li></ul>]]></description><content:encoded>&lt;![CDATA[<h6 id="ecea-executive-assistant--event-management--techincal-team--oct24---present"><strong>ECEA Executive Assistant | Event Management | Techincal Team , Oct'24 - Present</strong></h6><ul><li><strong>ECEA - Electronics And Communication Engineering Association @ NITC</strong></li><li>Arranged workshops and seminars in ECE department blocks</li><li>Helped freshers connect with faculty</li><li>Built relations with alumni and learned a lot during the process</li></ul><h6 id="tathva24-junior-executive--tech-conclave-committee-oct24"><strong>Tathva'24 Junior Executive | Tech Conclave Committee, Oct'24</strong></h6><ul><li><strong>Tathva - South India&rsquo;s Largest Techno-management Fest @ NITC</strong></li><li>Reaching out to YouTubers and influencers via cold emails to invite them to the event; successfully secured one guest attendee</li><li>Contributed to content writing and poster ideation for event promotion</li><li>Managed crowd coordination and logistics on the event day</li></ul><h6 id="rignitc-tech-member--nov24---forever"><strong>RIGNITC Tech Member | Nov'24 - Forever</strong></h6><ul><li><strong>RIGNITC - Robotics Interest Group @ NITC</strong></li><li>Worked on several real-world interdisciplinary projects as part of robotics enthusiast teams</li><li>Assisted in explaining lab visits to students from various schools</li><li>Helped in organizing Origo'25, the annual tech workshop by team RIG</li></ul><hr><hr><h6 id="scientific-volunteer--sep23"><strong>Scientific Volunteer | Sep'23</strong></h6><ul><li><strong>Institute for Plasma Research (IPR × NITC) | Plasma Exhibition</strong></li><li>Trained to explain plasma science, its applications, and nuclear fusion to exhibition visitors</li><li>Demonstrated and provided in-depth explanations of<strong>5+ plasma exhibits</strong> to over<strong>100 students</strong>, detailing their functions and construction while addressing queries</li></ul><h6 id="asteroid-hunter--may"><strong>Asteroid Hunter | May &lsquo;24 - Aug &lsquo;24</strong></h6><ul><li><strong>International Astronomical Search Collaboration (IASC × NASA × Saptarshi India | STAC)</strong></li><li>Trained in using<strong>Astrometrica</strong> software to analyze astronomical data for asteroid detection</li><li>Identified<strong>10+ potential asteroid</strong> signatures as part of<strong>NASA’s Citizen Science</strong> initiative</li></ul>
]]></content:encoded></item><item><title>Privacy Policy</title><link>https://mummanajagadeesh.github.io/privacy-policy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/privacy-policy/</guid><description>&lt;![CDATA[<h3 id="wait-why-is-this-even-here">Wait… Why Is This Even Here?</h3><p>No one will ever notice this privacy policy. I don’t even know why it exists. Except for someone like me. And now you. If you’re reading this, you found it. Regret it now.</p>]]></description><content:encoded>&lt;![CDATA[<h3 id="wait-why-is-this-even-here">Wait… Why Is This Even Here?</h3><p>No one will ever notice this privacy policy. I don’t even know why it exists. Except for someone like me. And now you. If you’re reading this, you found it. Regret it now.</p><h4 id="we-need-your-email-to-send-you-stuff">We Need Your Email (To Send You Stuff)</h4><p>By signing up, you give me your email. But don’t forget—once I have it, it could be used in ways you might not expect. Stay subscribed if you want to stay in the loop.</p><h4 id="your-info-is-safe-but-not-forever">Your Info Is Safe (But Not Forever)</h4><p>I keep your email secure, but once it’s in my system, it might end up in unexpected places. I won’t share it with random strangers, but I’ll use it to its full potential.</p><h4 id="cookies-are-just-for-the-website-and-we-track-you">Cookies Are Just for the Website (And We Track You)</h4><p>I use cookies to improve your experience on my site, and yes, I track you. It’s not personal—it’s business. You’re here, and I’m going to make sure you keep coming back.</p><h4 id="unsubscribe-yeah-you-can-but-seriously">Unsubscribe? Yeah, You Can. But Seriously…</h4><p>You can unsubscribe at any time, but you’ll miss out on all the cool stuff. Think hard before you hit that button. Once you’re gone, you might not come back.</p><h4 id="accidentally-ended-up-here">Accidentally Ended Up Here?</h4><p>If you clicked on this by mistake, too bad. You’re in it now. Might as well stick around and see what happens.</p><h4 id="questions-better-ask-quickly">Questions? Better Ask Quickly.</h4><p>Got questions? Reach out, but remember—I don’t do apologies. I do business.</p><hr><h3 id="tldr-yes-i-sell-your-email-ids-deal-with-it">TL;DR: Yes, I Sell Your Email IDs. Deal With It.</h3><p>Here’s the hard truth about your email. When you sign up, I collect your email address to send you updates. And guess what? I might share it with others. So if you want to keep getting my emails, you better stick around.</p>
]]></content:encoded></item><item><title>Projects</title><link>https://mummanajagadeesh.github.io/projects/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/projects/</guid><description>&lt;![CDATA[]]></description><content:encoded>&lt;![CDATA[]]></content:encoded></item><item><title>Scholarships Received</title><link>https://mummanajagadeesh.github.io/about/scholarships/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/about/scholarships/</guid><description>&lt;![CDATA[<h6 id="crec-sjet-scholarship"><strong>CREC-SJET Scholarship</strong></h6><ul><li><strong>Issued by:</strong> CREC Silver Jubilee Endowment Trust ·<em>Apr 2024</em></li><li><strong>Associated with:</strong> National Institute of Technology Calicut</li><li>Awarded in recognition of academic merit and financial need, enabling me to pursue my education and overcome financial barriers.</li></ul><h6 id="reliance-foundation-scholarship"><strong>Reliance Foundation Scholarship</strong></h6><ul><li><strong>Issued by:</strong> Reliance Foundation ·<em>Feb 2024</em></li><li><strong>Description:</strong> Recipient of a prestigious scholarship supporting meritorious first-year undergraduate students nationwide.</li><li><strong>Selection:</strong> Awarded on a merit-cum-means basis, with up to 5,000 scholars selected.</li><li><strong>Benefits:</strong> Besides financial aid, it also connects students to a strong alumni network.</li></ul><h6 id="fiitjee-tuition-fee-waiver"><strong>FIITJEE Tuition Fee Waiver</strong></h6><ul><li><strong>Issued by:</strong> FIITJEE ·<em>Mar 2021</em></li><li><strong>Associated with:</strong> FIITJEE</li><li><strong>Award:</strong> Received 100% tuition fee waiver for excelling in the admission test</li></ul>]]></description><content:encoded>&lt;![CDATA[<h6 id="crec-sjet-scholarship"><strong>CREC-SJET Scholarship</strong></h6><ul><li><strong>Issued by:</strong> CREC Silver Jubilee Endowment Trust ·<em>Apr 2024</em></li><li><strong>Associated with:</strong> National Institute of Technology Calicut</li><li>Awarded in recognition of academic merit and financial need, enabling me to pursue my education and overcome financial barriers.</li></ul><h6 id="reliance-foundation-scholarship"><strong>Reliance Foundation Scholarship</strong></h6><ul><li><strong>Issued by:</strong> Reliance Foundation ·<em>Feb 2024</em></li><li><strong>Description:</strong> Recipient of a prestigious scholarship supporting meritorious first-year undergraduate students nationwide.</li><li><strong>Selection:</strong> Awarded on a merit-cum-means basis, with up to 5,000 scholars selected.</li><li><strong>Benefits:</strong> Besides financial aid, it also connects students to a strong alumni network.</li></ul><h6 id="fiitjee-tuition-fee-waiver"><strong>FIITJEE Tuition Fee Waiver</strong></h6><ul><li><strong>Issued by:</strong> FIITJEE ·<em>Mar 2021</em></li><li><strong>Associated with:</strong> FIITJEE</li><li><strong>Award:</strong> Received 100% tuition fee waiver for excelling in the admission test</li></ul>
]]></content:encoded></item><item><title>Search Result</title><link>https://mummanajagadeesh.github.io/search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 UTC</pubDate><guid>https://mummanajagadeesh.github.io/search/</guid><description>&lt;![CDATA[]]></description><content:encoded>&lt;![CDATA[]]></content:encoded></item></channel></rss>