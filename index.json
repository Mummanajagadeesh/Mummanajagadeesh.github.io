[{"blog":["robotics","sensors","electronics","all","s"],"contents":"Sensors are to robots what eyes, ears, and skin are to humans—but with far fewer limits. While we rely on just five senses, robots can be equipped with many more, sensing distances, movement, vibrations, orientation, light intensity, and even chemical properties. These sensors form the bridge between the digital intelligence of a robot and the physical world it operates in.\nUnderstanding how sensors work is critical to designing effective robotic systems. Whether it’s a self-balancing robot, an autonomous vehicle, a drone, or a robotic vacuum cleaner, sensing the environment is the first step toward autonomous behavior. In this post, we’ll explore how ultrasonic sensors, LiDAR, and IMUs function internally, what kinds of data they provide, and how they’re used across different robotic systems—from hobbyist builds to industrial automation and research-grade platforms.\nUltrasonic Sensors Ultrasonic sensors are one of the most accessible and affordable distance-measuring tools used in robotics. They mimic echolocation, the same principle bats and dolphins use to \u0026ldquo;see\u0026rdquo; in the dark.\nHow Ultrasonic Sensors Work Internally The core of an ultrasonic sensor includes:\nTransmitter: Emits ultrasonic sound waves (usually 40 kHz). Receiver: Listens for echoes bouncing back from nearby objects. Timing Circuit: Measures the time between sending and receiving the pulse. When a microcontroller triggers the sensor, it sends out a pulse. The sound wave travels through the air, hits a surface, and reflects back. The time-of-flight is recorded, and with the known speed of sound (around 343 m/s), the distance is calculated.\nAt the heart of an ultrasonic sensor are piezoelectric transducers that convert electrical signals into mechanical vibrations (sound) and vice versa. The transmitter emits high-frequency sound waves (typically 40 kHz), which reflect off nearby objects and are captured by the receiver.\nThe key measurement is the time-of-flight $t$, which allows us to calculate the distance $d$ to the object using the equation:\n$$ d = \\frac{v \\cdot t}{2} $$\nWhere:\n$v$ is the speed of sound in air (~343 m/s at 20°C), $t$ is the round-trip travel time of the pulse, The division by 2 accounts for the round trip. The transducers are made of ceramic materials like lead zirconate titanate (PZT), which deform when voltage is applied (inverse piezoelectric effect), and generate voltage when deformed (direct effect). These are often mounted on a metal plate tuned for resonance.\nSensors like the HC-SR04 implement this in a very compact form factor, making them ideal for small robots.\nFabrication and Signal Processing Internally, most modules include:\nPulse generator circuit (typically a 555 timer or microcontroller), High-voltage driver for the transmitter (~80–100 V pulses), Amplifier and comparator for echo detection, Timing circuit or microcontroller to compute distances. Noise filtering (both analog and digital) is crucial due to environmental interference and multipath echoes. Advanced ultrasonic sensors also use frequency modulation and chirp signals for better resolution in noisy environments.\nCharacteristics and Behavior Range: Typically from 2 cm to 400 cm. Field of View: Approximately 15° to 30°, depending on the model. Update Rate: Usually around 10–20 Hz. Surface Sensitivity: Struggles with soft, angled, or sound-absorbing materials. Use Cases in Robotics Collision avoidance in indoor robots Wall-following behavior in maze-solving bots Height detection in drones during landing Presence detection for automatic doors Common Types and Modules HC-SR04: Low-cost, simple interface (trigger and echo pins) MaxBotix EZ series: Analog and serial output, waterproof variants available Parallax Ping))): Single-pin interface, good for microcontroller integration They’re often used in competitions like line-followers, Sumo bots, and micro-mouse challenges for close-proximity detection.\nLiDAR (Light Detection and Ranging) LiDAR revolutionizes robotic perception by creating precise, high-resolution maps of the environment using lasers. Unlike ultrasonic sensors, LiDARs are capable of measuring thousands of points per second.\nWorking Principle and Internal Architecture At its core, LiDAR works by firing laser pulses and measuring the time it takes for each to bounce back from surrounding objects.\nThere are two primary mechanisms:\nRotating Scanners: Use a spinning head to sweep the laser across a 2D or 3D field. Solid-State LiDARs: Use optical phased arrays or MEMS mirrors to steer laser beams without moving parts. Data points are time-stamped and angular-position-tagged to create a point cloud, which is a detailed 2D or 3D map of the surroundings.\nLiDARs generally operate in the near-infrared spectrum (around 850–1550 nm), and advanced models perform intensity correction and multi-echo analysis to detect partially transparent or low-reflectivity objects.\nUse Cases Across Robotic Systems SLAM for autonomous navigation Terrain mapping for aerial drones Object detection and classification in self-driving cars Localization and collision avoidance in warehouse robots In challenges like DARPA Urban Challenge or Amazon Picking Challenge, LiDAR has been central to navigation and environmental awareness.\nTypes of LiDAR and Their Operating Principles Time-of-Flight LiDAR (ToF) The simplest form, it emits short laser pulses and measures the return time. The distance $d$ is computed similarly to ultrasonic:\n$$ d = \\frac{c \\cdot t}{2} $$\nWhere:\n$c$ is the speed of light (~3×10⁸ m/s), $t$ is the measured time-of-flight. Because light travels so fast, even nanosecond delays matter. High-speed timing circuitry and avalanche photodiodes (APDs) are used for accurate detection.\nFrequency-Modulated Continuous Wave (FMCW) LiDAR Instead of pulsing, FMCW LiDAR emits a continuous laser whose frequency changes over time (chirp). When reflected light returns, it’s mixed with the transmitted signal, producing a beat frequency. This allows the sensor to calculate both distance and velocity, similar to radar.\nFMCW is more resistant to ambient light and interference and is used in high-resolution automotive LiDARs.\nFlash LiDAR Flash LiDARs emit a single broad laser pulse and use a 2D photodetector array (e.g., CMOS or SPAD arrays) to capture the entire scene at once. No moving parts are needed, but the resolution is often lower than rotating models.\nFabrication and Components Most rotating LiDARs use:\nLaser emitters (VCSELs or edge-emitting lasers), MEMS mirrors or spinning prisms for beam steering, Photodiodes or SPADs (single-photon avalanche diodes) for reception, High-frequency TDCs (Time-to-Digital Converters) for time measurements. LiDARs must manage heat dissipation, eye safety (Class 1 lasers), reflectivity compensation, and synchronization in multi-LiDAR systems.\nCoordinate Frames and Point Cloud Generation As the LiDAR rotates or sweeps the laser across space, it tags each point with an angle $\\theta$, elevation $\\phi$, and range $r$. These are converted to Cartesian coordinates via:\n$$ x = r \\cdot \\cos(\\phi) \\cdot \\cos(\\theta) \\ y = r \\cdot \\cos(\\phi) \\cdot \\sin(\\theta) \\ z = r \\cdot \\sin(\\phi) $$\nThe result is a dense point cloud representing 3D surfaces in the environment. These are fed into algorithms like ICP (Iterative Closest Point) or NDT (Normal Distributions Transform) for localization and mapping.\nSome LiDARs support ROS out-of-the-box, enabling seamless integration into SLAM, path-planning, and navigation stacks.\nInertial Measurement Units (IMUs) While ultrasonic and LiDAR sensors detect the external environment, IMUs detect internal motion. They act like a robot’s inner ear, measuring how it moves, tilts, rotates, and accelerates in 3D space.\nWhat\u0026rsquo;s Inside an IMU? An IMU typically consists of:\nAccelerometers (3-axis): Detect linear movement or tilt. Gyroscopes (3-axis): Measure angular velocity. Magnetometers (optional): Measure magnetic fields to help determine orientation relative to Earth’s magnetic north. Each of these components is based on micro-electro-mechanical systems (MEMS), where minuscule moving parts respond to inertia.\nAdvanced IMUs also include:\nDigital Signal Processors (DSPs) to fuse sensor data Temperature compensation mechanisms Calibration routines to correct biases and drift Motion Tracking and Sensor Fusion Raw sensor data can be noisy. To extract meaningful orientation (pitch, roll, yaw), systems apply fusion algorithms like:\nComplementary filters: Simple and fast, ideal for real-time use Kalman filters: Optimal estimation using uncertainty models Madgwick/Mahony filters: Lightweight quaternion-based filters for embedded systems IMUs can report:\nOrientation: Useful for balancing robots Velocity and displacement: By integrating acceleration Heading direction: With magnetometer fusion Applications in Robotics Flight stabilization in drones and quadcopters Dead reckoning in indoor mobile robots Motion tracking in robotic arms Gesture recognition in wearable robotics Balancing mechanisms in humanoid robots and self-balancing bots Widely Used Modules MPU-6050: Affordable 6-axis IMU (gyro + accelerometer) MPU-9250: 9-axis with magnetometer BNO055: Sensor-fusion onboard, outputs orientation directly Xsens MTi series: Precision-grade used in robotics research, VR, and aerospace In multi-sensor setups, the IMU fills in gaps where GPS, visual odometry, or LiDAR might be blind or slow—especially for fast-motion tracking or indoors.\nCombining the Three: Sensor Fusion in Robotics Robots almost never rely on a single sensor. Each sensor type has strengths and weaknesses. Combining multiple sensors allows for a more robust and accurate understanding of both the robot’s state and its environment.\nTypical Multi-Sensor Stacks Autonomous Ground Vehicles:\nLiDAR for 3D obstacle detection and SLAM IMU for estimating pose between LiDAR frames Ultrasonic for low-speed near-field collision avoidance Aerial Drones:\nIMU for stabilization and flight dynamics Ultrasonic or LiDAR for ground-following GPS and magnetometer for outdoor localization Mobile Robots (Indoors):\nLiDAR + IMU for accurate SLAM Ultrasonic for tight navigation in cluttered environments Wheel encoders for odometry Sensor Fusion Algorithms Data from IMU, LiDAR, and ultrasonic sensors can be fused using:\nExtended Kalman Filter (EKF): For localization Graph-based SLAM: For map building and loop closure Particle Filters: For uncertainty-based location estimation Robotic frameworks like ROS provide built-in packages (e.g., robot_localization, cartographer, gmapping) to handle sensor fusion, time synchronization, and state estimation.\nFinal Thoughts Sensors form the foundation of autonomy in robotics. Ultrasonic sensors help robots detect proximity with simple and cost-effective mechanisms. LiDAR delivers rich environmental mapping and spatial understanding. IMUs give robots a sense of their own movement and orientation. When used together, they transform a passive machine into a responsive, adaptive system capable of operating in complex, unpredictable environments.\nChoosing the right combination of sensors depends on your robotic platform’s goals, budget, and operating environment. Understanding how each sensor works internally gives you the power to not just use them—but to trust them, tweak them, and optimize them for your own unique robotics challenges.\n","date":"June 10, 2025","image":"/images/post/sensors/image_hu10576309995285800574.png","permalink":"/blog/sensors-in-robotics/","title":"Sensors in Robotics: How Ultrasonic, LiDAR, and IMU Work"},{"blog":["robotics","simulations","all","s"],"contents":"Hardware does not come with an undo button. Once you power it on, mistakes—from reversed wiring to faulty code—can result in costly damage. Motors may overheat, printed circuit boards (PCBs) can be fried, and sensors may break. These issues turn exciting projects into frustrating repair sessions. The autonomous drone shown above, designed for GNSS-denied environments in webots as part of the ISRO Robotics Challenge, is a perfect example—where careful planning, testing, and hardware safety were critical at every step\nBuilding a robot requires a fusion of mechanical engineering, electronics, and software development. Each component adds complexity and risk. Even carefully assembled hardware can fail due to unexpected interactions, timing errors, or unforeseen edge cases. For this reason, simulation has become indispensable in modern robotics development. It offers a safe, controlled environment for troubleshooting, iteration, and validation before risking physical damage.\nIt’s often said that simulations don’t match real-world behavior. In many cases, that belief comes from experience with the wrong tools—or not knowing how to use the right ones to their full potential. When applied thoughtfully, simulation is one of the most valuable steps in the engineering process. It doesn’t aim to replace physical testing, but it helps you explore possibilities, spot early failures, and make more informed decisions before anything is built.\nSimulations won’t capture every detail. Real-world factors like friction, wear, or long-term fatigue are complex. But even rough estimates can be extremely helpful. For example, you might not know the exact point a part will wear out, but simulation can still help you figure out if that’s likely to happen in 500 cycles or 10,000—enough to guide better choices early on.\nConsider a car suspension system. Simulating how it behaves under different loads or road conditions won’t cover every variable, but it will give insight into stress points, spring performance, and damping behavior. That saves time, reduces rework, and makes later testing more focused and meaningful.\nThe same applies across engineering—from robots and drones to embedded systems and mechanical linkages. Simulation helps reduce guesswork, not eliminate all uncertainty.\nWith that in mind, let’s now focus on simulation tools designed specifically for robotics, where virtual environments allow safe, iterative development of autonomous systems using physics-based testing and sensor feedback.\nWhy Simulation Should Be Your First Step Testing robotics systems on physical hardware can be slow, risky, and expensive. Running a new control algorithm directly on a robot means that every bug risks damaging motors, servos, or electronics. Additionally, resetting or fixing hardware after a failure is often time-consuming.\nSimulation provides a virtual workspace where you can freely test your ideas. It lets you push your robot’s limits without concern for physical harm. For instance, a line-following robot that veers off in the real world could cause tire wear or crashes. In simulation, the same behavior results in easy-to-reset failure states.\nBeyond safety, simulation accelerates development cycles. You can rapidly test variations, try extreme cases, and collect data for analysis—all without hardware setup. This speed allows more design exploration and better performance tuning before physical trials.\nExploring Popular Robotics Simulation Software Gazebo (Ignition Gazebo) Gazebo is one of the most widely adopted simulators in robotics, especially for projects built around the Robot Operating System (ROS). Its strengths include:\nAccurate Physics Simulation: Supports rigid body dynamics, collision detection, joint constraints, and sensors like cameras, IMUs, and LIDAR. Extensibility: Custom plugins allow developers to add specialized behaviors, sensor models, or controllers. Integration with ROS: Facilitates seamless interaction between simulation and ROS nodes, enabling realistic sensor data streaming and control commands. Gazebo’s physics engine realistically models gravity, friction, and inertia, which is critical when validating mechanical designs and control strategies. For example, you can simulate a robotic arm lifting an object, observing torque requirements and joint limits without risking hardware damage.\nWebots Webots offers a user-friendly environment with a library of pre-built robots and sensors. It is popular in academia and education due to its accessibility and versatility.\nSensor Simulation: Supports cameras, GPS, LIDAR, and sonar sensors. Multi-language Support: Compatible with C, C++, Python, MATLAB, and ROS. Rich Visualization: Allows realistic rendering of environments and robots. Webots is excellent for prototyping robot navigation, sensor fusion, and multi-agent behaviors, such as swarm robotics or cooperative tasks.\nCoppeliaSim (formerly V-REP) CoppeliaSim excels in fast prototyping, multi-robot simulation, and scripting flexibility.\nEmbedded Lua Scripting: Enables complex robot logic directly inside the simulation. Custom Physics Engines: Supports multiple physics engines like Bullet and ODE. Complex Robots: Ideal for testing articulated arms, humanoids, and swarms. It is particularly useful for robotics researchers testing novel kinematics, grasping algorithms, or coordination strategies.\nUnity and Unreal Engine Game engines like Unity and Unreal offer photorealistic rendering and advanced physics but require more setup.\nVisual Fidelity: Useful for human-robot interaction research or virtual reality applications. Physics Customization: Allows integration of custom robot models and environments. Cross-Platform: Simulations can be deployed on VR headsets or mobile devices. These engines are increasingly used to simulate drones, autonomous vehicles, or robots interacting in complex, visually rich environments.\nMATLAB and Simulink These platforms are favored in control system design and embedded systems prototyping.\nControl System Modeling: Easily create and tune PID controllers, state machines, and observers. Co-simulation: Interfaces with physical hardware and other simulators. Real-Time Testing: Supports Hardware-in-the-Loop (HIL) simulations. Simulating Mechanical Behavior and Motion Control Mechanical failures often originate from unexpected or excessive forces on robot components. Inverse kinematics calculations that do not respect joint limits or unexpected acceleration spikes can physically damage servos, gears, or frames.\nSimulators allow you to:\nTest mechanical limits by setting maximum joint angles and torque constraints. Visualize robot trajectories in 3D, including velocities and accelerations. Detect collisions between robot parts or with obstacles. Simulate dynamic loads such as inertia, friction, and impact forces. For example, a six-axis robotic arm performing complex pick-and-place tasks can be analyzed in CoppeliaSim or Gazebo to confirm smooth movements and avoid collisions before being built.\nThis kind of virtual stress-testing is critical, especially in custom-built or highly integrated systems where hardware replacement costs are high.\nSensor Simulation and Synthetic Data Generation Sensor data is inherently noisy, affected by environmental conditions, and often unpredictable. Testing sensor-dependent algorithms on real hardware is complicated by fluctuating lighting, temperature changes, and electromagnetic interference.\nSimulators provide:\nSynthetic sensor data that mimics real-world noise, resolution, latency, and distortion. Emulated GPS signals with configurable drift or dropout. Simulated LIDAR and sonar scans with realistic point clouds. Virtual cameras replicating different resolutions, frame rates, and field of view, supporting early-stage computer vision development. For example, in Webots or Gazebo, you can simulate a robot navigating a warehouse environment with virtual LIDAR data to develop and validate obstacle avoidance algorithms.\nControl Algorithm Tuning and Feedback Loop Testing Tuning controllers like PID, MPC, or adaptive algorithms directly on hardware can be risky and inefficient. Aggressive parameters can cause hardware stress, while conservative ones reduce performance.\nIn simulation, you can:\nTune parameters interactively and observe system responses in real-time. Apply disturbances or sudden setpoint changes to test controller robustness. Visualize control variables such as error, output commands, and system states. Run automated test sequences to evaluate stability over many iterations. Such systematic tuning shortens development time and reduces hardware wear.\nNavigation, Mapping, and Path Planning Testing mobile robot navigation algorithms on physical hardware is logistically challenging.\nSimulation lets you:\nCreate complex maps and dynamic environments with moving obstacles. Simulate sensor inputs for localization algorithms (e.g., SLAM). Test various path planning algorithms and obstacle avoidance strategies. Collect detailed metrics on path efficiency, collisions, and timing. For instance, Gazebo allows simulation of robots navigating city blocks or indoor environments with pedestrians, which would be costly and time-consuming to replicate physically.\nFault Injection and Robustness Testing Unexpected faults like sensor failures, communication loss, or power drops pose serious risks.\nSimulation enables fault injection scenarios such as:\nSimulating a failed IMU or GPS sensor during operation. Injecting corrupted sensor data or signal delays. Simulating battery voltage drops or power interruptions. Testing emergency stop responses. These tests help ensure your system can safely handle real-world faults.\nTransitioning from Simulation to Hardware While simulation can closely approximate reality, some factors are difficult to replicate precisely, including thermal effects, electrical noise, and material inconsistencies.\nA staged approach to hardware testing is recommended:\nBegin with low-power, low-risk tests. Monitor electrical current, motor temperatures, and mechanical stresses. Implement manual overrides and emergency stops. Gradually increase complexity while verifying system behavior. Simulations prepare your system, but careful hardware validation remains essential.\nConclusion Simulation is a cornerstone of responsible and efficient robotics development. It provides a safe environment to iterate faster, catch errors early, and improve system robustness. Robots that pass extensive simulated testing have a significantly higher chance of succeeding in real-world deployment.\nBefore powering on your hardware, ask: Have I validated this thoroughly in simulation? Taking the extra time to simulate can save you wires, money, and frustration.\n","date":"June 4, 2025","image":"/images/post/sim/image_hu2676445633058820239.png","permalink":"/blog/robotics-simulation-tools/","title":"Debugging a Robot In Simulation Before You Burn Wires"},{"blog":["robotics","autonomous-vehicles","AI","all","s"],"contents":"Tesla\u0026rsquo;s bold claim that “humans drive with eyes and a brain, so our cars will too” sparked one of the most polarizing debates in autonomous vehicle (AV) technology: Can vision-only systems truly compete with—or even outperform—multi-sensor fusion architectures?\nAt the core of this debate lies a fundamental design choice in how self-driving cars perceive and interpret their environment. Do you rely solely on high-resolution cameras feeding convolutional neural networks (CNNs), or do you incorporate data from LiDARs, radars, ultrasonic sensors, GPS, and IMUs into a probabilistic fusion framework?\nTesla Bet on Vision Alone—But Is That Enough?\nThis blog post dives deep into the technical foundations of both approaches—no marketing hype, just real-world system trade-offs.\nThe Vision-Only Pipeline: Simplicity at Scale Computer vision-driven pipelines depend exclusively on camera feeds and vision algorithms to interpret the driving scene. In such systems, perception, localization, and planning are all derived from 2D (and inferred 3D) visual inputs. The typical sensor suite includes multiple cameras (e.g., wide-angle, fisheye, and long-range monocular) with overlapping fields of view.\nKey Components and Algorithms:\nConvolutional Neural Networks (CNNs) for real-time object detection and semantic segmentation Monocular and stereo depth estimation for 3D understanding without active range sensors Structure-from-Motion (SfM) and Visual SLAM (Simultaneous Localization and Mapping) for ego-position tracking Visual-inertial odometry (VIO) when paired with an IMU for improved pose estimation Occupancy grid prediction and Bird\u0026rsquo;s-Eye View (BEV) transformation layers in modern neural nets Advantages of Vision-Only:\nLower hardware cost — cameras are significantly cheaper than LiDAR or high-resolution radar High spatial resolution — fine-grained details for understanding signs, lane markings, pedestrians Easier to scale in terms of manufacturing and deployment Lightweight compute pipelines when paired with dedicated vision accelerators (e.g., Tesla\u0026rsquo;s FSD Chip) Challenges and Limitations:\nPoor low-light and adverse weather performance — vision degrades under fog, glare, snow, and nighttime Depth ambiguity — monocular cameras have no inherent mechanism to measure absolute distances Occlusion vulnerability — cannot \u0026ldquo;see through\u0026rdquo; or around obstacles like radar can Requires massive training datasets to learn long-tail edge cases and rare scenarios Longer convergence time in dynamic environments compared to systems with direct ranging Sensor Fusion: Redundancy with Precision Sensor fusion-based approaches rely on the integration of multiple complementary sensors to provide a robust, redundant understanding of the driving environment. The goal is not just perception, but confidence in perception—measured through probabilistic uncertainty modeling.\nTypical Sensor Suite:\nLiDAR for high-fidelity 3D point clouds and environmental geometry Radar for accurate long-range object detection and relative velocity estimation Ultrasonic sensors for near-field object proximity detection IMU (Inertial Measurement Unit) and GNSS/GPS for precise localization and heading Cameras for classification, semantic understanding, and visual cues Fusion Techniques Used:\nKalman Filters and Extended Kalman Filters (EKF) for tracking and data fusion Bayesian occupancy grids and particle filters for probabilistic modeling of the environment Point cloud registration and scan-matching for SLAM and localization Multi-modal neural networks (early fusion, late fusion, or mid-level fusion) to learn sensor interactions Time-synchronized sensor pipelines using shared timestamps or common clocks (e.g., via PTP) Strengths of Sensor Fusion:\nRedundant sensing enables failover — if one sensor fails or underperforms, others compensate Highly accurate localization — LiDAR + GPS + IMU enables centimeter-level positioning Weather and lighting resilience — radar works in fog, LiDAR handles low light Easier validation for safety standards due to decoupled, interpretable sensor models Real-time tracking of fast-moving or partially occluded objects with fused data Trade-Offs and Challenges:\nIncreased hardware complexity and cost — LiDAR units alone can be prohibitively expensive Synchronization challenges — multiple sensors require precise time alignment Data bandwidth and processing overhead — high-resolution LiDAR and camera feeds demand large compute Sensor calibration drift — performance can degrade over time without periodic recalibration Integration and testing effort scales non-linearly with each added sensor type Compute Stack Comparison: Monolithic Learning vs Modular Architecture The divergence in sensing strategies also impacts the compute architecture. Vision-only systems often favor end-to-end learning models that map raw pixels directly to driving actions or high-level waypoints. These are usually supported by tightly-coupled hardware and software co-designs.\nVision-Based Stack Characteristics:\nUses centralized inference models — often transformer-based or multi-scale CNNs Optimized for throughput and low-latency inference on vision accelerators Relies heavily on self-supervised learning and temporal context (e.g., through recurrent layers or attention mechanisms) End-to-end learning reduces manual engineering but increases opacity In contrast, sensor fusion stacks adopt a modular approach. Perception, localization, prediction, and planning are separated into distinct components that pass structured data between them.\nSensor Fusion Stack Characteristics:\nModular by design — allows for independent validation of components Uses traditional control theory (e.g., PID, Kalman filters) alongside neural nets Compatible with middleware systems like ROS 2 and DDS for real-time communication Emphasizes redundancy and explainability — each decision can be traced to specific sensor data Safety, Certification, and Regulatory Considerations Safety is not just about perception quality—it\u0026rsquo;s about guaranteeing performance under all conditions, including edge cases and sensor degradation.\nVision-Only Stack Concerns:\nEnd-to-end models are harder to verify and interpret under functional safety standards like ISO 26262 or UL 4600 Lack of redundancy makes failure modes harder to recover from Susceptible to adversarial attacks and perceptual illusions (e.g., adversarial stop signs) Sensor Fusion Advantages in Safety:\nModular architecture aligns better with safety assurance frameworks Redundancy enables graceful degradation of performance Easier to simulate and test sensor failure scenarios in isolation Can validate sensor outputs individually and in combination under SOTIF (Safety of the Intended Functionality) The Scalability Factor: Mass Production vs Pilot Programs Another axis of comparison is scalability. Vision-only systems are inherently more scalable due to simpler hardware, lower BOM (bill of materials) costs, and smaller form factors. They are well-suited for consumer-grade driver-assistance systems (ADAS) and incremental autonomy.\nSensor fusion stacks, with their higher hardware and compute requirements, are currently more viable in geo-fenced pilot programs (e.g., Waymo One) or tightly-controlled urban environments.\nVision-only: Better fit for high-volume consumer vehicles with OTA updates and rapid iteration Sensor fusion: More reliable for fixed-route autonomous shuttles, robotaxis, or logistics fleets So, Who Wins the Race? There is no universal winner—only trade-offs shaped by technical, economic, and regulatory constraints. Here’s how the field splits:\nVision-only may excel in:\nHighway driving with structured lanes and predictable scenarios Scalable manufacturing and over-the-air (OTA) software improvements Cost-efficient autonomy at Level 2+ and Level 3 with human supervision fallback Sensor fusion remains critical for:\nComplex urban environments with pedestrians, cyclists, and occlusions Full Level 4 and Level 5 autonomy with no human fallback Use cases where safety and fault tolerance take precedence over cost Final Thought The real question isn\u0026rsquo;t “who wins?” but “what are you optimizing for?” Is it affordability and mass-market deployment? Or is it unassailable safety and fail-safe operation in all weather and lighting conditions?\nVision systems are getting better, and the gap is closing fast—but for now, sensor fusion still holds the edge in operational safety and robustness. Future systems may blur the lines entirely, with vision-first architectures augmented by sparse LiDAR or next-gen radar.\nUntil then, the self-driving race remains open—driven as much by philosophy as by engineering.\n","date":"May 30, 2025","image":"/images/post/ros/cv-sf_hu6298417913611717518.png","permalink":"/blog/cv-vs-sensor-fusion/","title":"Computer Vision vs. Sensor Fusion: Who Wins the Self-Driving Car Race?"},{"blog":["AIML","Basics","all","s"],"contents":"Machine learning is something that shows up all around us today—whether we\u0026rsquo;re aware of it or not. From personalized suggestions on YouTube and Netflix to automatic spam filtering in our inboxes, it’s quietly powering a lot of the tools we use daily.\nIf you’re just starting out and trying to figure out what machine learning actually is, this post is for you. I’ll try to keep things simple and honest, with no jargon or unnecessary complexity.\nWhat is Machine Learning? At its core, machine learning is about teaching computers to make decisions or predictions by learning from data, rather than following hard-coded instructions.\nIn traditional programming, we tell the computer exactly what to do in every situation. But for tasks like recognizing a handwritten digit or classifying whether an email is spam, it’s almost impossible to write those rules by hand.\nThat’s where machine learning comes in—it lets the computer learn patterns from examples instead.\nWhy It’s Useful There are many problems that are too complex or too fuzzy for us to define in exact rules. For example:\nHow do you program a robot to recognize a cat in a photo? Can you write fixed rules to understand spoken language? Rather than trying to write these rules ourselves, we let the machine figure them out by showing it many examples. The idea is: the more data it sees, the better it gets.\nThe Ingredients While machine learning can get quite advanced, the basic building blocks stay the same. Here are a few core ideas:\nData This is where everything begins. We usually have inputs (like images or text) and sometimes labels (like the correct digit or category). The quality and quantity of data heavily affect how well the model learns.\nModel This is the actual system that tries to learn patterns from the data. Think of it as a function that maps inputs to outputs—like taking an image and predicting the digit it shows.\nTraining During training, the model looks at the data and tries to adjust itself to make better predictions. This involves a lot of trial and error, usually guided by math behind the scenes.\nTesting After training, we check how well the model does on new data it hasn’t seen before. This helps us understand how well it might work in the real world.\nDifferent Types of Learning Depending on the kind of data and problem we’re dealing with, machine learning can work in a few different ways.\nSupervised Learning Here, we have both the input and the correct output (label). The model learns by comparing its predictions to the actual answers. This is used for tasks like image classification or spam detection.\nUnsupervised Learning In this case, the data doesn’t come with labels. The model tries to find patterns or groupings in the data on its own. It’s useful for things like clustering customers based on behavior.\nReinforcement Learning This is a bit different. The model learns by interacting with an environment and getting feedback in the form of rewards or penalties. It\u0026rsquo;s often used in games or robotics.\nA Simple Example Let’s say we’re building a model to recognize handwritten digits.\nWe collect thousands of images of digits (like from the MNIST dataset). We train a model by showing it these images along with the correct digit. Over time, it learns to recognize which patterns match which numbers. We then test it on new images to see how well it does. No need to manually define what a “2” looks like—it learns from seeing enough examples.\nWhere It’s Used Machine learning shows up in more places than we might expect:\nEmail spam filters Voice assistants like Siri or Google Assistant Self-driving car systems Personalized news feeds Predicting stock prices or health conditions It’s not always perfect, but in many cases, it’s a practical and powerful tool.\nAlgorithms You Might Hear About If you start looking into ML, you’ll come across names like:\nLinear Regression Decision Trees Neural Networks K-Nearest Neighbors Random Forests Support Vector Machines Each of these is a different way to learn from data. You don’t need to memorize them right away—just knowing the names can help when you come across them later.\nWant to Explore It Yourself? If you\u0026rsquo;re curious to try things out, here are a few beginner-friendly ways to get started:\nPlay with Google’s Teachable Machine, where you can train simple models without writing any code. Learn Python if you\u0026rsquo;re new to coding—it’s the go-to language for ML. Look into tools like scikit-learn, which make it easier to build and test models. Explore beginner datasets like the Iris dataset or handwritten digits. The best way to learn is to get your hands dirty. Start small, make mistakes, and keep going.\nFinal Thoughts Machine learning can seem intimidating at first, especially with all the terminology and math behind it. But at the heart of it, it’s really just about using data to make better decisions.\nIf you’re interested in this space, there’s plenty of room to grow—step by step. You don’t need to know everything to get started. I definitely don’t. But the more you explore, the clearer things become.\n","date":"May 22, 2025","image":"/images/post/ml/image_hu4614632101585577635.png","permalink":"/blog/understanding-the-basics-of-ml/","title":"Understanding the Basics of Machine Learning"},{"blog":["hardware","VLSI","all","s"],"contents":"In an industry where silicon efficiency, design flexibility, and time-to-market are paramount, the limitations of proprietary processor architectures are becoming increasingly apparent. RISC-V, an open-source ISA, is reshaping the VLSI design landscape by enabling deeper hardware-software co-design, architectural customization at the RTL level, and tighter control over power, performance, and area (PPA)—without the constraints of licensing or closed specifications\nUnderstanding Architecture in Processor Design At the heart of any computing system is the Instruction Set Architecture (ISA). This defines the set of operations a processor can perform and how software communicates with the hardware. It shapes not just how processors work internally, but how software is written, optimized, and ultimately executed on silicon.\nTwo dominant ISAs in today’s landscape are x86 (used in most desktops and servers) and ARM (used in mobile and embedded systems). Both are proprietary—x86 is maintained by Intel/AMD, and ARM is owned by Arm Holdings. Companies must license these architectures, often facing high costs and restricted design flexibility.\nThis proprietary control has defined semiconductor innovation for decades, but now, with increasing pressure to design application-specific silicon, these limitations are being challenged—especially by RISC-V.\nThe Introduction of RISC-V RISC-V was born at UC Berkeley in 2010 as the fifth iteration of a Reduced Instruction Set Computing project—hence the “V.” It is open, modular, and extensible by design, and can be used without paying royalties or navigating restrictive agreements.\nFor VLSI engineers, this means unprecedented freedom. You get to start from a clean, minimal base and selectively add extensions tailored to your needs, reducing unnecessary silicon overhead and enabling custom solutions from IoT to high-performance computing.\nRISC and CISC: Architectural Context To appreciate RISC-V’s strengths, it helps to contrast CISC (Complex Instruction Set Computing) with RISC:\nCISC (like x86): Complex instructions, often with multiple micro-operations. Efficient in code density, but hard to pipeline, verify, and implement in silicon.\nRISC (like ARM and RISC-V): Simple, fixed-length instructions. Easier to decode and pipeline, and better suited for high-frequency, low-power design.\nRISC-V embraces the RISC model but modernizes it—providing a clean ISA spec, optional extensions (e.g., vector, DSP, security), and a focus on hardware simplicity. This aligns beautifully with modern VLSI workflows, where modularity, timing closure, and area optimization matter deeply.\nWhy Open Source Architecture Matters From a VLSI perspective, an open ISA like RISC-V introduces significant, tangible advantages:\nDesign Freedom: Tailor the core to fit application-specific workloads without bloated instruction logic. Cost Efficiency: No licensing fees—especially important for startups, academia, and emerging markets. Custom Extensions: Build domain-specific accelerators (e.g., ML, DSP, crypto) directly into the core via new instructions. Toolchain Ecosystem: Support from GCC, LLVM, GDB, QEMU, Verilator, and commercial EDA tools (Synopsys, Cadence, Siemens) is growing rapidly. Security Transparency: Open ISAs allow easier auditing and implementation of secure enclaves or custom threat models. This flexibility fuels innovation in SoC design, chiplets, and application-specific processors across industries.\nDeep Dive: Why VLSI Engineers Are Embracing RISC-V Let’s go deeper into how RISC-V benefits VLSI design workflows from RTL to GDSII.\n1. RTL Simplicity and Design Modularity Minimal baseline (RV32I/64I) → Lean RTL for quick bring-up Modular extensions → Easy to drop or add features like floating-point, vector, or custom logic Clean decode logic → Easier RTL-level integration, debug, and retiming You can design an ultra-small controller with 10k gates—or scale to a multicore out-of-order CPU using Rocket/BOOM/CVA6 cores.\n2. Power, Performance, Area (PPA) Optimization Power: Fewer gates = lower dynamic power. Custom instructions can reduce instruction count → less switching. Performance: Small pipelines → high frequencies with low stalls. Custom ALUs enable single-cycle workloads for ML/DSP. Area: Only include what you need—strip unused logic for smaller die area. Example: A stripped-down RV32I core can synthesize to \u0026lt;20K gates on 28nm, 30% smaller than ARM Cortex-M0, while supporting custom extensions.\n3. Backend Flow Advantages Predictable logic patterns → Better synthesis QoR Fewer stages and hazards → Easier place-and-route (PnR) and timing closure Clock tree optimization → Simpler pipeline = less skew, reduced power in CTS This translates to faster physical implementation and fewer ECOs in advanced nodes (e.g., 7nm and below).\n4. Easier Verification and DFT Smaller ISA state space → Fewer corner cases during simulation and formal RISCV-DV and RISCV-ISAC tools → Random test generation and ISA compliance coverage DFT: Cleaner logic → higher scan coverage, easier ATPG, and shorter test times 5. Seamless IP Integration and SoC Interoperability Custom Coprocessors: Easily attach accelerators via native ISA hooks Chiplet Design: Lightweight, configurable cores for distributed compute Tight Coupling: Use custom load/store instructions to interface directly with on-die IP blocks In SoC design, RISC-V lets you co-optimize the core around your IP rather than the other way around.\nIndustrial Adoption and Ecosystem Growth RISC-V has gone from academic research to real-world silicon:\nSiFive, Alibaba T-Head, Andes, Microchip, and others have released commercial cores IP vendors like Codasip and Ventana offer performance-tuned RISC-V blocks Tool vendors support RISC-V in mainstream flows (Synopsys Design Compiler, Cadence Genus, Siemens Questa) Standard extensions include:\nVector instructions: For ML/DSP Bit manipulation: For crypto/security Hypervisor \u0026amp; Privileged modes: For OS-level execution This ecosystem lets VLSI teams move from RTL to verified tapeout-ready silicon faster.\nRelevance for VLSI Design Teams Here’s why VLSI teams increasingly prefer RISC-V over proprietary ISAs:\nSmaller logic footprint Faster simulation and timing convergence Lower licensing and NRE cost Scalable from MCU to datacenter chiplet Freedom to innovate at the ISA level In aggressive tapeout schedules—where every mm², mW, and ns counts—RISC-V’s flexibility is a strategic asset.\nLooking Ahead The shift from general-purpose chips to domain-specific silicon is accelerating. RISC-V aligns with this trend:\nEdge AI → Custom instructions for inference workloads Automotive → Safety-certifiable minimal cores Aerospace/Defense → Fully auditable ISA and toolchains Cloud → High-performance chiplets with custom ISA tuning With open-source RTL (e.g., PicoRV, Ibex, BOOM) and toolchain maturity, RISC-V gives hardware teams full-stack control—from architecture to layout.\nConclusion RISC-V represents more than just a new instruction set—it’s a philosophical shift in how hardware is developed. For VLSI engineers, it breaks the ceiling imposed by closed architectures and unlocks new ways to optimize for PPA, flexibility, and silicon-level innovation.\nIt’s not just about replacing ARM or x86. It’s about building what you need—nothing more, nothing less.\nIn the era of domain-specific computing, RISC-V is not just an option—it’s a strategic advantage.\n","date":"May 15, 2025","image":"/images/post/riscv/image_hu17697809258383421589.png","permalink":"/blog/why-riscv-is-better/","title":"Why RISC-V Can Be a Game Changer?"},{"blog":["VLSI","hardware","all","s"],"contents":"Wonder why AI, modern smartphones, and countless digital devices have become so powerful yet compact? The secret lies in the ability to pack billions of transistors into tiny silicon chips — a feat accomplished through Very Large-Scale Integration (VLSI). At the core of this accomplishment is a complex, multi-step design flow that transforms abstract hardware concepts into a physical chip ready for fabrication.\nThis flow spans multiple abstraction layers, engineering disciplines, and verification cycles, culminating in a GDSII layout file — the silicon mask data sent to the foundry. The path from Register Transfer Level (RTL) to GDSII involves a series of crucial steps, each demanding specialized tools and deep expertise. Understanding this journey is essential to appreciate how high-level design intentions translate into intricate transistor arrangements and interconnected metal layers, ultimately defining chip functionality, performance, power, and reliability.\nOverview of VLSI Design Flow Stages Before diving into the details, here is a list of the core stages in the VLSI design flow, following the journey from high-level design to final chip layout:\nRTL Design Functional Verification Logic Synthesis Design for Testability (DFT) Insertion Floorplanning Placement Clock Tree Synthesis (CTS) Routing Static Timing Analysis (STA) Physical Verification (DRC, LVS, etc.) GDSII File Generation Each stage transforms the design closer to a physical reality and imposes critical constraints for the subsequent steps. Let’s explore each of these stages with a detailed, low-level perspective.\nRTL Design: Abstract Behavioral Description At the foundation of any digital IC design lies the Register Transfer Level (RTL) description. Here, engineers use Hardware Description Languages like Verilog or VHDL to describe data flow between registers and the combinational logic applied to that data during clock cycles.\nThe RTL is an algorithmic, cycle-accurate model specifying how registers transfer data, how multiplexers select inputs, how finite state machines transition between states, and how arithmetic operations execute. It abstracts away transistor-level details and focuses on the logic-level function.\nThis abstraction allows for flexible architectural exploration — for example, deciding the number of pipeline stages or datapath widths — long before hardware resources are allocated. The RTL code forms the contract between what the chip should do and how it will later be implemented.\nFunctional Verification: Exhaustive Behavioral Testing Designing a complex chip solely based on code is risky without rigorous validation. Functional verification ensures that the RTL code behaves exactly as intended, catching logical errors, corner cases, and unexpected behaviors.\nVerification engineers write testbenches, which generate input stimulus and check outputs against expected results. They use simulation tools to emulate the design cycle-by-cycle, checking timing, control signals, and data correctness. Formal verification techniques and assertion-based verification (SVA) are also employed to mathematically prove design properties.\nThe verification scope covers both typical and edge cases, attempting to exhaustively exercise all possible scenarios. Detecting and correcting bugs at this stage prevents costly iterations in physical design.\nLogic Synthesis: Mapping RTL to Gate-Level Implementation Once the RTL passes functional verification, it enters logic synthesis — the translation of behavioral code into a gate-level netlist of standard cells.\nSynthesis tools map abstract RTL constructs like if-else statements and case blocks into logic gates such as NANDs, NORs, and flip-flops from a predefined standard cell library. The process includes:\nTechnology mapping: Choosing gate types optimized for the target fabrication technology node (e.g., 7nm, 14nm). Logic optimization: Minimizing gate count, reducing delay, and balancing power consumption. Constraint application: Incorporating timing goals, area budgets, and power constraints specified by the designer. The synthesized netlist represents a logical implementation ready for physical design. However, it remains technology-dependent and not yet placed or routed on silicon.\nDesign for Testability (DFT) Insertion: Facilitating Post-Silicon Testing Modern chips consist of billions of transistors, making direct transistor-level testing impractical. Therefore, Design for Testability (DFT) techniques are integrated to enable effective manufacturing tests.\nCommon DFT techniques include:\nScan insertion: Flip-flops are replaced or augmented with scan cells connected into scan chains, allowing serial loading and observation of internal states. Built-In Self-Test (BIST): Logic to generate test patterns internally to check memory arrays or logic blocks. Boundary scan: For I/O pin testing using standardized protocols (IEEE 1149.1). DFT insertion is carefully balanced against area overhead and timing impact but is essential to detect manufacturing defects and improve yield.\nFloorplanning: Defining the Physical Architecture Transitioning from logical netlists to a physical layout starts with floorplanning. This stage defines the macro-level organization of the chip:\nSilicon core size and aspect ratio Placement of Input/Output (I/O) pads Approximate locations for large blocks like memories and IP cores Power grid and clock domain planning Floorplanning establishes the canvas for subsequent steps. It aims to reduce routing congestion, optimize timing, and provide sufficient space for power delivery networks. Poor floorplanning choices can cascade into routing difficulties, timing violations, and increased power consumption.\nPlacement: Detailed Cell Positioning With a floorplan set, the standard cells from the netlist must be precisely placed. Placement tools assign exact coordinates for each cell to:\nMinimize interconnect wirelength, which impacts delay and power Reduce congestion to facilitate routing Optimize timing by clustering cells on critical paths Placement algorithms consider timing slack, cell density, and power grid connections to ensure a balanced, manufacturable layout. Placement quality significantly affects the chip’s speed, power, and routability.\nClock Tree Synthesis (CTS): Distributing the Clock Signal The clock signal synchronizes operations across the chip and must arrive at every sequential element with minimal skew and jitter. Clock Tree Synthesis (CTS) constructs a buffered network of clock drivers and buffers:\nDistributes the clock signal evenly Minimizes clock skew between registers Balances clock latency Reduces power consumption of clock distribution A well-constructed clock tree is critical to meet timing constraints and avoid data corruption caused by clock uncertainties.\nRouting: Connecting the Circuit Routing completes the physical implementation by creating metal interconnections between placed cells. Routing is performed in two phases:\nGlobal routing: Assigns approximate routing regions to avoid congestion hotspots Detailed routing: Lays out exact wire paths on available metal layers Routing must obey design rules, spacing, and layer usage constraints. It must also mitigate parasitic effects such as crosstalk and signal delay, which can impact signal integrity and timing.\nRouting is often the most time-consuming step due to the sheer complexity and number of nets.\nStatic Timing Analysis (STA): Timing Closure Verification Post-routing, the design undergoes Static Timing Analysis (STA) to verify all timing paths meet constraints:\nChecks setup and hold times for sequential elements Computes timing slack for all paths Detects timing violations that can cause malfunction at the target clock frequency STA is exhaustive and deterministic, analyzing every possible timing path without requiring test vectors. Timing failures must be resolved iteratively through design changes or buffering until timing closure is achieved.\nPhysical Verification: Ensuring Manufacturability and Correctness Before sending the design for fabrication, it undergoes rigorous physical verification to ensure manufacturability:\nDesign Rule Checks (DRC): Verify all layout geometry complies with foundry spacing, width, and layering rules. Layout Versus Schematic (LVS): Ensures the physical layout matches the netlist connectivity and topology. Additional checks for antenna effects, electromigration, and other reliability concerns are performed. These steps prevent costly manufacturing errors and guarantee that the chip will function as designed.\nGDSII File Generation: The Final Tape-Out The entire physical design is converted into a GDSII file, a binary format that contains detailed polygonal data representing every layer and feature of the chip layout.\nThis file is the definitive mask data used by the semiconductor foundry to fabricate the chip via photolithography. It encapsulates the result of months of design effort, verification, and optimization into a manufacturable format.\nThe GDSII is often the final output delivered to fabrication, marking the transition from design to physical silicon.\nConclusion The VLSI design flow from RTL to GDSII is a meticulous, multi-disciplinary journey that transforms abstract digital logic into a physical silicon chip. Each step — from writing behavioral code to verifying manufacturability — involves deep technical challenges and precision engineering.\nUnderstanding this flow is critical for anyone engaged in semiconductor design, as it reveals how a digital idea becomes a tangible, functioning integrated circuit powering today’s technology landscape.\nThe constant interplay between design abstraction, physical constraints, and verification rigor drives innovation and ensures chips meet ever-increasing demands for speed, power efficiency, and functionality.\n","date":"May 8, 2025","image":"/images/post/vlsi/image_hu1860035184891984411.png","permalink":"/blog/vlsi-design-flow/","title":"Introduction to VLSI Design Flow: RTL to GDSII"},{"blog":["robotics","ROS","all","s"],"contents":"Is ROS 1 still the right choice for your next robotics project, with its well-established tools and wide community support? Or, given the growing demand for real-time performance, scalability, and modern middleware, is it finally time to make the move to ROS 2?\nAs of the end of this month—May 2025—official support for ROS 1 will come to a close. This marks a major turning point in the robotics software ecosystem. For over a decade, ROS 1 (Robot Operating System) has served as the backbone of robotic development across research and industry. But the shift to ROS 2 is more than just an upgrade—it\u0026rsquo;s a rethinking of the platform from the ground up.\nThis article provides a clear look at the changes introduced in ROS 2, why ROS 1 is being sunset, and what this means for developers, researchers, and organizations working in robotics.\nWhat is ROS? ROS (Robot Operating System) is a flexible framework for writing robot software. Despite the name, it’s not a full-fledged OS but a middleware layer that sits on top of your actual operating system, handling the complexity of communication, modularity, and distributed execution.\nAt its core, ROS provides:\nA message-passing architecture for processes (nodes) to communicate. Tools to visualize data, record logs, and debug system behavior. Packages to handle everything from perception and motion planning to hardware abstraction. It’s the glue that connects sensors, control logic, actuators, and high-level decision-making in a robotics application. In ROS, each node performs a specific task—say, reading from a LIDAR sensor or planning a path—and publishes/subscribes to messages over topics. This enables a clean, modular architecture where components are reusable and decoupled.\nWhy ROS? What Difference Does It Make? Building robot software is not like writing a web app or scripting a microcontroller. Robots deal with real-world inputs—noisy data, real-time constraints, hardware failures—and are inherently distributed systems. Without a framework like ROS, you\u0026rsquo;d have to write your own communication protocols, serialization formats, data pipelines, logging infrastructure, and process management tools.\nROS abstracts these repetitive tasks. It provides:\nStandardized communication: Publish/subscribe and service-based interaction. Sensor integration: Drivers for cameras, IMUs, GPS, and more. Visualization: RViz, rqt, and introspection tools. Hardware abstraction: URDF and ROS Control for modeling and interfacing with robot hardware. With ROS, developers focus on algorithms and behavior, not the plumbing. It also offers a large open-source ecosystem, so you’re rarely starting from scratch. Whether you\u0026rsquo;re working on SLAM, navigation, or perception, someone’s likely already done 70% of the work.\nWhy ROS 1 Was Right (and Also Wrong) ROS 1 got a lot of things right. It gave researchers a shared language, simplified complex system integration, and accelerated robotics development. But its architecture was never built with real-time guarantees, security, or modern networking in mind.\nWhat ROS 1 did well:\nMade robotics accessible and modular. Encouraged rapid prototyping and experimentation. Built a massive ecosystem and developer base. Created tools like rosbag, RViz, and rqt that became industry standards. Where ROS 1 struggled:\nCommunication relied on a centralized ROS Master node, a single point of failure. No support for real-time scheduling or deterministic behavior. Lacked Quality of Service (QoS) configuration for topics. Poor multi-robot support—nodes weren’t namespace-aware by default. No native support for embedded devices or resource-constrained platforms. No built-in security model—data over the network was unencrypted and unauthenticated. These issues became roadblocks as robotics moved out of the lab and into industrial, consumer, and mission-critical environments.\nWhat Changed? Why ROS 2 is Better ROS 2 was designed to address the structural limitations of ROS 1. It’s a complete architectural rewrite based on modern software engineering practices and real-world deployment needs.\nHere\u0026rsquo;s a deeper look at what\u0026rsquo;s new:\nDDS-Based Communication (No More ROS Master) ROS 2 uses the Data Distribution Service (DDS) protocol for inter-process communication. DDS is decentralized, so there’s no need for a master node. Nodes can come and go freely, discover each other dynamically, and communicate peer-to-peer.\nThis enables:\nDistributed multi-robot systems with zero configuration. Fault tolerance—if a node dies, the system doesn’t collapse. Scalability across networks, platforms, and locations. Real-Time Support Unlike ROS 1, ROS 2 supports real-time execution. You can now:\nRun control loops with deterministic timing. Assign thread priorities and memory bounds. Interface directly with real-time operating systems (RTOS). This makes ROS 2 viable for hard real-time applications like industrial robot arms, autonomous vehicles, and flight controllers.\nQuality of Service (QoS) DDS allows developers to fine-tune how messages are delivered with QoS profiles. You can configure:\nReliability (best effort vs guaranteed delivery) Durability (do new subscribers get old messages?) Latency budgets Liveliness detection (detecting dead publishers) This gives developers precise control over how nodes interact, especially in lossy or high-throughput networks.\nSecurity Built-In Security was an afterthought in ROS 1. In ROS 2, it’s a first-class citizen. It includes:\nEncryption of messages in transit Authentication of nodes Access control policies to restrict who can publish/subscribe Security is crucial for commercial robotics, especially in fields like healthcare, defense, and autonomous transport.\nImproved Node Lifecycle Management ROS 2 introduces managed lifecycles for nodes. A node can be explicitly initialized, activated, deactivated, and shut down. This allows:\nCleaner startup and shutdown sequences Better error handling Easier introspection and supervision This is particularly useful in systems that require staged initialization or watchdog monitoring.\nCross-Platform \u0026amp; Embedded Ready ROS 2 is designed to work across platforms:\nNative support for Linux, Windows, and macOS (though Windows and macOS are not considered production-grade for most robotics deployments) Builds on ARM and embedded Linux devices Supports cross-compilation for real-time microcontrollers This means you can build a system with a Jetson for perception, a Raspberry Pi for control, and a desktop for planning—ROS 2 will tie it all together.\nROS 1 Doesn’t Stop Overnight The end of official support doesn’t mean that ROS 1 systems will suddenly break. Existing applications and research projects built on ROS 1 will continue to run. But the risks will grow over time:\nNo new bug fixes or security patches Incompatibility with newer operating systems and compilers Reduced community activity and package maintenance No support for evolving hardware or standards For small-scale projects, this may not matter. But for production environments, long-term maintenance, and scalable deployments—ROS 2 is the way forward.\nFinal Thoughts ROS 2 isn’t just a newer version of ROS—it’s a complete rethink, engineered for where robotics is headed. Real-time control, secure communication, decentralized systems, and embedded readiness are no longer optional—they’re essential.\nYes, migrating to ROS 2 can be a non-trivial effort. APIs are different. Not all ROS 1 packages have 1-to-1 equivalents. But the foundational improvements are worth it—and necessary.\nIf you\u0026rsquo;re starting a new robotics project in 2025, choosing ROS 1 now would be like choosing Python 2 for a new web app. It might work, but you’re building on borrowed time.\nSo—what changed? Everything. And that’s why it matters.\n","date":"May 2, 2025","image":"/images/post/ros/image_hu14324171076089879362.png","permalink":"/blog/ros1-vs-ros2/","title":"ROS 2 vs ROS 1: What Changed and Why It Matters?"},{"blog":["all","s","robotics","algorithms","localization"],"contents":"In robotics, SLAM—Simultaneous Localization and Mapping—is regarded as one of the most fundamental and complex problems. At its core, SLAM addresses a deceptively simple question: \u0026ldquo;Where am I, and what does the world around me look like?\u0026rdquo;\nFor a robot to navigate autonomously, it must construct a map of an unknown environment while simultaneously localizing itself within that map. This dual-estimation problem underpins most mobile robotics systems, making SLAM essentially the cognitive core or brain that enables autonomy.\nThe Formal Problem Definition The SLAM problem can be formally expressed in probabilistic terms. Let:\nxₜ denote the robot’s state (pose) at time t. m represent the map of the environment (which may be feature-based, occupancy grid, or landmark-based). uₜ be the control input (e.g., wheel velocity commands). zₜ be the observation or sensor measurement at time t. The goal of SLAM is to compute the posterior distribution:\n$$ P(x_{1:t}, m \\mid z_{1:t}, u_{1:t}) $$\nThat is, we want to estimate the trajectory of the robot x₁:t and the map m, given all sensor observations z₁:t and control inputs u₁:t up to time t. This joint estimation makes SLAM non-trivial, since errors in one (pose or map) propagate into the other.\nA Bayesian Perspective The SLAM problem is inherently a Bayesian inference problem. The recursive Bayes filter provides the framework:\n$$ P(x_t, m \\mid z_{1:t}, u_{1:t}) = \\eta \\cdot P(z_t \\mid x_t, m) \\cdot \\int P(x_t \\mid x_{t-1}, u_t) P(x_{t-1}, m \\mid z_{1:t-1}, u_{1:t-1}) dx_{t-1} $$\nWhere:\n\\(\\eta\\) is a normalizing constant. \\(P(z_t \\mid x_t, m)\\) is the observation model, also known as the likelihood. \\(P(x_t \\mid x_{t-1}, u_t)\\) is the motion model, describing the robot\u0026rsquo;s kinematics. This recursive structure is at the heart of many SLAM algorithms and leads to various implementations like Extended Kalman Filter SLAM, Particle Filter SLAM (FastSLAM), and Graph-Based SLAM.\nState Estimation and Mapping In SLAM, there are two intertwined estimation problems:\nLocalization: Estimating the robot’s pose \\(x_t\\) given a map. Mapping: Estimating the map \\(m\\) given the pose history. These are not independent—if the robot’s pose is uncertain, the resulting map will be too. This mutual dependency necessitates a joint estimation.\nMotion Model Assuming a differential drive robot, the motion model is often modeled as:\n$$ x_t = f(x_{t-1}, u_t) + w_t $$\nWhere:\n\\(f\\) is the motion function (e.g., based on odometry). \\(w_t\\) is process noise, often modeled as zero-mean Gaussian noise with covariance \\(Q_t\\). Observation Model Sensor measurements (from LIDAR, sonar, or cameras) depend on the robot\u0026rsquo;s pose and the environment:\n$$ z_t = h(x_t, m) + v_t $$\nWhere:\n\\(h\\) is the observation function. \\(v_t\\) is observation noise, modeled as Gaussian with covariance \\(R_t\\). Variants and Algorithms Depending on how one approximates the posterior, several algorithmic variants of SLAM arise:\nExtended Kalman Filter SLAM (EKF-SLAM) EKF-SLAM linearizes the motion and observation models around the current estimate and tracks the mean and covariance of the joint state \\((x_t, m)\\). It scales poorly with the number of landmarks due to quadratic complexity in the state size.\nParticle Filter SLAM (FastSLAM) FastSLAM uses a Rao-Blackwellized particle filter, factorizing the SLAM posterior:\n$$ P(x_{1:t}, m \\mid z_{1:t}, u_{1:t}) = P(m \\mid x_{1:t}, z_{1:t}) \\cdot P(x_{1:t} \\mid z_{1:t}, u_{1:t}) $$\nParticles represent different possible robot trajectories, and each particle maintains its own map, allowing for greater scalability and handling of non-linear, non-Gaussian systems.\nGraph-Based SLAM Graph SLAM formulates SLAM as a nonlinear optimization problem. Nodes in the graph represent robot poses and landmarks, while edges represent spatial constraints derived from motion and sensor data. The optimization seeks to find the configuration that minimizes the total error (e.g., via least-squares):\n$$ x^* = \\arg \\min_x \\sum_{i,j} | z_{ij} - h(x_i, x_j) |^2_{\\Omega_{ij}} $$\nWhere \\(\\Omega_{ij}\\) is the information matrix (inverse of the covariance).\nGraph SLAM is currently one of the most popular SLAM formulations, especially in systems like g2o and Ceres Solver, due to its flexibility and ability to incorporate loop closures and global constraints.\nThe Role of SLAM in Mobile Robots In the broader system architecture of a mobile robot, SLAM functions as the spatial intelligence unit. All downstream tasks—path planning, obstacle avoidance, exploration, and task execution—rely on accurate state and environment estimates.\nSLAM enables:\nReal-time tracking of the robot\u0026rsquo;s pose. Dynamic updates to the map as new observations are made. Consistent correction of errors via loop closure detection. Integration with higher-level cognition systems like planning and decision-making. Without SLAM, a mobile robot is effectively blind and disoriented—capable of sensing but not understanding its environment.\nSLAM and Lie Groups For robots operating in 2D or 3D, pose estimation involves dealing with SE(2) or SE(3) Lie groups. A pose is not just a vector but a transformation matrix involving rotation and translation:\n$$ T = \\begin{bmatrix} R \u0026amp; t \\ 0 \u0026amp; 1 \\end{bmatrix} \\in SE(3) $$\nWhere:\n\\(R \\in SO(3)\\) is a rotation matrix. \\(t \\in \\mathbb{R}^3\\) is a translation vector. Modern SLAM back-ends incorporate these group structures to ensure consistency in optimization, using tools like manifold-aware optimization and Lie algebra exponential maps.\nSLAM Beyond Geometry While SLAM traditionally focuses on geometric mapping, there is increasing interest in semantic SLAM—embedding meaning and object-level understanding into the map. This blends SLAM with computer vision and deep learning, creating a more human-interpretable world model.\nFinal Thoughts SLAM is not just a technique—it is the cognitive foundation of spatial intelligence in autonomous robots. It encapsulates estimation theory, probability, optimization, geometry, and control into a cohesive framework. As robotics systems grow more complex and dynamic, SLAM continues to evolve at the intersection of theory and real-world performance.\nWhether using Kalman filters, particle filters, or factor graphs, SLAM remains the mathematical brain that allows robots to think spatially—bridging perception with action.\n","date":"April 20, 2025","image":"/images/post/ros/slam_hu15444853054509355950.jpeg","permalink":"/blog/what-is-slam/","title":"What is SLAM? And Why It’s the Brain of Mobile Robots"},{"blog":["linux","operating-systems","all","s"],"contents":"You\u0026rsquo;re not really using your computer — you\u0026rsquo;re being allowed to. If the system decides how you work, who’s really in control? Linux doesn’t assume how you want to use your machine — it asks.\nWhat is an Operating System? An operating system (OS) is a low-level software layer responsible for managing hardware resources and providing services to user-level applications. It handles memory allocation, process scheduling, file systems, I/O operations, and device management.\nIf you\u0026rsquo;re using Windows or macOS, you\u0026rsquo;re already relying on an OS to abstract hardware complexity and provide user-facing utilities. These platforms are tightly integrated, prioritize consistency, and limit exposure to internal system mechanisms. For most users, this abstraction works. But it comes at the cost of reduced flexibility and limited access to internals.\nWhat Is Linux? Linux refers to the kernel, not the entire OS. It\u0026rsquo;s a monolithic kernel that manages system-level operations. When combined with userland tools (like GNU coreutils, shells, systemd, libraries), you get a Linux distribution.\nUnlike proprietary OSes, Linux is:\nModular – Components can be swapped or removed entirely. Transparent – Everything can be inspected or modified, from the bootloader to the shell. Open Source – Source code is available for nearly all components, from the kernel to window managers. You can control the boot process, recompile the kernel, trace syscalls, patch drivers, and monitor performance in real-time — all with native tools.\nWhy Use Linux? Full System Visibility and Control On Linux, logs are in /var/log or accessible via journalctl. Services are managed with systemd, and devices are exposed in /dev. Nothing is hidden behind a registry or proprietary interface.\nRich CLI and Scripting Ecosystem The command-line interface (CLI) is a first-class environment, not a fallback. Tools like grep, awk, sed, find, cut, and xargs make text processing and system automation efficient.\nPackage Management Software is installed via package managers (apt, dnf, pacman, etc.), with cryptographic signature verification and automatic dependency resolution.\nSecurity User and group-based permissions (UMASK, chmod, chown) Access Control Lists (ACLs) and capabilities Mandatory Access Control (MAC) via SELinux, AppArmor No admin by default – root privileges require sudo or root shell Performance and Efficiency Linux runs on everything from embedded devices with \u0026lt;64 MB RAM to multi-core servers. You can run headless servers, real-time systems, or full-fledged graphical environments — all tuned to your use case.\nUnderstanding Linux Distributions (Distros) A distribution is a collection of:\nThe Linux kernel Core userland tools (from GNU or others) Package manager Optional GUI (X11/Wayland + DE/WM) Configuration and init system (usually systemd, sometimes OpenRC or others) Some common distributions:\nUbuntu Beginner-friendly, maintained by Canonical Uses apt package manager Default DE: GNOME Good hardware support, LTS releases available Debian Parent of Ubuntu, more conservative Very stable, large repositories Suitable for servers and advanced users Fedora Sponsored by Red Hat Cutting-edge packages, SELinux by default Uses dnf for package management Arch Linux Rolling release, minimal install User builds system from ground up pacman for package management Requires good understanding of Linux internals Others openSUSE (uses zypper) Alpine Linux (musl libc, great for containers) Void Linux (runit init system) Gentoo (source-based, maximum customization) How to Try Linux Live USB Download ISO from distro website (e.g., ubuntu.com, debian.org, archlinux.org) Use tools like Rufus, Etcher, or dd to flash the ISO to a USB stick Boot from USB, run the OS live without installing Virtual Machine Use VirtualBox, QEMU, or VMware Create a VM, allocate resources, and boot into the ISO Great for learning CLI tools and basic configuration Dual Boot Shrink existing OS partition Install Linux alongside Windows/macOS Choose OS on boot via GRUB Linux Command Line: Where to Start Navigation pwd # Print current directory cd /path # Change directory ls -l # List files with details File Operations touch file.txt # Create file mkdir dir # Make directory cp src dest # Copy file mv old new # Move/rename file rm -rf dir # Delete directory recursively Package Management (Debian/Ubuntu) sudo apt update # Update package list sudo apt upgrade # Upgrade all packages sudo apt install pkg # Install package sudo apt remove pkg # Remove package Processes and System ps aux # List all processes top / htop # System monitor kill PID # Terminate process df -h # Disk usage free -m # RAM usage uname -r # Kernel version Networking ip a # Show IP addresses ping 8.8.8.8 # Check connectivity netstat -tulnp # Show open ports Logs and Services journalctl -xe # View system logs systemctl status ssh # Check SSH service sudo systemctl restart ssh Linux Community and Ecosystem One of Linux’s strengths is its global, active developer and user community. Resources include:\nArch Wiki – Best technical documentation, not limited to Arch Stack Overflow / Unix StackExchange – Troubleshooting, config help Reddit – r/linux, r/linuxadmin, r/linux4noobs IRC / Matrix / Discord – Real-time help for specific distros Mailing Lists and Git Repos – Access to upstream development discussions Bug reports and patches are community-driven. Discussions happen in public. You can track issues, read commits, or contribute code.\nCommon Use Cases Development: Native GCC, Clang, Python, Rust, Go, Node, etc. Bash/Python scripting, Makefiles, CI/CD tools all integrate natively. Servers: SSH, Docker, NGINX, PostgreSQL, systemd timers, firewall rules Embedded: Custom kernel builds, cross-compilation, BusyBox, buildroot Networking: IPTables, WireGuard, TCPDump, Netplan, NetworkManager CLI Security: GPG, TPM, AppArmor, auditd, fail2ban Minimal Systems: CLI-only builds, tiling WMs, no unnecessary services Where Linux Falls Short Some commercial software (Adobe Suite, MS Office) isn’t natively supported. Hardware vendors may not provide official Linux drivers (especially Wi-Fi or GPUs). Gaming is improving, but anti-cheat and some titles still rely on Windows-only libraries. The learning curve is real if you\u0026rsquo;re new to the CLI or package management. Final Notes Linux gives you the system as-is. No unnecessary abstraction, no locked settings, no background processes phoning home. You\u0026rsquo;re responsible for maintaining your system — and in return, you gain deep insight and full control.\nThe trade-off is worth it if you care about reproducibility, transparency, and performance.\n","date":"April 15, 2025","image":"/images/post/linux/image_hu12539179471411500279.png","permalink":"/blog/switch-to-linux-and-thank-me-later/","title":"Switch to Linux and Thank Me Later"},{"blog":["hardware","communication-protocols","all","s"],"contents":"I²C is a synchronous, half-duplex, multi-master, multi-slave serial communication protocol developed by Philips (now NXP) in the 1980s. It was designed for on-board communication between integrated circuits, especially in systems with multiple low-speed peripherals controlled by a microcontroller.\nIt prioritizes simplicity and scalability over speed, making it ideal for communication over short distances with minimal wiring.\nFundamental Signal Lines I²C uses only two bi-directional lines:\nSDA (Serial Data Line) – Carries the data. SCL (Serial Clock Line) – Carries the clock signal generated by the master. Both lines are open-drain (or open-collector), meaning devices can pull the line low but cannot drive it high. Instead, external pull-up resistors are required to bring the lines to a logic HIGH when not pulled LOW. This ensures multiple devices can safely share the bus.\nCommon pull-up resistor values range from 2.2kΩ to 10kΩ, depending on bus speed and capacitance.\nBus Topology and Roles Multi-master: Any device capable of initiating communication can act as a master. Multi-slave: All devices on the bus have unique addresses, allowing shared access over just two wires. Every transaction is initiated by a master, and the addressed slave responds.\nThe bus supports hot-swapping—devices can be connected/disconnected dynamically as long as the electrical characteristics are observed.\nAddressing Scheme Each I²C device has a 7-bit or 10-bit address:\n7-bit addressing is standard (supports up to 128 devices) Some modern devices use 10-bit addressing (rare in practice) The master begins communication by sending a START condition, followed by the 7-bit address and a R/W bit:\n0: Write to slave 1: Read from slave The addressed slave responds with an ACK (acknowledge) bit.\nBasic Transaction Format A typical I²C communication consists of:\nSTART condition: SDA goes LOW while SCL is HIGH Address + R/W bit ACK/NACK from slave Data byte ACK/NACK [Optional] More data bytes and ACKs STOP condition: SDA goes HIGH while SCL is HIGH Clock stretching: Slaves can hold SCL LOW to delay communication if they\u0026rsquo;re not ready to respond, allowing for flow control.\nData Framing and Timing All data is sent MSB-first Each byte is 8 bits, followed by a 1-bit ACK/NACK Timing is defined by Standard-mode (100 kHz), Fast-mode (400 kHz), Fast-mode Plus (1 MHz), and High-Speed mode (3.4 MHz) Master controls the clock in all modes, but in clock stretching, slaves can pause the clock by holding SCL low.\nElectrical and Physical Characteristics Voltage levels: Typically 3.3V or 5V (must match or use level shifters) Bus capacitance: Must be below 400pF for reliable operation Open-drain lines: Essential for multi-master and shared-bus safety Wiring: Only two lines needed, which makes I²C excellent for PCB routing and connector reduction Advantages of I²C Minimal wiring: Just two wires for all devices Addressable devices: Built-in addressing supports multiple devices with no extra CS logic Standardized protocol: Robust and well-documented across microcontrollers and ICs Clock stretching: Enables slow slaves to hold off the master Multiple masters: Optional, with arbitration built-in ACK/NACK mechanism: Basic error detection Limitations Slower speeds compared to SPI: Maxes out at 3.4 MHz (HS mode), with most devices using 100–400 kHz Half-duplex: One direction at a time Protocol overhead: Extra bits for addresses and ACKs reduce net throughput Complexity in software: Due to start/stop conditions, arbitration, and retries Shared bus: A fault in one device (e.g., pulling SDA low) can hang the whole bus Less suitable for high-speed or large data transfers Multi-Master Arbitration I²C allows multiple masters to coexist. If two masters start communication simultaneously:\nArbitration is resolved by checking the SDA line during transmission The master that detects a mismatch (it sends 1 but sees 0) backs off The one that wins continues the transfer This makes the protocol robust in multi-master environments but slightly complex to implement in firmware.\nDebugging and Analysis I²C can be debugged using:\nLogic analyzers: With I²C protocol decoders to inspect address, R/W, data, and ACK bits\nOscilloscopes: To check waveform timing, pull-up voltage levels, and START/STOP conditions\nWatch out for:\nMissing ACKs (bad address, bad wiring) Stuck SDA/SCL (often due to a crashed device or improper pull-ups) Incorrect pull-up resistor values (signal degradation or failure to reach Vcc) Common Use Cases I²C is ideal for connecting moderately slow devices to a microcontroller where simplicity and pin efficiency are key:\nTemperature, humidity, pressure sensors RTCs (Real-Time Clocks) EEPROMs GPIO expanders OLED displays (lower resolution) Low-speed ADCs/DACs Power management ICs Microcontrollers often have dedicated I²C hardware modules, and many system-on-chips expose I²C buses for configuration or control.\nConclusion I²C is the go-to protocol for short-range, low-speed communication between chips, especially when pin count and simplicity are important. Its addressing, shared bus, and clock control features make it extremely efficient for connecting dozens of devices with just two wires. While it\u0026rsquo;s not designed for high-throughput or time-critical tasks, it excels in environments where convenience and integration matter more than speed.\n","date":"April 8, 2025","image":"/images/post/protocols/i2c_hu12879587869166235930.png","permalink":"/blog/i2c/","title":"I²C: Fundamentals and Practical Aspects of Inter-Integrated Circuit Communication"},{"blog":["hardware","communication-protocols","all","s"],"contents":"SPI is a synchronous serial communication protocol designed for high-speed, full-duplex data exchange between a master device and one or more peripheral (slave) devices. It was originally developed by Motorola and remains widely adopted in microcontrollers, sensors, memory chips, ADCs/DACs, displays, and more.\nUnlike UART, SPI uses a shared clock line, which allows for more precise timing, faster communication, and simpler hardware synchronization.\nFundamental Signal Lines A typical SPI bus consists of four essential lines:\nSCLK (Serial Clock) – Generated by the master. Synchronizes data transfer. MOSI (Master Out Slave In) – Data sent from master to slave. MISO (Master In Slave Out) – Data sent from slave to master. SS or CS (Slave Select or Chip Select) – Active-low line used to select individual slave devices. In a basic setup, the master controls the clock and selects which slave to talk to by asserting its dedicated CS line low. All data transmission is synchronized to the clock signal generated by the master.\nFull-Duplex Operation SPI is inherently full-duplex, meaning data can be transmitted and received simultaneously. Every clock pulse shifts one bit out on MOSI and one bit in on MISO.\nThis also means that every transmission is bi-directional, even if you only care about one direction (e.g., writing to a DAC—you still receive bits back, which are usually ignored).\nData Framing and Configuration Parameters SPI doesn’t have a fixed data frame like UART. Instead, the configuration is flexible, and both master and slave must be set to agree on several parameters:\nClock Polarity (CPOL): Determines the idle state of the clock.\nCPOL = 0: Clock idles low CPOL = 1: Clock idles high Clock Phase (CPHA): Defines on which clock edge data is sampled.\nCPHA = 0: Data sampled on the first clock edge CPHA = 1: Data sampled on the second clock edge These two parameters define four SPI modes (Mode 0 to Mode 3). Master and slave must use the same mode:\nMode CPOL CPHA Clock Idle Sample Edge 0 0 0 Low Rising edge 1 0 1 Low Falling edge 2 1 0 High Falling edge 3 1 1 High Rising edge Also configurable:\nBit Order: MSB-first (default) or LSB-first Word Size: Usually 8 bits, but 16-bit, 32-bit, or arbitrary-length transfers are possible Multi-Slave Topology SPI supports multiple slaves, but with limitations:\nIndependent CS lines: Each slave must have a dedicated chip-select line from the master. Only one CS line is asserted low at a time. Shared MISO/MOSI/SCLK: All other lines can be shared between devices. Alternatives to reduce pin count:\nSPI daisy-chaining: Used in some chips (e.g., shift registers) where MISO of one device connects to MOSI of the next. Multiplexers or SPI expanders: Manage CS lines via GPIO expanders. This makes SPI not truly multi-master or multi-drop—it\u0026rsquo;s a master-driven protocol with limited scalability in terms of wiring.\nElectrical and Speed Characteristics No fixed electrical standard: Logic levels depend on devices (e.g., 3.3V, 5V). Always match or level-shift. No inherent error checking: Unlike UART or I2C, SPI has no parity, ACK/NACK, or CRC by default. Extremely fast: Speeds of 1–50+ Mbps are common. Some devices can go beyond 100 MHz. Maximum reliable speed depends on:\nBus capacitance (wiring length) PCB layout and impedance matching Slave device timing limitations (check datasheets) Advantages of SPI High speed: Ideal for large data throughput (e.g., sensors, memory, screens) Simple hardware: No addressing or arbitration logic Full-duplex: Allows simultaneous send/receive Flexible data word size: Can send arbitrary-length packets No protocol overhead: Efficient raw data transfer Limitations No standard device addressing: Requires separate CS per slave or custom protocol No error detection or correction: Needs to be implemented at the application layer Requires more wires: 4 signals minimum, plus extra CS lines for more slaves Single master only (in most practical implementations) Flow Control and Interrupts SPI typically does not use flow control. The master initiates all communication and must ensure that:\nThe slave is ready (by polling a READY pin or delay) Data is handled fast enough (especially for high-speed transfers) Slaves may offer:\nREADY/BUSY lines: Optional GPIO used by the slave to signal status DMA support: Offloads high-speed SPI to memory without CPU intervention Applications and Use Cases SPI is preferred when speed and efficiency are key. Typical applications include:\nFlash memory chips (e.g., NOR, NAND) SD cards (in SPI mode) LCD/OLED display modules High-speed ADCs and DACs Real-time sensors (gyroscopes, accelerometers) Audio CODECs (I2S is a variant of SPI) Many sensors come in both SPI and I2C versions, letting designers choose between higher performance (SPI) or fewer wires (I2C).\nDebugging SPI Debugging SPI can be trickier than UART because it’s clocked and often involves multiple devices. Use:\nLogic analyzers (Saleae, Logic Pro) to decode SPI frames\nOscilloscopes to confirm proper edge alignment (especially CPOL/CPHA)\nCheck for:\nMisconfigured SPI mode Incorrect bit order Bad CS handling (glitches, multiple CS active) Slave not responding (often a sign of CS or mode mismatch) Conclusion SPI is a powerful and versatile protocol, especially when speed, simplicity, and full-duplex operation are required. Though it doesn’t scale well in large multi-device systems and lacks built-in error checking, its deterministic timing, low protocol overhead, and wide support make it an essential tool in any embedded engineer’s arsenal.\nWhether you\u0026rsquo;re streaming data to a display, reading high-speed sensors, or managing memory chips, SPI provides a direct and efficient path for raw, fast communication.\n","date":"April 1, 2025","image":"/images/post/protocols/spi_hu14692535899283855295.png","permalink":"/blog/spi/","title":"SPI: Understanding the Serial Peripheral Interface Protocol"},{"blog":["hardware","communication-protocols","all","s"],"contents":"UART is one of the oldest and most fundamental methods of serial communication in embedded systems. As its name suggests, it operates asynchronously, meaning there is no shared clock signal between the transmitter and the receiver. This makes UART especially attractive in scenarios where simplicity and minimal wiring are important.\nFundamental Concept In a UART communication system, data is transmitted serially, bit by bit, over a single wire (TX), and received on another (RX). Each device in the communication link has its own internal clock, and both must agree on a common baud rate (bits per second). Because there’s no external clock signal, this baud rate agreement is critical—if either side drifts too much, communication breaks down.\nData Framing Structure Every UART frame consists of several components to delimit and validate the transmission:\nStart bit – Marks the beginning of a data frame. It\u0026rsquo;s always a logic 0 (low). Data bits – Typically 5 to 9 bits, though 8 bits is most common. Sent LSB-first. Optional parity bit – Used for rudimentary error checking (even or odd parity). Stop bit(s) – One or more bits of logic 1 (high) to signal the end of the frame. A typical 8-N-1 UART frame (8 data bits, no parity, 1 stop bit) looks like this:\nLine Idle: HIGH ↓ Start Bit (0) → [D0 D1 D2 D3 D4 D5 D6 D7] → Stop Bit (1) During idle periods (no transmission), the line remains at logic high.\nBaud Rate and Timing Constraints Since UART is asynchronous, both ends must know the exact baud rate—for example, 9600, 115200, or even up to a few Mbps depending on hardware. A mismatch in baud rate beyond ~2-3% can lead to framing errors.\nFor example, at 9600 baud:\nEach bit lasts ~104.17 microseconds. A full 10-bit frame (start + 8 data + stop) takes ~1.04 ms. UART receivers usually oversample the incoming data line (commonly at 16× the baud rate) to detect the start bit and sample data bits at the correct time.\nError Detection and Robustness UART includes minimal error handling:\nParity Bit (optional): Basic check for single-bit error detection. Framing Error: If the stop bit isn\u0026rsquo;t logic high at the expected time. Overrun Error: If the receiver buffer isn’t read fast enough and gets overwritten. Break Condition: If the line is held low for longer than a full frame—used to signal an intentional interruption. Due to the lack of built-in CRC or acknowledgment schemes, higher-layer protocols (like Modbus, NMEA, or custom ones) often wrap UART transmissions with additional error-checking logic.\nFlow Control Mechanisms When sending variable-sized or fast streams of data, there\u0026rsquo;s a risk of buffer overflow. To mitigate this:\nSoftware Flow Control: Special characters like XON (0x11) and XOFF (0x13) are sent to pause/resume transmission. Hardware Flow Control: Uses additional lines like RTS (Request to Send) and CTS (Clear to Send) for handshake. These features are not part of the UART standard itself but are often implemented by UART peripherals or bridges (e.g., USB-to-UART chips).\nElectrical Characteristics UART is purely a signaling protocol—it doesn’t define voltage levels. Typical implementations use:\nTTL logic levels: 0V (low), 5V or 3.3V (high) RS-232: ±12V for signaling (used in PCs, legacy serial ports) Make sure voltage levels match between devices or use level shifters to avoid damage.\nTopology and Limitations UART supports only point-to-point communication. You cannot have more than two devices on the same TX/RX lines without a multiplexer or external logic.\nThis makes UART unsuitable for multi-device networks unless augmented with switches, hubs, or protocol conversion (e.g., UART-to-I2C/SPI bridges or USB).\nBuffering and Interrupts Most modern microcontrollers include a UART peripheral with internal FIFO buffers (commonly 16, 32, or 64 bytes). This allows temporary storage of incoming/outgoing bytes to decouple hardware timing from software response.\nReception can be handled via:\nPolling – CPU actively checks status register Interrupts – ISR is triggered when data is received DMA (Direct Memory Access) – High-speed, CPU-independent transfer Applications and Use Cases UART is used everywhere, especially in scenarios where:\nOnly two devices need to communicate\nSimple, low-speed data exchange is sufficient\nDebugging via serial terminals (e.g., printf over UART)\nCommunication with modules like:\nGPS receivers GSM modems Bluetooth (e.g., HC-05) Serial LCDs or displays USB-to-serial bridges (e.g., FT232, CH340) Typical Baud Rates and Limitations Common Baud Rates Notes 9600 Very common, reliable 38400 Used in some industrial devices 115200 Common for debugging/logging 1 Mbps+ Achievable on high-performance MCUs The upper limit depends on:\nClock accuracy Signal integrity (cable length, crosstalk) Peripheral and driver capability Debugging UART Troubleshooting UART issues often involves:\nChecking baud rate configuration (both sides must match) Monitoring line using an oscilloscope or logic analyzer Using tools like PuTTY, minicom, or serial monitor in IDEs Verifying no framing or parity errors Ensuring buffers aren’t overflowing (especially on RX) Conclusion UART is simple, efficient, and nearly universal. Its asynchronous nature makes it easy to use with just two wires, but that simplicity comes at the cost of scalability and robustness. While not ideal for high-speed or multi-device communication, it remains indispensable for low-complexity, point-to-point data exchange.\nWhether you\u0026rsquo;re printing debug logs or interfacing with legacy peripherals, understanding UART deeply is foundational for any embedded system designer.\n","date":"March 27, 2025","image":"/images/post/protocols/uart_hu12192536807118004167.png","permalink":"/blog/uart/","title":"UART: A Detailed Overview of Asynchronous Serial Communication"},{"blog":["hardware","communication-protocols","all","s"],"contents":"In both VLSI design and embedded systems, no chip operates in isolation. Whether it\u0026rsquo;s a microcontroller interfacing with sensors, a processor communicating with memory modules, or multiple peripherals synchronizing data, inter-chip communication is fundamental to building reliable and scalable hardware systems.\nAs systems grow in complexity, so does the demand for structured, efficient, and purpose-fit communication protocols. Direct parallel connections are rarely feasible due to board space, pin limitations, and synchronization challenges. Instead, designers turn to well-established serial communication standards that enable devices to exchange data with minimal wiring and clear electrical and timing definitions.\nThree of the most widely used protocols for such tasks are:\nUART (Universal Asynchronous Receiver/Transmitter) SPI (Serial Peripheral Interface) I²C (Inter-Integrated Circuit) Each of these protocols is optimized for different trade-offs—speed, simplicity, scalability, and wiring complexity—making them suited for specific use cases in VLSI blocks, SoCs, microcontroller boards, and embedded devices.\nBrief Overview of the Protocols UART – Universal Asynchronous Receiver/Transmitter UART is a point-to-point, asynchronous serial protocol used to transmit data without needing a clock line. Instead, both devices agree on a predefined baud rate. Communication occurs over two lines: TX (transmit) and RX (receive).\nIt\u0026rsquo;s widely used for debugging, console communication, and bootloaders, especially when simplicity and human-readable interfaces (e.g., serial terminals) are priorities. Since it doesn\u0026rsquo;t support addressing or multiple devices, it\u0026rsquo;s typically used for direct, one-to-one communication.\nSPI – Serial Peripheral Interface SPI is a synchronous, full-duplex protocol that uses four wires in its standard configuration: MOSI, MISO, SCLK, and CS (Chip Select). Data is clocked simultaneously in both directions, allowing high-speed transfers.\nSPI is ideal when speed and performance matter—common in display drivers, flash memory, ADCs/DACs, and high-throughput sensors. It supports multiple slaves but requires a separate chip-select line for each, which can limit scalability.\nI²C – Inter-Integrated Circuit I²C is a synchronous, half-duplex protocol that operates on just two wires: SDA (data) and SCL (clock). It allows for multi-master and multi-slave communication, where each device has a unique address.\nI²C is favored for connecting multiple low-speed peripherals, such as sensors, EEPROMs, and configuration ICs, particularly when board space and wiring simplicity are critical. Its slower speeds and shared bus make it less ideal for high-volume data transfer but excellent for control applications.\nProtocol Comparison at a Glance Feature UART SPI I²C Communication Type Asynchronous, full-duplex Synchronous, full-duplex Synchronous, half-duplex Wires Required 2 (TX, RX) 4 (MOSI, MISO, SCLK, CS) 2 (SDA, SCL) Multi-Device Support No Yes (via multiple CS lines) Yes (via addressing) Speed Moderate (depends on baud rate) Very High (10s of MHz) Low to Moderate (up to ~3.4 MHz) Complexity Low Medium Medium-High (due to addressing and arbitration) Best For Debugging, serial logging High-speed sensors, memory Low-speed sensors, config ICs Flow Control Optional (hardware/software) Master controlled Clock stretching by slave Hardware Requirement UART peripheral SPI controller I²C controller with open-drain pins Which One Should You Use? The choice of protocol depends on system constraints:\nUse UART when you need simple, direct communication, especially for console output or single peripheral interfaces. Use SPI for high-speed communication with a small number of peripherals that demand fast response or large data transfer volumes. Use I²C when you need to connect multiple peripherals with minimal wiring and address-based access, especially in sensor arrays and configuration-heavy devices. In the sections that follow, we’ll explore each protocol in technical depth—from electrical characteristics and signal timing to addressing, framing, and typical use cases. The goal is to not just describe them, but to understand why each behaves the way it does, and how to integrate them correctly into real-world systems.\nStay tuned for the deep dives on UART, SPI, and I²C.\n","date":"March 24, 2025","image":"/images/post/protocols/image_hu14921424412665270831.png","permalink":"/blog/communication-between-chips/","title":"Demystifying UART, SPI, and I2C: Communication Between Chips"},{"blog":["PID","PID1","RubiksCubeSolver","all","s"],"contents":"Kociemba’s algorithm revolutionizes Rubik’s Cube solving by efficiently navigating the immense complexity of the cube’s state space using advanced mathematical tools from group theory and heuristic search. This two-phase method strikes a balance between tractability and optimality, making it a cornerstone of computational puzzle solving.\nThe Mathematical Setup: Group Theory and Cosets Recall that the Rubik’s Cube group (G) consists of all legal cube states under composition of moves. Kociemba’s method strategically divides (G) into cosets of a carefully chosen subgroup (H \\subseteq G):\n$$ G = \\bigcup_{g \\in G/H} gH $$\nHere, (H) is the subgroup of cube states where all edges and corners are oriented correctly and all middle slice edges are located within the middle layer. This subgroup is sometimes called the \u0026ldquo;half-turn metric group\u0026rdquo;, since allowed moves in (H) correspond to half-turns of certain faces.\nBy reducing the problem to first finding the coset representative (g) that brings the cube into (H) (Phase 1), and then solving within (H) (Phase 2), the algorithm exploits the quotient structure (G/H) to manage complexity.\nPhase 1: Reducing to the Subgroup (H) Phase 1’s goal is to transform any scrambled cube state (s \\in G) into a state (s\u0026rsquo; \\in H) . Formally, find a sequence of moves (m_1) such that:\n$$ s\u0026rsquo; = m_1 \\cdot s \\in H $$\nWhere (m_1) is a product of face turns (quarter or half turns).\nEdge orientation: Each edge can be in two states — oriented or flipped — giving a binary invariant. There are (2^{12} = 4096) possible edge orientation states. Corner orientation: Each corner can be oriented in 3 ways, so (3^8 = 6561) corner orientations. Middle slice edges: The position of the four middle-layer edges is constrained. All these constraints define (H) , which contains approximately (10^{10}) states.\nPhase 1’s problem reduces to finding a minimal-length (m_1) to reach (H) , which is a constrained subgroup problem in the Rubik’s cube group.\nPhase 2: Solving within (H) Once the cube is in (H) , Phase 2 searches for a move sequence (m_2) restricted to moves within (H) such that:\n$$ m_2 \\cdot s\u0026rsquo; = I $$\nwhere (I) is the solved state.\nSince moves in (H) preserve edge and corner orientation and slice position, the search space is drastically smaller than the full (G) .\nHeuristics and Pattern Databases: Guiding the Search Direct brute-force search over (G) or even (H) is impossible due to the astronomical number of states. To tackle this, Kociemba’s algorithm employs heuristic search algorithms — specifically, Iterative Deepening A* (IDA*) — powered by pattern databases (PDBs).\nWhat are Pattern Databases? PDBs are precomputed lookup tables storing the exact minimal number of moves required to solve specific cube features, such as edge orientation or corner permutation, ignoring other features.\nFor example, one PDB might store the minimal moves to solve edge orientation, regardless of corner orientation or permutation.\nSince these databases cover disjoint subsets of the cube’s state, their heuristic values can be combined by taking the maximum to ensure admissibility (never overestimating).\nThis maximum value guides IDA* to explore only states promising to be close to the solution, pruning vast regions of the search space.\nIDA* Search Algorithm IDA* is a memory-efficient variant of A* that performs depth-first searches with increasing cost thresholds.\nAt each iteration:\nThe search depth limit is set by the heuristic cost (f = g + h) , where:\n(g) = cost from start to current node (number of moves so far) (h) = heuristic estimate of moves to goal (from PDB) The algorithm explores all nodes with (f \\leq) threshold.\nIf no solution found, threshold increases, and search repeats.\nIDA*’s use in Kociemba’s algorithm ensures that:\nThe first solution found is guaranteed to be minimal within the move restrictions. Memory overhead remains manageable, even for large state spaces. Formalizing the Heuristic Function If we define heuristic functions:\n(h_{EO}(s)): minimal moves to solve edge orientation in state (s) (h_{CO}(s)): minimal moves to solve corner orientation (h_{EP}(s)): minimal moves to place edges in their correct slices (h_{CP}(s)): minimal moves to permute corners correctly Then the combined heuristic (h(s)) guiding the search is:\n$$ h(s) = \\max { h_{EO}(s), h_{CO}(s), h_{EP}(s), h_{CP}(s) } $$\nThis ensures the search never expands nodes that cannot lead to an optimal solution.\nSummary of the Mathematical Power Kociemba’s algorithm elegantly balances:\nAlgebraic structure: Using subgroup and coset decompositions to reduce complexity Heuristic efficiency: Pattern databases provide admissible heuristics guiding the search Search algorithm: IDA* ensures optimal paths under given constraints This synergy enables the solver to consistently find solutions averaging around 20 moves (half the move length of Thistlethwaite’s original algorithm) in milliseconds on modern computers — a feat that would be impossible without this deep mathematical foundation.\nOther Algorithms: A Brief Overview and Comparison While Kociemba’s algorithm is among the most widely used for computational Rubik’s Cube solving due to its efficiency and near-optimal solutions, it is part of a broader landscape of methods, each with its own mathematical basis, advantages, and limitations.\nThistlethwaite’s Algorithm Revisited As discussed earlier, Thistlethwaite’s algorithm partitions the cube group into a sequence of four nested subgroups:\n$$ G = G_0 \\supset G_1 \\supset G_2 \\supset G_3 \\supset G_4 = { I } $$\nBy solving the cube progressively through these subgroups, it guarantees a solution in at most 52 moves (in half-turn metric). While theoretically elegant, the multiple phases and larger intermediate state spaces make it computationally heavier compared to Kociemba’s two-phase approach.\nGod\u0026rsquo;s Algorithm God’s Algorithm is the theoretical ideal that always finds the shortest possible solution for any cube position. It relies on exhaustive search of the entire state space ((\\approx 4.3 \\times 10^{19}) states) using massive computational resources and precomputed tables, such as the famous Table of 20 moves or less (God’s Number = 20).\nWhile this guarantees the absolute shortest solution, it is impractical for general use due to storage and computation demands. Kociemba’s algorithm often approximates God’s Algorithm efficiently by using heuristics and subgroup constraints.\nCFOP (Fridrich Method) On the practical speedcubing side, CFOP (Cross, F2L, OLL, PLL) is a human method rather than a computer algorithm. It relies on heuristic algorithms and pattern recognition rather than exhaustive search or mathematical group theory. While CFOP solves the cube very fast for humans, its move counts tend to be longer and less optimized compared to computational methods like Kociemba’s.\nOther Computational Approaches Korf’s Algorithm: Utilizes IDA* with large pattern databases for optimal solutions but is computationally expensive. Macro-Operators and Pruning Tables: Many solvers employ precomputed tables for specific cube configurations or use machine learning to predict move sequences. Genetic Algorithms and AI: Recent work explores reinforcement learning and evolutionary strategies to solve cubes without explicit group theory, focusing on policy learning and move prediction. Comparison Summary Algorithm Move Optimality Computational Complexity Use Case God’s Algorithm Guaranteed minimal Very high Theoretical, research Kociemba’s Algorithm Near-optimal (~20 moves) Moderate Fast, practical solvers Thistlethwaite’s Moderate (~52 moves max) Higher Theoretical, educational CFOP Longer (~50+ moves) Low Human speedcubing Korf’s Algorithm Optimal Very high Small subsets or specific puzzles Final Thoughts The mathematical sophistication of Rubik’s Cube algorithms reveals how computers transform the art of puzzle-solving. From the elegant subgroup decompositions of Thistlethwaite and Kociemba to heuristic-guided searches, computers convert what once was a purely human trial-and-error activity into a rigorous, near-optimal science.\nUnderstanding these algorithms highlights the power of:\nAlgebraic abstractions (groups, cosets) to simplify complex states Heuristic functions to efficiently guide searches Iterative search algorithms that balance time and space constraints In the next post, we will delve deeper into practical implementations of Kociemba’s algorithm and how modern solvers leverage these concepts to provide instant solutions.\n","date":"March 18, 2025","image":"/images/post/comprub/kociemba_hu2630830792071634680.png","permalink":"/blog/kociembas-alg-the-2-phase-breakthrough/","title":"Kociemba’s Algorithm – The Two-Phase Breakthrough #PID1.5"},{"blog":["PID","PID1","RubiksCubeSolver","all","s"],"contents":"Throughout history, puzzles have intrigued the human mind, not merely for entertainment but for the challenge they pose to logic, creativity, and persistence. From ancient labyrinths to Sudoku and the Rubik’s Cube, solving a puzzle often feels like an art — but beneath that art lies a surprising amount of structure. And where there is structure, computers can often outperform intuition.\nAt their core, puzzles are problems with constraints, and solving them requires a systematic approach to navigating possibilities. This is precisely the realm where computers shine. Unlike humans, computers are not limited by short-term memory, fatigue, or bias. They can execute algorithms tirelessly, evaluate vast search spaces, and spot patterns at a scale impossible for the human brain. As a result, computers don’t just mimic human puzzle-solving — they fundamentally transform it.\nWhy the Rubik’s Cube? Among all puzzles, the Rubik’s Cube has emerged as a particularly fascinating object of study in computer science, mathematics, and algorithm design. Invented in 1974 by Ernő Rubik, it has over 43 quintillion unique configurations (43,252,003,274,489,856,000 to be exact). That’s more than the number of grains of sand on Earth or seconds since the Big Bang. And yet, with the right approach, any configuration can be solved in 20 moves or fewer — a result known as God’s Number.\nBut how does one go from a jumbled cube to the solution in such a short number of moves, especially when the total number of configurations is so massive?\nThis is where the mathematical beauty of the cube comes into play.\nThe Mathematics Behind the Cube To a computer scientist or a mathematician, the Rubik’s Cube is more than just a toy — it\u0026rsquo;s a group. In group theory (a branch of abstract algebra), we define a group as a set with an operation that satisfies certain axioms: closure, associativity, identity, and inverses. Each move you make on a Rubik’s Cube — whether it’s a quarter-turn of a face or a full rotation — corresponds to an operation in this group.\nThe entire set of all possible cube states forms a permutation group. More specifically, it\u0026rsquo;s a subgroup of the symmetric group on the cube’s stickers, where each legal operation is a permutation of the cube’s pieces. The solved state is the group’s identity element, and solving the cube is equivalent to finding the inverse sequence of moves that returns any configuration to this identity.\nFrom a computational perspective, this means that solving the cube becomes a path-finding problem in a highly structured space. The “nodes” are the cube states, and the “edges” are the moves that transform one state into another.\nBut this graph of states is unimaginably vast — far too large for brute force to be practical. Even if a computer could check a billion configurations per second, it would still take centuries to exhaustively search all 43 quintillion. Clearly, we need something smarter.\nWhy Brute Force Fails — and Algorithms Succeed Imagine trying to solve the cube purely by random moves — it would be virtually impossible to land on the solved state, even in a lifetime. Even a depth-limited brute-force search, where the computer tries all sequences up to, say, 20 moves, quickly becomes intractable. At 18 legal face turns per move and 20 moves deep, that’s 18²⁰ ≈ 10²⁵ possibilities — still far beyond what any computer can handle.\nThat’s why we need algorithms. Algorithms introduce structure, allowing us to prune the search space intelligently. They leverage mathematical symmetries, identify key cube properties (like orientation, permutation, and parity), and break the problem into smaller, tractable sub-problems.\nRather than treating all configurations as equal, a good algorithm guides the computer through the space more like a mountaineer scaling peaks via well-worn trails rather than hacking blindly through a jungle.\nWhat Computers Bring to Puzzle Solving So why are computers particularly well-suited to this domain?\nSpeed: Computers can simulate millions of cube manipulations per second. Memory: They can store large lookup tables — precomputed solutions to subproblems. Exhaustiveness: They don’t get bored or distracted; they follow through every branch of a search tree. Precision: No errors, no forgetfulness. Every move is logical, every decision traceable. These strengths allow computers not only to solve the Rubik’s Cube but to solve it optimally — finding the shortest or most efficient solution using algorithmic planning.\nFrom Human Intuition to Mathematical Algorithms Before computers entered the scene, cube-solving was based largely on heuristics — trial-and-error, memorized sequences, and intuition. Speedcubers developed methods that worked well in practice but didn’t guarantee minimal solutions. It was the introduction of mathematical algorithms that changed the landscape.\nThe first major breakthrough came in the form of Thistlethwaite’s algorithm in the 1980s. It introduced the idea of reducing the cube\u0026rsquo;s complexity gradually by defining nested subgroups. From there, even more optimized approaches like Kociemba’s algorithm emerged, leveraging symmetries and lookup tables to reduce average solutions to around 20 moves.\nEach of these algorithms doesn’t just find a solution — they exploit the cube’s algebraic structure to find efficient, systematic paths through its vast configuration space.\nThistlethwaite’s Algorithm – From Chaos to Structure By the early 1980s, the Rubik’s Cube had become a global phenomenon — and a challenge that captivated not only hobbyists but also mathematicians. One such mind was Morwen Thistlethwaite, a mathematician who, in 1981, proposed one of the first major algorithmic breakthroughs in cube solving. His approach laid the foundation for many of the advanced solvers used today — not by brute force, but by applying the elegant machinery of group theory.\nAt its core, Thistlethwaite’s method turns the Rubik’s Cube into a layered mathematical structure — reducing the problem not in one step, but through a sequence of increasingly constrained subproblems. Each stage progressively shrinks the space of possible cube states, leveraging nested subgroups to transform an otherwise intractable problem into one that can be solved efficiently.\nModeling the Cube as a Group To understand Thistlethwaite’s insight, we first need to recognize how the cube operates in algebraic terms.\nEvery legal move on a Rubik’s Cube can be considered a group generator — a function that permutes the pieces of the cube. The entire collection of these permutations forms a group G under function composition. This group is known as the Rubik\u0026rsquo;s Cube group, and it contains all 43 quintillion possible cube states.\nMathematically, a group (G) can be described by a presentation — a set of generators (moves) and relations (how these moves interact). In the cube\u0026rsquo;s case, common generators might include:\n(U): rotate the upper face 90° clockwise (R): rotate the right face 90° clockwise (F): front face, and so on From just these generators and their inverses, all cube states can be reached.\nThe Key Insight: Subgroup Reduction Thistlethwaite’s key insight was to partition the problem of solving the cube into a sequence of four nested subgroups:\n$$ G_0 \\supset G_1 \\supset G_2 \\supset G_3 \\supset G_4 = { I } $$\nWhere:\n(G_0): the full cube group (all states) (G_1): subgroup where edge orientations are correct (G_2): subgroup where corners are also oriented (G_3): subgroup where all pieces are in correct slice layers (G_4): the identity group — the solved state Each subgroup (G_{i+1}) is a proper subgroup of (G_i), meaning it contains fewer states, but is still closed under certain restricted moves.\nFor example:\nTo move from (G_0)to (G_1), we only use moves that don\u0026rsquo;t disturb solved edge orientations. From (G_1) to (G_2), we constrain the move set further to preserve both edge and corner orientations. By progressing through these subgroups, the algorithm ensures that with each phase, the cube becomes increasingly constrained — gradually forcing it into a state where the solution becomes trivial.\nPhase-by-Phase Breakdown Let’s briefly outline the four phases of Thistlethwaite’s algorithm:\nPhase 1: Reduction to (G_1) Objective: Correct the orientation of all 12 edges.\nMove set allowed: all face turns Edge orientation is a binary invariant (flipped or not) The space of states reduces from ~(4.3 \\times 10^{19}) to about (10^9) Phase 2: Reduction to (G_2) Objective: Correct the orientation of corners and place all edges in their correct slice layers (middle or outer).\nMove set restricted to half-turns on some faces (e.g., only U, D, R2, L2, F2, B2) Corner orientation: 3 values per corner (0°, 120°, 240°) State space drops to ~(10^7) Phase 3: Reduction to (G_3) Objective: Place all pieces (edges and corners) in their correct orbits (positions relative to centers).\nOnly even permutations are allowed Reduction to ~(10^5) possible states Phase 4: Solve the cube (from (G_3) to identity) Objective: Use only the remaining allowed moves to reach the solved state.\nAt each stage, Thistlethwaite’s method restricts the move set, guiding the cube closer to a structured state while simultaneously reducing the number of legal transformations — and therefore the size of the search space.\nWhy It Works: Mathematics Meets Efficiency This hierarchical decomposition is more than a clever trick. It relies on coset decomposition from group theory. If we think of the Rubik’s Cube group (G) as a forest of interconnected trees, Thistlethwaite’s method picks one “layer” of branches at a time, cutting off all but the ones that eventually lead to the root (solved state). This avoids wandering aimlessly through the forest and allows for guided, phase-wise convergence.\nAnother advantage is modularity. Since each phase has a much smaller state space, it becomes feasible to precompute lookup tables (called pruning tables) for each subgroup. These tables store the shortest number of moves needed to reach the next subgroup from any configuration in the current one — dramatically reducing computation time during solving.\nConclusion Although Thistlethwaite’s algorithm does not always find the absolute shortest solution (i.e., not always “God’s Algorithm”), it typically solves any scrambled cube in 45 to 52 moves — a remarkable feat considering the cube’s astronomical complexity.\nThistlethwaite’s algorithm laid the groundwork for efficient Rubik’s Cube solving by breaking down the problem into manageable phases — next, we’ll explore how Kociemba’s algorithm builds on this foundation to achieve even faster and shorter solutions.\n","date":"March 7, 2025","image":"/images/post/comprub/image_hu7467307557230332304.png","permalink":"/blog/computers-in-the-art-of-solving-puzzles/","title":"How Do Computers Come into the Art of Solving Puzzles? #PID1.4"},{"blog":["blogging","tools","all","s"],"contents":"Hugo is a fast, flexible, and open-source static site generator that allows you to build websites with ease. Originally popular for blogging, Hugo’s versatility makes it ideal for creating a wide range of sites — from personal portfolios and academic project showcases to documentation hubs and even e-commerce sites. Whether you’re building a professional portfolio, a research site to share your academic work, or a personal blog, Hugo has you covered.\nThis guide will take you through the entire process of building a website using Hugo, from installation to deployment, with practical tips to make your site look professional and unique.\nInstall Hugo macOS (using Homebrew) If you\u0026rsquo;re on macOS and have Homebrew installed, this is the easiest way to install Hugo:\nbrew install hugo Windows (using Chocolatey) For Windows, use the Chocolatey package manager:\nchoco install hugo -confirm Linux (Debian/Ubuntu) If you\u0026rsquo;re on Linux, use the following commands to install Hugo:\nsudo apt-get update sudo apt-get install hugo Alternatively, you can download a precompiled binary for your platform from the Hugo releases page and extract it manually.\nVerifying Installation Once installed, verify the installation by running:\nhugo version This will show the version of Hugo you have installed, confirming that it is ready to go.\nCreate a New Hugo Site After Hugo is installed, you can create a new site with a simple command. Open a terminal (or command prompt on Windows) and run the following:\nhugo new site \u0026lt;your-site-name\u0026gt; This creates a new directory (\u0026lt;your-site-name\u0026gt;) with the basic structure of a Hugo site. You’ll see directories like content/, layouts/, and themes/.\nChoose and Install a Theme Hugo uses themes to determine how your website looks. To browse available themes, head to the Hugo Themes website. There are hundreds of free and open-source themes to choose from.\nOnce you\u0026rsquo;ve chosen a theme, you can add it to your site by following these steps:\nUsing Git Submodule (Recommended) Inside your Hugo site directory, initialize a Git repository (if it isn’t already initialized):\ngit init Add the theme as a submodule:\ngit submodule add \u0026lt;theme-repository-url\u0026gt; themes/\u0026lt;theme-name\u0026gt; git submodule update --init --recursive Configure your site to use the new theme. Open config.toml (or config.yaml or config.json, depending on your configuration format) and set the theme:\ntheme = \u0026#34;\u0026lt;theme-name\u0026gt;\u0026#34; Alternatively, you can download the theme manually, but using Git submodules is more efficient for managing updates.\nUnderstanding Hugo’s Directory Structure Hugo uses a specific directory structure to organize your website’s content, assets, and configuration. Here\u0026rsquo;s a breakdown of the most important directories and files:\ncontent/: Where your content lives. This is where you will add markdown files for posts, pages, and other content types. themes/: Contains the themes you use in your site. Each theme will have a layouts/ directory, which contains the theme’s templates. static/: This directory holds static assets like images, CSS, JavaScript files, etc. Files here are copied directly to the root of the public/ directory when Hugo generates the site. layouts/: This folder is used for your custom templates. You can override theme templates or create your own templates for specific types of content. config.toml (or config.yaml, config.json): This is your site’s configuration file, where you set global parameters like the site’s title, base URL, language, theme, and more. Create and Organize Content Now it’s time to add content to your website. You can create content types like blog posts, pages, or custom content.\nCreating a New Page or Post To create a new page or post, run the following command:\nhugo new \u0026lt;content-type\u0026gt;/\u0026lt;page-name\u0026gt;.md For example, to create a blog post:\nhugo new posts/my-first-post.md This will create a markdown file in content/posts/my-first-post.md.\nMarkdown Syntax for Content In the generated .md file, you\u0026rsquo;ll see frontmatter and markdown content:\n--- title: \u0026#34;My First Post\u0026#34; date: 2025-04-02 draft: true --- # Welcome to my blog! This is a simple markdown file to demonstrate Hugo. The frontmatter (between the --- lines) contains metadata for your content. You can set fields like title, date, draft, and custom parameters like tags or author. The content section uses standard markdown syntax. You can write paragraphs, lists, headings, links, images, and much more. Publishing Content Once you’re ready to publish, you can set draft: false in the frontmatter and run hugo server to preview the site.\nStart the Development Server To see your site in action, you can run a local development server:\nhugo server By default, this will run a server on http://localhost:1313. As you modify content or templates, Hugo will automatically regenerate the site and refresh the page.\nBuilding the Site for Production When you\u0026rsquo;re satisfied with your site, you can build it for production. Run the following command:\nhugo This will generate the static website in the public/ directory. The public/ directory will contain all the HTML files, assets, and other content required to host your site.\nDeploying the Site Once your site is built, you can deploy it to a variety of hosting platforms. Hugo sites are static, so they can be deployed on platforms like:\nGitHub Pages: You can push the contents of the public/ directory to a GitHub repository and serve it using GitHub Pages. Netlify: A popular static site hosting platform. Just link your GitHub repository to Netlify, and it will automatically build and deploy your site. Vercel: Another static site hosting platform similar to Netlify. Your Own Server: If you have a hosting provider or VPS, you can upload the files in the public/ directory to your web server. Each hosting platform will have specific instructions for deploying Hugo sites, but most of them integrate easily with Git-based workflows.\nCustomizing Your Site Creating Custom Layouts To modify the look and feel of your site, you can create custom templates in the layouts/ directory. You can override default templates from the theme by placing your custom templates here.\nFor example, to modify the homepage layout, you can create layouts/index.html or layouts/_default/baseof.html to adjust the base layout structure.\nAdding Shortcodes Hugo supports shortcodes, which are snippets of reusable content. You can use shortcodes to easily insert dynamic elements like galleries, videos, or calls to action. Here’s an example of using a shortcode to embed a YouTube video:\nShortcodes can be defined in the layouts/shortcodes/ directory.\nAdditional Tips Taxonomies: You can organize content using taxonomies like categories or tags. Add this configuration in config.toml:\n[taxonomies] category = \u0026#34;categories\u0026#34; tag = \u0026#34;tags\u0026#34; Multilingual Sites: Hugo supports multilingual sites. You can define different content for different languages in the content/ folder, such as content/en/ and content/es/.\nHugo Modules: Hugo also supports modules, which allow you to manage external dependencies in your site, such as themes or libraries.\nGitHub Actions: You can automate your Hugo build and deployment process using GitHub Actions for continuous deployment.\nConclusion Hugo is a powerful tool for building fast, static websites. With this guide, you should now be able to create, customize, and deploy your own Hugo-powered site. Whether you\u0026rsquo;re building a blog, portfolio, or documentation site, Hugo\u0026rsquo;s flexibility and speed make it a fantastic choice for modern static websites.\nHappy building!\nQuick Note: The site you’re reading this from is also built using Hugo — but with a ton of tweaks to make it uniquely mine! I started with the Geeky-Hugo theme as the base and added a bunch of customizations, including:\nCustom Layouts: Modified to fit my style and content structure.\nShortcodes: Added some handy ones for embedding interactive elements.\nCustom CSS: To give it a personal touch and make it look just right.\nLaTeX Support: For displaying complex mathematical equations seamlessly.\nExtended Pages: Not just limited to blogs — I’ve got project showcases, technical documentation, and more.\nIt’s proof that Hugo isn’t just about simple blogs — with some effort, you can turn it into a full-fledged portfolio or academic site!\n","date":"March 1, 2025","image":"/images/post/hugo/image_hu10657592199608426879.png","permalink":"/blog/getting-started-with-hugo/","title":"Getting Started with Hugo: A Step-by-Step Guide"},{"blog":["PID","PID1","RubiksCubeSolver","all","s"],"contents":"The Rubik’s Cube is not just a puzzle; it’s a deep mathematical object grounded in group theory, combinatorics, and geometry. Understanding the math behind it allows us to grasp why it has 43 quintillion possible states, how we categorize moves, and why some solutions are more efficient than others.\nGroup Theory and the Rubik’s Cube Group theory is a branch of mathematics that studies sets with operations that follow specific rules. The Rubik’s Cube can be seen as a mathematical group where:\nEach state of the cube is an element of the group. Each valid move (rotating a face) is a group operation. The identity element is the solved state of the cube. Moves have inverses (e.g., turning the right face clockwise can be undone by turning it counterclockwise). The Rubik’s Cube belongs to a finite group since it has a limited number of positions. The set of all possible cube configurations, with the operation of applying a sequence of moves, forms a non-abelian group (meaning that order matters—doing move A then B is not the same as doing move B then A).\nHere’s the updated version incorporating center orientation in the usual order for 3×3 scales:\nOrder of an Element in the Rubik’s Cube Group In group theory, the order of an element is the number of times it must be applied to return to the identity (solved state). In the Rubik’s Cube, certain moves or sequences have different orders:\nA single quarter-turn of a face has order 4 (doing it four times returns the cube to the original state). A 180-degree turn has order 2. Certain complex sequences have higher orders, meaning they take more repetitions to cycle back to the starting position. On a standard 3×3 Rubik’s Cube, center pieces do not change position, but their orientation can matter in some cases, such as in supercube variants where sticker orientation is tracked. In such cases, center rotations may introduce elements of order 2 or 4, depending on the move sequence. Understanding the order of moves, including center orientation, helps in designing efficient solving algorithms.\nCounting the 43 Quintillion Permutations To compute the number of possible Rubik’s Cube states, we analyze the degrees of freedom:\nThere are 8 corner pieces, each of which can be arranged in (8!) ways. Each corner has three orientations, giving (3^7) possibilities (the last one is determined by the others). There are 12 edge pieces, which can be arranged in (12!) ways. Each edge has two orientations, giving (2^{11}) possibilities (the last one is determined by the others). However, only even permutations of corners and edges are possible, so we divide by 2. Thus, the total number of possible Rubik’s Cube states is:\n$$ \\frac{8! \\times 3^7 \\times 12! \\times 2^{11}}{2} = 43,252,003,274,489,856,000 $$\nwhich is approximately 43 quintillion.\nFor the 2×2×2 Rubik’s Cube, we use a similar method but without considering edges:\nThe 8 corner pieces can be arranged in (8!) ways. Each has 3 orientations, giving (3^7) (since the last one is determined). Only even permutations are possible, so we divide by 2. Thus, the number of possible 2×2×2 states is:\n$$ \\frac{8! \\times 3^7}{2} = 3,674,160 $$\nwhich is significantly smaller than the 3×3×3 but still quite large.\nGod’s Number and Move Metrics God’s Number is the maximum number of moves required to solve the worst-case scenario of a Rubik’s Cube optimally. In 2010, researchers proved that God’s Number for a standard 3×3×3 Rubik’s Cube is 20 moves in the quarter-turn metric (where each 90-degree face turn counts as one move).\nMove Metrics Quarter-Turn Metric (QTM): Every 90-degree turn is counted as one move. This is the standard used in the 20-move God’s Number proof. Half-Turn Metric (HTM): Both 90-degree and 180-degree turns count as one move. In this metric, God’s Number is 18 moves. Face-Turn Metric (FTM): Any rotation of a face, whether 90, 180, or 270 degrees, is counted as one move. Different solving methods optimize for different metrics. For example, speedcubers prioritize fewer moves in practice rather than the theoretical minimum number of moves.\nEuclidean and Quaternion Mathematics in the Rubik’s Cube Euclidean Geometry and the Rubik’s Cube The Rubik’s Cube exists in three-dimensional Euclidean space, meaning its transformations can be represented using classical geometric tools such as matrices and vector operations.\nRotation Matrices: Each face rotation can be described using a 3×3 rotation matrix. A 90-degree clockwise rotation about the x, y, or z-axis can be represented as:\n$$ R_x(90^\\circ) = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; -1 \\ 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix}, $$\n$$ R_y(90^\\circ) = \\begin{bmatrix} 0 \u0026amp; 0 \u0026amp; 1 \\ 0 \u0026amp; 1 \u0026amp; 0 \\ -1 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix}, $$\n$$ R_z(90^\\circ) = \\begin{bmatrix} 0 \u0026amp; -1 \u0026amp; 0 \\ 1 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix}. $$\nVector Representation: Each cubie (small cube piece) has a position vector ( v ), and applying a rotation matrix transforms it to a new position: $$ v\u0026rsquo; = R v. $$ Using these transformations, all possible moves on the cube can be described mathematically.\nQuaternion Representation and the Rubik’s Cube Quaternions offer an alternative way to describe rotations in 3D space. A quaternion is defined as: $$ q = a + bi + cj + dk, $$ where ( a, b, c, d ) are real numbers, and ( i, j, k ) are imaginary unit vectors satisfying specific multiplication rules.\nRotation Using Quaternions: Any 3D rotation can be represented as: $$ q = \\cos\\left(\\frac{\\theta}{2}\\right) + \\sin\\left(\\frac{\\theta}{2}\\right)(xi + yj + zk), $$ where ( \\theta ) is the rotation angle, and ( (x, y, z) ) is the rotation axis.\nApplying a Rotation: Given a point represented by a quaternion ( p ), the rotated point ( p\u0026rsquo; ) is obtained as: $$ p\u0026rsquo; = q p q^{-1}. $$\nUsing quaternions avoids issues like gimbal lock and allows smooth, efficient calculations, making them useful in robotic cube solvers and computer simulations.\nComparison of Euclidean and Quaternion Methods Method Advantages Use Case in Rubik’s Cube Rotation Matrices Simple, easy to compute Manual cube manipulation, algebraic solving Quaternions No gimbal lock, computationally efficient Robotics, computer simulations While human solvers primarily use group-theoretic approaches, understanding Euclidean and quaternion mathematics is valuable for computational methods and AI-driven solutions.\nAdvanced Mathematics Behind the Rubik’s Cube Graph Theory and the Rubik’s Cube The entire state space of the Rubik’s Cube can be visualized as a graph, where:\nEach node represents a unique cube configuration. Each edge represents a valid move between two configurations. This allows us to analyze cube solving as a shortest path problem (like in Dijkstra’s algorithm). The challenge is that this graph is huge, containing about 43 quintillion nodes! Researchers have used Breadth-First Search (BFS) to explore how quickly the cube can be solved from any state, leading to the proof of God’s Number (20 in QTM).\nMarkov Chains and Random Scrambles If you randomly twist a Rubik’s Cube, how many moves does it take before it is \u0026ldquo;fully scrambled\u0026rdquo;? This is a classic Markov Chain problem, where each move represents a random transition between states. Studies suggest that after about 19-20 random moves, the cube is statistically close to a uniformly random state. This insight is used in competitive cubing to ensure fairness in official scramble generation.\nGroup Structure: Conjugacy Classes and Commutators The Rubik’s Cube group has special elements called commutators and conjugates, which are fundamental to advanced solving techniques:\nCommutator: ([A, B] = A B A^{-1} B^{-1}) – used in many algorithms to isolate cube pieces. Conjugate: (X A X^{-1}) – applies a transformation in a different context. These concepts allow cube solvers to move a small set of pieces without disrupting the rest, forming the basis for algorithms like CFOP, Roux, and ZZ methods.\nWhy Is Solving the Cube Hard? Computational Complexity Solving an arbitrary cube position optimally (in the least moves) is an NP-hard problem. That means there is no known efficient algorithm that can solve every case optimally in polynomial time. This is why human solvers use heuristic-based approaches like CFOP, Petrus, and Roux, rather than brute force computation.\nWhat’s Next? Computers and Efficient Cube Solving In our next post, we will explore how computers approach solving the Rubik’s Cube, including AI techniques, heuristics, and optimal solvers like Kociemba’s Algorithm and DeepCubeA.\n","date":"February 25, 2025","image":"/images/post/mathrub/image_hu11510849546453357576.png","permalink":"/blog/mathematics-behind-rubiks-cube/","title":"The Mathematics Behind the Rubik's Cube #PID1.3"},{"blog":["hardware","tools","all","s"],"contents":"Google Colab is a cloud-based platform that allows you to run code in a Jupyter Notebook environment. While it\u0026rsquo;s primarily designed for Python, it can also be adapted to run Verilog simulations using Icarus Verilog. This guide walks you through setting up Icarus Verilog on Colab, writing and compiling Verilog code, running simulations, and generating waveform files for debugging—all in the cloud.\nYou might wonder why we’d even consider simulating Verilog code on Colab when there are many industry-grade tools available that offer synthesis, timing analysis, and complete hardware design capabilities. The answer lies in accessibility. Icarus Verilog is an open-source, lightweight alternative that remains incredibly relevant—especially for students, educators, and hobbyists. It’s perfect for academic projects, quick prototyping, and learning digital design fundamentals without the overhead of licensed tools or heavy installations.\nOne major advantage of using Colab is its seamless integration with the web—allowing you to import datasets or files directly from URLs or cloud storage. This becomes particularly useful for projects where Verilog testbenches need structured data, such as input vectors, weights, or test cases. With Python handling the preprocessing and formatting, you can easily generate files that your Verilog code can read, enabling a smooth and flexible software-hardware co-design workflow.\nMy own journey into this setup began with a somewhat unconventional idea: training a neural network in Verilog. It was a fun and technically challenging experiment that led me to build this workflow on Colab. If you\u0026rsquo;re curious to see how that turned out, feel free to check out my project here.\nWhy Use Icarus Verilog on Google Colab? Icarus Verilog is an open-source Verilog simulation and synthesis tool that supports a wide range of Verilog constructs. Running it on Google Colab offers several advantages:\nNo need to install software on your local machine. Easy collaboration and sharing through Colab notebooks. Accessible from any device with an internet connection. Free computational resources provided by Google. Now, let\u0026rsquo;s get started with setting up Icarus Verilog on Google Colab.\nInstalling Icarus Verilog Before you can run Verilog simulations, you need to install Icarus Verilog on your Colab environment. To do this, execute the following commands:\n!sudo apt-get update !sudo apt-get install -y iverilog Once installed, verify the installation by checking the version:\n!iverilog -v If the installation is successful, you will see the version information displayed in the output.\nWriting and Running a Simple Verilog Program To test if Icarus Verilog is working correctly, let\u0026rsquo;s write a simple Verilog program that prints a message. In Colab, you can use the %%writefile magic command to create and save Verilog files:\n%%writefile test.v module hello; initial begin $display(\u0026#34;Hello, Icarus Verilog on Colab!\u0026#34;); $finish; end endmodule Compiling and Running the Verilog Code Once the Verilog file is created, compile it using iverilog:\n!iverilog -o test.out test.v Now, run the compiled Verilog file using vvp:\n!vvp test.out If everything is working correctly, you should see the message \u0026ldquo;Hello, Icarus Verilog on Colab!\u0026rdquo; printed in the output.\nGenerating and Viewing Waveform Files In addition to printing messages, you can also generate waveform files to analyze signal behavior. This is particularly useful for debugging digital designs.\nTo generate a VCD (Value Change Dump) file, modify your Verilog code to include the necessary $dumpfile and $dumpvars commands:\n%%writefile wave.v module wave; initial begin $dumpfile(\u0026#34;wave.vcd\u0026#34;); $dumpvars(0, wave); $display(\u0026#34;Generating wave.vcd...\u0026#34;); #10; $finish; end endmodule Compiling and Simulating Compile and simulate the modified Verilog file using the following commands:\n!iverilog -o wave.out wave.v !vvp wave.out You should see a message confirming that wave.vcd has been generated.\nDownloading and Viewing the Waveform File Once the wave.vcd file is created, you can download it to your local machine for analysis using GTKWave, a popular waveform viewer. Use the following command to download the file:\nfrom google.colab import files files.download(\u0026#34;wave.vcd\u0026#34;) After downloading, open the file in GTKWave and inspect the signal transitions.\nDownload the Notebook You can download the Jupyter Notebook containing all the above steps from the following link:\nDownload the Jupyter Notebook\nConclusion Using Google Colab for Icarus Verilog simulations provides a simple and convenient way to write, compile, and debug Verilog code without requiring any local installations. Whether you\u0026rsquo;re a beginner learning Verilog or an experienced engineer testing small designs, this setup allows you to quickly prototype and verify your digital circuits.\nBy following the steps outlined in this guide, you can:\nInstall Icarus Verilog in Google Colab. Write and execute Verilog programs. Generate and analyze waveform files. What\u0026rsquo;s more—Colab isn\u0026rsquo;t just limited to Verilog. With a bit of setup, you can manage a range of open-source tools like Yosys, Graywolf, and many components from the OpenROAD flow, with GUIs disabled. This opens the door to enabling a full end-to-end digital design workflow—all within the cloud.\nThis setup is especially useful for students, researchers, and hobbyists looking for a hassle-free environment to explore digital design, without worrying about system configurations or heavy installations.\nHappy coding!\n","date":"February 19, 2025","image":"/images/post/iver/image_hu6369762148287695236.png","permalink":"/blog/setting-up-icarus-verilog-on-google-colab/","title":"Setting Up Icarus Verilog on Google Colab"},{"blog":["PID","PID1","RubiksCubeSolver","all","s"],"contents":"\rSolving a Rubik’s Cube isn’t just about memorizing algorithms — it’s about understanding how moves affect the pieces. There are several solving methods, each with its own approach. Some prioritize speed, some focus on efficiency or fewer rotations.\nHere\u0026rsquo;s a breakdown of the main ones:\nCFOP: The Classic Speedcubing Approach CFOP (Cross – First Two Layers – Orientation of Last Layer – Permutation of Last Layer), also known as the Fridrich Method, is the most widely used method among speedcubers. It breaks the solve into four logical steps, enabling high efficiency and minimal pause between sequences.\nThe Cross The first step is solving a cross on one face of the cube, typically white. The aim is to position the four edge pieces correctly while minimizing moves. Advanced solvers focus on efficiency, ensuring that each edge is inserted optimally without unnecessary cube rotations. Many speedcubers practice solving the cross in eight moves or fewer to optimize their solving time.\nFirst Two Layers (F2L) Rather than solving corners and edges separately, F2L pairs them up before inserting them into their respective slots. This is a crucial speed improvement over beginner methods, reducing move count significantly. F2L can be learned intuitively, but advanced cubers memorize key cases and algorithms for increased efficiency.\nOrientation of the Last Layer (OLL) Once the first two layers are completed, the next step is orienting all pieces on the top layer so that the face becomes a uniform color. There are 57 possible cases, but beginners can use a two-step OLL approach with just ten algorithms.\nPermutation of the Last Layer (PLL) The final step is to permute the last layer pieces into their correct positions. This step requires knowledge of 21 algorithms in the full PLL method or just six in the two-look PLL approach. Mastering PLL allows for faster transition times and optimized finger tricks to reduce execution delays.\nCFOP is the go-to method for many speedcubers because of its structured approach and ability to handle high turn-per-second (TPS) solves with ease. Even I use CFOP for solving the cube now—I do an intuitive cross and F2L, then use 2-look OLL and 1-look PLL.\nFor 2x2, I use the CLL (Corners Last Layer) method. You can check out my times here: Cubelelo Profile (it\u0026rsquo;s unofficial, but yeah!).\nRoux: The Efficient Block-Building Method Roux, developed by Gilles Roux in 2003, takes a vastly different approach from CFOP. It focuses on reducing move count and minimizing cube rotations, making it ideal for one-handed solving. Unlike CFOP, Roux relies heavily on intuitive solving techniques and block-building rather than strict algorithm memorization.\nFirst Block The solve begins by constructing a 1x2x3 block on one side of the cube. This is done by strategically placing the edge and corner pieces in the correct positions without disrupting already solved parts.\nSecond Block A second 1x2x3 block is then built on the opposite side of the cube. At this point, the left and right blocks are complete, leaving only the middle slice and top layer unsolved.\nCMLL (Corner Orientation \u0026amp; Permutation) Instead of solving the last layer in steps like OLL and PLL, Roux addresses all four corners at once using CMLL (Corners of the Last Layer). This step requires only 42 algorithms but can be broken into smaller subsets for easier learning.\nLSE (Last Six Edges) The final stage focuses on solving the remaining six edges using M and U moves exclusively. This step is what makes Roux unique, as it avoids rotations and allows for smooth, fast execution.\nThe strength of Roux lies in its efficiency—solves often require fewer moves than CFOP, and its reliance on intuitive solving makes it an excellent alternative for those who prefer a different approach.\nZZ: The Method Designed for Ergonomics The ZZ method, named after its creator Zbigniew Zborowski, aims to balance efficiency and turning ergonomics. It pre-orients edges early in the solve, allowing the rest of the cube to be solved with only R, U, and L moves, eliminating cube rotations and awkward finger placements.\nEOLine (Edge Orientation \u0026amp; Line) The solve begins by orienting all edges while placing two key edges along the bottom. This setup ensures that later steps can be executed smoothly without disrupting edge orientation.\nFirst Two Layers (F2L) Unlike CFOP, which requires cube rotations for F2L, the ZZ method allows for rotationless F2L execution. Because edge orientation was handled in EOLine, all remaining F2L pairs can be inserted using only R, U, and L moves.\nLast Layer Since all edges are already oriented, solving the last layer can be approached using CFOP-style algorithms or ZZ-specific techniques. The most advanced ZZ solvers use ZBLL (Zborowski-Bruchem Last Layer), which solves the entire last layer in one step, requiring knowledge of over 400 algorithms.\nZZ is an excellent choice for solvers who want to improve ergonomics while maintaining low move counts. However, EOLine can be challenging to master, making it slightly more difficult for beginners compared to CFOP.\nLayer-by-Layer (LBL): The Beginner-Friendly Method The Layer-by-Layer (LBL) method is the most common beginner method. It involves solving the cube in three distinct layers:\nSolve the first layer by completing a cross and inserting corners. Solve the second layer by inserting edge pieces into their correct slots. Solve the last layer using algorithms to orient and permute the pieces. This method is easy to learn and provides a solid foundation for more advanced techniques like CFOP.\nPetrus Method: The Block-Building Alternative The Petrus Method, developed by Lars Petrus, is a block-building approach that reduces move count and improves efficiency:\nSolve a 2x2x2 block anywhere on the cube. Expand it to a 2x2x3 block. Orient the edges early to make solving easier. Solve the remaining pieces with minimal moves. This method is useful for those who want an alternative to CFOP and prefer a more flexible solving approach.\nChoosing the Right Method Each method has its strengths, and the best one depends on your goals:\nCFOP is the best choice for speedcubers aiming for high TPS and efficiency. Roux is ideal for solvers who prefer intuitive solving and minimal rotations. ZZ is suited for those who want ergonomic solves with fewer rotations. LBL is great for beginners starting with the cube. Petrus is perfect for those who enjoy a block-building approach. No matter which method you choose, improving your lookahead, finger tricks, and efficiency will always be key to becoming a faster solver. Try out different approaches and see what works best for you!\nHappy cubing!\n","date":"February 12, 2025","image":"/images/post/solverubcub/poster_hu9180065793020124339.png","permalink":"/blog/solving-the-rubiks-cube/","title":"Solving The Rubiks Cube #PID1.2"},{"blog":["AIML","foundation-models","all","s"],"contents":"Over the last few weeks, one name has kept popping up across AI forums, Twitter feeds, and GitHub discussions: DeepSeek. Whether it\u0026rsquo;s about their models, benchmarks, or training philosophy, there\u0026rsquo;s been a noticeable shift in attention toward this new but fast-rising player in the open-source LLM space. But beyond the hype and headlines, what actually makes DeepSeek stand out in an increasingly crowded field?\nLet’s break it down — technically.\nWhat is DeepSeek? At a glance, DeepSeek is a research group developing high-performance large language models with a strong emphasis on transparency and openness. The team, based in China, has already released a suite of general-purpose and domain-specific models, most notably under the names DeepSeek LLM and DeepSeek Coder.\nThese models aren’t just experimental—they’re capable enough to compete with some of the most well-known names in the LLM space. What\u0026rsquo;s drawn even more interest is that DeepSeek builds everything from scratch, doesn’t rely on LLaMA-style foundations, and releases its work with open weights and documentation.\nGeneral-purpose and Code-focused Models The DeepSeek family is essentially split into two branches:\nDeepSeek LLM, designed for general language tasks like reasoning, summarization, translation, and dialogue. DeepSeek Coder, tailored specifically for code generation and software reasoning, capable of writing, completing, and debugging code across various programming languages. The Coder line in particular has turned heads because of its competitive performance in real-world coding benchmarks. For tasks like function completion, problem solving, and even more subtle software engineering prompts, DeepSeek Coder has shown results on par with, or better than, popular models like StarCoder2, CodeLLaMA, and GPT-4-turbo in some scenarios.\nWhy People Are Paying Attention A few technical choices and philosophical directions make DeepSeek especially interesting to both researchers and developers.\nTraining From Scratch One of the most significant decisions DeepSeek made is to train their models from scratch. They don’t use Meta’s LLaMA weights, nor do they fine-tune on top of existing models. This means they have more control over the architecture, training data, tokenizer, and ultimately, the behavior of the model.\nThis also makes their models free from some of the licensing limitations that surround other open-weight models that build on proprietary foundations.\nMassive Pretraining Dataset DeepSeek’s models are trained on an impressively large and diverse dataset — reportedly around 2 trillion tokens. That places them among the larger pretraining efforts in the open-source LLM space.\nThe dataset includes a healthy mix of languages, code, web data, academic content, and mathematics. This broad exposure allows the models to generalize well across tasks and domains. Importantly, a significant portion of the data is in both English and Chinese, giving their models strong bilingual capabilities right out of the box.\nMixture of Experts (MoE): Smarter, Not Heavier One of the more novel architectural choices in DeepSeek’s larger models is their use of Mixture of Experts (MoE). This is a technique where, instead of activating every part of the model for every input, only a selected group of “experts” (sub-models) are used depending on the input token.\nFor example, their 236-billion parameter model doesn’t use all 236B weights at once. Instead, only about 64 billion parameters are involved in any single forward pass. This keeps the model’s memory footprint and computational load lower than a dense model of the same size.\nThe benefits are pretty substantial — you can scale up the number of parameters (which helps capture a wider range of concepts and tasks), without paying the full cost of computing with all of them every time. It’s a step toward more efficient LLMs that don’t compromise too much on performance.\nStrong Performance in English and Chinese Another area where DeepSeek quietly but consistently stands out is its handling of both English and Chinese language tasks. Many LLMs today are English-centric, with support for other languages tacked on or unevenly represented in training data. DeepSeek took a different route, treating Chinese and English as equally important, and this balance shows in benchmarks.\nWhether it’s reading comprehension, reasoning tasks, or translation, the models handle both languages with surprising fluency — making them especially relevant for bilingual or cross-lingual use cases.\nEngineering and Infrastructure While DeepSeek hasn’t released full technical papers detailing every training step, we do know quite a bit about their engineering stack. Their training infrastructure seems to combine many of the current best practices in large-scale model training:\nFlashAttention-2, which speeds up attention computation while saving memory FP16 and bfloat16 mixed precision, to balance precision and performance Distributed training techniques like ZeRO and expert parallelism, for efficient use of GPUs It’s clear the team has optimized for both speed and stability at scale, which explains how they’ve managed to roll out models that perform well across a wide range of tasks without major quality issues.\nOpen Source with Real Substance One of the biggest reasons DeepSeek is getting the attention it is today: they’re fully committed to open source. The models come with publicly available weights, usable licenses, tokenizer files, and in many cases, full training logs.\nThis is particularly important right now, as more major labs are shifting toward closed APIs and proprietary models. DeepSeek has positioned itself as one of the few teams at this scale that are still willing to share their work openly, enabling downstream research, fine-tuning, and deployment by the community.\nThis openness has already led to growing interest from academic labs, hobbyists, and even startups looking for strong base models they can build on.\nBenchmarks and Real-World Performance In testing, DeepSeek’s models have delivered strong results on both standard and specialized benchmarks:\nOn HumanEval+, MBPP, and APPS, their Coder models show excellent Python and multi-language programming performance. On general reasoning and academic tasks like GSM8K, MMLU, and ARC, they’re often competitive with, or close to, the best models in their weight class. Their models are also being used more and more in community evaluation tasks like Arena, Chatbot Arena, and other crowd-ranking systems. And it’s not just the top-line accuracy that impresses — many users have commented on how stable and predictable the models are across diverse input types, which is a strong signal of a solid training pipeline.\nWhat’s Next for DeepSeek? While we don’t have a full roadmap, the team has hinted at a few areas they’re exploring next:\nExpanding into multimodal models that can process images and text together Integrating tools and APIs to create agentic systems (LLMs that can use a calculator, browser, or code interpreter) Possibly scaling up even further, or making more advanced MoE variants public There are also signs that DeepSeek is starting to build a stronger community presence, with model cards in English, discussion threads on HuggingFace, and public checkpoints optimized for inference and fine-tuning.\nIn Summary DeepSeek has earned its recent buzz for good reason. It’s not just another LLM release — it’s a thoughtfully engineered, from-scratch family of models that manage to be both capable and open.\nBy combining large-scale bilingual training, efficient architectures like MoE, and a strong open-source stance, DeepSeek is carving out a unique space in the rapidly evolving LLM ecosystem.\nIt’s still early days, but if they continue in this direction, DeepSeek could play a major role in shaping the next generation of open AI tools — not just in China or in research circles, but globally.\n","date":"February 6, 2025","image":"/images/post/ml/deepseek_hu15517040979365898706.png","permalink":"/blog/why-everyone-is-talking-about-deepseek/","title":"Why Everyone’s Talking About DeepSeek?"},{"blog":["PID","PID1","RubiksCubeSolver","all","s"],"contents":"The Rubik’s Cube is a 3D combination puzzle that has fascinated minds for decades. Invented in 1974 by Ernő Rubik, a Hungarian architect and professor, it was originally called the \u0026ldquo;Magic Cube.\u0026rdquo; Designed as a teaching tool to explain 3D movement, it quickly became a global sensation. The challenge? Scramble it, then restore each face to a single color—sounds simple, but millions have struggled (and succeeded) at it since!\nIf you are curious, you\u0026rsquo;ll find the puzzles around you. If you are determined, you will solve them -Erno Rubik\nHow a Rubik’s Cube is Structured At first glance, the Rubik’s Cube appears to be just 27 smaller cubes arranged in a 3x3 grid, but its internal mechanics are far more sophisticated. The core mechanism allows for smooth rotations, holding everything together while letting the outer pieces move freely.\nThe cube consists of three main types of pieces:\nCore: The core holds the entire structure intact. It consists of six fixed center pieces that never move relative to each other. Edges: The cube has 12 edge pieces, each with two colors, positioned between the centers. Corners: There are 8 corner pieces, each with three colors, which determine the cube’s orientation. Each of these pieces interlocks in a way that allows rotation without disassembling the puzzle. When twisted, the cube rearranges itself by moving these smaller components around the core, yet the entire structure remains stable.\nHow It Rotates and Functions Despite its scrambled look, a Rubik’s Cube follows a well-structured mechanical system:\nEach face rotates independently, thanks to the internal core mechanism. The center pieces remain static, acting as reference points for solving. Edge and corner pieces move around freely, rearranging their positions to create new patterns with every turn. Each move you make affects multiple pieces at once, creating complex shifts that can be solved using known algorithms. The key is to understand how these pieces interact with each turn to work towards a solution.\nRubik’s Cube Notation (3x3 Standard Moves) To communicate Rubik’s Cube solutions, we use standard notation:\nFace Turns:\nR (Right) – Rotate the right face 90° clockwise R\u0026rsquo; (Right Prime) – Rotate the right face 90° counterclockwise L (Left) – Rotate the left face 90° clockwise L\u0026rsquo; (Left Prime) – Rotate the left face 90° counterclockwise U (Up) – Rotate the top face 90° clockwise U\u0026rsquo; (Up Prime) – Rotate the top face 90° counterclockwise D (Down) – Rotate the bottom face 90° clockwise D\u0026rsquo; (Down Prime) – Rotate the bottom face 90° counterclockwise F (Front) – Rotate the front face 90° clockwise F\u0026rsquo; (Front Prime) – Rotate the front face 90° counterclockwise B (Back) – Rotate the back face 90° clockwise B\u0026rsquo; (Back Prime) – Rotate the back face 90° counterclockwise Double Turns:\nMoves followed by 2 (e.g., R2, U2) indicate a 180° turn in either direction. This notation is used universally among cubers, making it easier to follow solving guides and algorithms.\nColor Schemes and Face Orientation Most official Rubik’s Cubes follow a standardized color arrangement, which helps speedcubers recognize patterns quickly. The opposite face pairs are usually:\nWhite ↔ Yellow Blue ↔ Green Red ↔ Orange This scheme remains consistent across most cubes, ensuring uniformity in solving strategies. Recognizing the relationship between opposite faces is crucial when learning solving techniques, as many algorithms rely on the cube’s color orientation.\nDifferent Types of Rubik’s Cubes and Modifications The classic 3x3 is just the beginning! There are countless variations, each adding its own twist to the challenge:\nNxN Cubes (Larger and Smaller Variants) 2x2 (Mini Cube) – A simpler version, great for beginners. 3x3 (Standard Cube) – The original and most popular. 4x4 (Rubik’s Revenge) – Extra layers mean extra complexity. 5x5 (Professor’s Cube) – More layers, more challenge. Even larger cubes like 6x6, 7x7, and beyond exist for advanced solvers. Shape Modifications Fisher Cube – A 3x3 shape mod that appears to shift its center axis, making it visually deceptive. Windmill Cube – Features diagonal cuts, making rotations feel unpredictable. Axis Cube – Turns into a bizarre, asymmetric mess when scrambled. Mirror Cube – Solved by shape instead of color, adding an extra challenge. Other Unique Cubes Pyraminx – A triangular-based twisty puzzle with simpler movement mechanics. Megaminx – A 12-sided dodecahedron cube with an extra layer of complexity. Ghost Cube – A shape-shifting cube that must be solved by aligning irregularly shaped pieces rather than colors. Each of these variations presents a unique solving challenge, keeping cubers engaged for years!\nFinal Thoughts The Rubik’s Cube is more than just a toy—it’s a mechanical marvel, a brain workout, and an endless source of fun. Whether you’re solving a classic 3x3 or diving into the wild world of modded cubes, the principles remain the same: patterns, patience, and persistence. Solving a Rubik’s Cube can enhance cognitive skills like problem-solving, pattern recognition, and spatial awareness, making it a fantastic hobby for all ages.\nSo, grab a cube, start twisting, and embrace the puzzle madness! 🔄✨\n","date":"January 31, 2025","image":"/images/post/rubcubemech/poster_hu18086298204613167302.png","permalink":"/blog/mechanics-of-rubiks-cube/","title":"Mechanics of Rubiks Cube #PID1.1"},{"blog":["PID","PID1","RubiksCubeSolver","all","s"],"contents":"\rWhether it\u0026rsquo;s a crossword, Sudoku, or a complex jigsaw, puzzles have a unique way of capturing our attention. But what makes solving them feel so rewarding? Beyond the entertainment, puzzles play a significant role in enhancing cognitive abilities, improving problem-solving skills, and offering a tangible sense of accomplishment.\nAt their essence, puzzles present structured challenges. They prompt us to ask, “How do I figure this out?”—a question that resonates well beyond recreational activities. In many ways, life itself can be viewed as a series of puzzles, each requiring thoughtful decisions, strategic approaches, and adaptability. Developing puzzle-solving skills can equip us to handle larger, more complex challenges with confidence.\nWhy Solve Puzzles? Engaging with puzzles is comparable to exercising the brain. They support memory retention, concentration, and cognitive flexibility. Additionally, puzzles foster patience, persistence, and creative thinking—skills applicable in fields ranging from software development to everyday problem-solving.\nCompleting a puzzle also stimulates a dopamine release in the brain, providing a sense of reward. This combination of mental exercise and positive reinforcement contributes to their enduring appeal.\nCoding: A Digital Puzzle Programming is often described as one of the most intellectually satisfying forms of problem-solving. Every feature implemented or bug resolved represents a mini-puzzle—requiring logic, creativity, and precision.\nThose familiar with debugging or optimizing code know that programming is both an evolving challenge and a continual learning process. As tools and requirements shift, so do the problems—and the satisfaction of building functional, elegant solutions remains consistent.\nPhysical Puzzles: Engaging Both Mind and Body Physical puzzles, such as the Rubik’s Cube, engage both mental and physical faculties. These types of challenges activate logical reasoning while also enhancing spatial awareness and fine motor skills. The process of manipulating a physical object while working through a solution engages multiple regions of the brain, offering a holistic cognitive workout.\nSolving physical puzzles often requires a combination of strategy, precision, and patience—attributes that extend to a wide range of disciplines and tasks.\nThe Rubik’s Cube: A Learning Experience The Rubik’s Cube is a timeless example of a puzzle that combines logic, memory, and pattern recognition. Like many, my initial attempts were far from successful. But through practice—and a few tutorials—I was able to understand the underlying logic and eventually solve it.\nSolving the Rubik’s Cube helps develop a structured approach to problem-solving. It encourages thinking several steps ahead, recognizing patterns, and maintaining focus under pressure.\nRubik’s Cube Solver: A Project Overview The Rubik’s Cube Solver series merges physical problem-solving with technical implementation. The goal is to design a system that can autonomously solve a Rubik’s Cube—integrating mechanical understanding with algorithmic efficiency.\nThe project involves simulating the cube’s mechanics, writing solution algorithms, and eventually building a hardware prototype. While the implementation is still in progress, the process offers opportunities for exploration, optimization, and iterative learning.\nNote: The project is currently in its simulation phase. Hardware integration and performance improvements are ongoing. The series documents this development journey step by step.\nIntroducing the PID Series The Rubik’s Cube Solver is the first in the PID: Project IN Detail series—a collection that provides insight into technical projects, including design processes, problem-solving methods, and key takeaways.\nUpcoming posts will cover foundational Rubik’s Cube solving techniques before transitioning into how algorithms and computer vision can automate and optimize the process.\nGetting Started Puzzles offer a valuable framework for developing critical thinking and adaptive problem-solving. Whether you’re a programmer, a puzzle enthusiast, or simply curious, this series is designed to offer something for everyone.\nWith that, let’s begin the journey into the world of puzzles—step by step, and twist by twist.\n","date":"January 24, 2025","image":"/images/post/puzzles/puzzles_hu16412763637766407020.jpg","permalink":"/blog/why-should-you-start-solving-puzzles/","title":"Why Should You Start Solving Puzzles? #PID1.0"},{"blog":["robotics","events","personal","all","s"],"contents":"Last year, I missed ROSCon India due to exams and, honestly, had no idea what I was missing out on. This year, though, I made it, and it turned out to be more than I ever imagined. The two days I spent at ROSConIN'24 were nothing short of transformative, and this blog itself is a result of the inspiration I drew from the event.\nSetting the Stage The second edition of ROSCon India, following the overwhelming success of last year’s event with 750+ attendees, was hosted by ARTPARK at the Indian Institute of Science (IISc), Bangalore. This event brought together developers, hobbyists, researchers, and industry professionals to share, learn, and network over all things ROS (Robot Operating System).\nWhat makes ROSCon India special is its resemblance to the international ROSCon, with a local flavor that emphasizes India’s growing influence in robotics and automation. It was heartwarming to see support from Open Robotics and the enthusiastic efforts of Acceleration Robotics, RigBetel Labs, and ARTPARK in organizing such an impactful gathering.\nDay 0: Workshops Galore The event kicked off with Workshop Day on December 4th. We could choose one out of three workshops, and I opted for the third one:\nWorkshop 1: Next-Gen Robotics Development with NVIDIA ISAAC, GenAI, and ROS – Organized by NVIDIA, this workshop offered a comprehensive dive into cutting-edge robotics development. Though I couldn’t attend it, the buzz from the attendees made it clear how enriching it was.\nWorkshop 2: Leveraging Zenoh as a ROS 2 Middleware Layer – Conducted by Zettascale Technology, this session explored Zenoh as an innovative middleware layer for ROS 2. It intrigued many participants and opened up discussions on its potential.\nWorkshop 3: ROS 2 Controls, Navigation, and Advanced Communication Study – Organized by ARTPARK, this was my pick! A deep dive into ROS 2 controls, navigation, and communication felt perfectly aligned with my interests. The hands-on experience and insights I gained were invaluable.\nDay 1: A Conference to Remember December 5th began with registrations and a warm welcome address by the organizers, setting an enthusiastic tone. Some highlights from the day included:\nKeynotes: Geoffrey Biggs (CTO, Open Robotics) and Yadunund Vijay (Intrinsic) discussed the state of Open Robotics in 2024, followed by Jigar Halani from NVIDIA sharing insights on robotics development. Inspiring Talks: From Yuvraj Mehta’s session on RoboGPT to Sarvesh Kumar Malladi’s insights on Universal Robots’ ROS2 features, each talk added depth to the learning experience. Industry Focus: Anish Dalvi from TCS delved into automotive protocols, while Somdeb Saha highlighted retail automation with ROS. Both sessions demonstrated the vast industrial applications of ROS. Panel Discussion: The day ended with an engaging panel on cracking the product-market fit in robotics, featuring founders and investors sharing valuable insights. Day 2: Deep Dives and Future Directions December 6th brought more enlightening sessions, including:\nKeynote by Angelo Corsaro (ZettaScale Technology): An in-depth look at Zenoh as an alternative middleware layer for ROS 2. Technical Sessions: From Ajay Sethi and Prateek Nagras introducing the Robotics Application Stack to Nidhi Choudhary’s integration of physics-based neural networks with Gazebo, the variety of topics covered was astounding. Fireside Chat: The event concluded with a thought-provoking discussion on the future of ROS, featuring Geoffrey Biggs, Yadunund Vijay, and Angelo Corsaro. Networking and Personal Highlights One of the most fulfilling aspects of ROSConIN’24 was the chance to meet incredible people. I connected with alumni from my college, including Aryan Jaguste and Jatin Vera, whose experience in robotics left me inspired. I also met many other professionals who have been in this field for years, generously sharing their knowledge and encouraging me to keep learning and experimenting.\nIt was this vibrant exchange of ideas and stories that inspired me to start this blog. ROSConIN’24 wasn’t just a conference; it was a catalyst for my growth in the robotics domain.\nA Big Shoutout A massive thanks to all the companies and individuals who made this event possible, including Acceleration Robotics, RigBetel Labs, ARTPARK, NVIDIA, Zettascale Technology, Tata Consultancy Services, Universal Robots, and many others like Autodiscovery, Botsoverkill, Bullwork, Golain, I-Hub Jodhpur, IISc, Kikobot, Nawe, Neuralzome, Thundroids, Vicharak, Virya, and xTerra.\nI recently shared a post on LinkedIn about a bunch of interesting startups I came across — not a promotion, just something I felt like doing for the community. These were all booths or projects I ran into during the event, and I wanted to highlight them for the sheer variety and effort behind them.\nAmong all, Vicharak stood out the most to me.\nThey’re building:\nVaaman – an FPGA-based board that’s capable of running AI/ML workloads. That’s not something you see every day in the Indian embedded space.\nAxon – a single-board edge computer, built for compact high-performance tasks at the edge.\nTheir work hits that sweet spot between hardware and intelligence — definitely something to keep an eye on.\nIf you\u0026rsquo;re into edge compute, FPGAs, or just curious about what’s brewing in the local hardware scene, check them out: vicharak.in\nClosing Thoughts Attending ROSConIN’24 was a valuable experience that offered more than just technical insights. It provided a sense of community and highlighted the shared interest in robotics and automation among participants. For anyone interested in these fields, the event presents a great opportunity to learn, connect, and engage with others who share similar interests.\nThe following day, I attended the GNOME Asia Summit in Bangalore. This event offered another perspective—bringing together open-source contributors and Linux enthusiasts. It was a great chance to learn from the broader open-source ecosystem and understand its ongoing contributions and challenges.\nNavigating Bangalore came with its own set of challenges, especially when it came to traffic. However, the energy of the city and its role as a major tech hub made the overall experience worthwhile. Attending two major conferences back-to-back proved to be both intensive and rewarding, offering a condensed but enriching exposure to various communities.\nInteractions at both events further encouraged me to explore robotics and open source more deeply. It was during ROSConIN’24 that the idea of starting this blog took shape—as a way to document and share ongoing learning and projects in a more structured format.\nInterestingly, I was one of the few people there who received an ID with a handwritten name instead of a printed one. A few people were even unsure if it was genuine—which honestly just added to the uniqueness of the experience.\n","date":"January 17, 2025","image":"/images/post/roscon/roscon_hu3821629981689278023.jpg","permalink":"/blog/my-rosconin24-experience/","title":"My RosConIN'24 (+GNOME Asia Summit) Experience"},{"blog":["blogging","hugo","all","s"],"contents":"\rIn 2025, with the internet brimming with TikToks, reels, and AI-generated articles, you might wonder—is blogging still worth it?\nThe answer is a resounding YES, and here’s why.\nWhy Blog in 2025? Share Your Unique Perspective In a world of AI-generated content, your personal voice matters more than ever. AI might generate the basics, but stories, experiences, and personal insights are uniquely human. Whether you’re building your first robot, sharing parenting tips, or learning a new language, your journey can inspire others.\nThink about it: How many times have you Googled a problem, stumbled upon a blog, and found exactly what you needed? That could be you helping someone else.\nBuild Your Digital Legacy Your blog is your corner of the internet—a space to leave your mark. Unlike fleeting social media posts, blogs are evergreen, searchable, and build a record of your growth. For developers, it can be a portfolio of your work; for creatives, it’s a gallery of your creations.\nI started my blog to document my tech projects, but I realized it’s also helping me keep track of my ideas, progress, and experiments. Plus, I’ve already met people who share the same passions—thanks to this little space!\nLearn as You Share Writing is an incredible teacher. To explain something clearly, you need to truly understand it yourself.\nDevelopers often blog about solutions to bugs or coding techniques, which not only helps others but reinforces their own knowledge. For non-tech folks, writing about personal projects—whether it’s DIY, cooking, or fitness—gives clarity and keeps you motivated. Pro Tip: Blogging can make you a better problem-solver. Breaking down problems into digestible steps is the essence of both writing and coding.\nBuild Connections and Opportunities Blogging isn’t just about putting your thoughts into words; it’s about starting conversations. Your blog can:\nAttract collaborators who resonate with your ideas. Impress potential employers or clients by showcasing your expertise. Connect you with a like-minded community. Think of it as networking without the awkward handshakes.\nStay Relevant in the AI Era AI is great for automating tasks, but creativity, originality, and storytelling? That’s all you. A blog lets you flex those creative muscles and prove you’re not just keeping up with the times—you’re shaping them.\nWhy Students Should Start Blogging 🎓 As a student, blogging can be a game-changer for your personal and professional growth. Here’s why:\nShowcase Your Skills:\nYour blog can act as a dynamic portfolio. Whether it’s coding projects, research papers, or even creative writing, it’s a platform to demonstrate your expertise and passion. Employers and professors love seeing initiative.\nDocument Your Learning:\nWriting about what you’re learning—whether it’s a tough algorithm, a robotics project, or study hacks—helps reinforce your understanding and creates a resource for others.\nStand Out:\nIn a competitive world, a well-maintained blog sets you apart. It shows that you’re not just a passive learner but someone who actively contributes to the community.\nBuild Connections:\nBlogging opens doors to collaborations, internships, and mentorships. Sharing your work publicly can attract like-minded peers, professors, or even recruiters.\nPro Tip for Students: Start small. Write about a project or concept you recently worked on in class—it’s a great way to begin!\nHow to Start Blogging in 2025 If all this has convinced you, let’s talk about how to get started! Whether you’re a dev documenting code or someone sharing life hacks, blogging has never been easier.\nChoose Your Purpose Ask yourself: Why do I want to blog?\nIs it to document your journey (like me)? Share your expertise? Build a personal brand? Just for fun? Defining your purpose will help you stay motivated and give your blog a clear focus.\nPick the Right Platform Here are a few options to suit different needs:\nTechies: Use GitHub Pages for free hosting or platforms like Jekyll/Hugo for custom setups. Beginners: Try WordPress or Ghost—they’re user-friendly and have tons of templates. Minimalists: Substack or Medium are great for simple, distraction-free writing. What I Use: I opted for GitHub Pages because I love having full control over my blog’s look and feel.\nWrite What You Know (And Love) Find your niche. You don’t need to be an expert—just share your journey as you learn.\nDevs: Write about side projects, tutorials, or debugging solutions. Non-devs: Document hobbies, productivity hacks, or personal experiences. Remember: What’s obvious to you might be groundbreaking to someone else.\nKeep It Simple (At First) Don’t overcomplicate it. Your first post can be:\nAn introduction to who you are. A story about a project you worked on. A simple “lesson learned” post. It’s okay if your first post isn’t perfect—it’s better to start and improve as you go.\nLeverage AI to Help You In 2025, AI tools can make blogging easier:\nUse ChatGPT for brainstorming post ideas. Grammarly can polish your grammar. Tools like Jasper AI can even generate draft content. But remember: your voice is the star. AI can assist, but authenticity is irreplaceable.\nPromote Your Blog Once your blog is live, share it!\nPost about it on LinkedIn, Instagram, or Twitter. Join communities (Reddit, Discord, forums) related to your niche. Collaborate with others by guest-posting or linking to their work. If you’re consistent, people will notice.\nEmbrace the Process Blogging is a journey. Don’t stress about being perfect—just keep writing, experimenting, and learning. Tools like Google Analytics can show you what’s working and help you refine your style.\nFinal Thoughts Blogging in 2025 is about more than just writing—it’s about sharing your voice, building connections, and leaving a legacy. Whether you’re a coder, a hobbyist, or someone with a passion to share, there’s never been a better time to start.\nYour Blog, Your Rules: It doesn’t have to be fancy. It just has to be you.\nWhat’s Next? If you’re thinking of starting a blog, go for it! Your ideas are worth sharing. Feel free to reach out if you need help setting things up or brainstorming ideas—I’d love to hear from you.\nReady to take the plunge? Hit that \u0026ldquo;New Blog\u0026rdquo; button and let the world hear your voice!\n","date":"January 10, 2025","image":"/images/post/blog/why-blog_hu16664361137983714441.jpg","permalink":"/blog/why-blog-in-2025/","title":"Why Blog in 2025? (And How to Get Started)"},{"blog":["blogging","personal","all","s"],"contents":"\rHello, and thank you for visiting. My name is Jagadeesh, and this blog is a personal and professional record of my journey through college, projects, and the experiences that continue to shape my interests and aspirations.\nCurrently, I am pursuing my undergraduate studies at NIT Calicut, where I am learning, building, and continuously exploring new ideas across engineering, technology, and beyond.\nThis space reflects not only my technical pursuits but also my broader experiences—from milestones and challenges to reflections on the people and places that have influenced my path.\nI know no one would care to read this, even I sound like someone who thinks they do\u0026hellip; :( but anyways.\nThrough this blog, I aim to document my work thoughtfully, share insights, and connect with others who are equally passionate about learning and creating.\nPurpose of This Blog The main purpose of this blog is to organize and document my projects, ideas, and experiences in a structured way.\nOver time, I realized that many of my projects ended up scattered across private repositories or forgotten folders. This space is simply an effort to gather them in one place, with a bit more context and thoughtfulness. While most of the projects here are small or experimental, documenting them helps me reflect and learn. If anyone else happens to find something useful along the way, that would just be a nice bonus.\nThe content here includes technical notes, project write-ups, tutorials, and personal reflections from different experiences.\nAbout Me I am Jagadeesh Mummana, a sophomore at NIT Calicut, majoring in Electronics and Communication Engineering with a minor in Robotics and Automation.\nMy focus is on electronics and hardware design, particularly in VLSI systems, and how these can be used to optimize hardware for AI applications. I’m interested in improving hardware performance to support the intensive computational needs of AI models. I also enjoy the hands-on process of building real-world robots from scratch, where integrating hardware and software effectively is essential to their operation.\nI enjoy taking the time to understand systems; building them, exploring how they work, and trying to make small improvements through trial and error.\nOutside of academics, I spend time on personal projects, club activities, and technical experiments, as well as trying to slowly build both technical and creative skills.\nAbout This Site This site originally began as a portfolio project for my final submission in the CS50x 2024 Course.\nThe initial version remains accessible here or through the navigation bar.\nSince then, it has grown into a broader personal and technical blog.\nThe site is built with Hugo, using the Geeky-Hugo theme as a foundation, which I have extensively customized to better meet my evolving requirements. It is hosted via GitHub Pages.\nThe inspiration to start a formal blog came during my visit to ROSCon India 2024 (IISc Banglore).\nAt the event, I had the opportunity to interact with peers and seniors from several institutions, many of whom were doing exceptional work across different technical domains. I was struck by their clarity when explaining their projects, yet I noticed that structured documentation and public presentation of their work were often lacking.\nThrough conversations, it became apparent that this was not due to a lack of ability but rather due to documentation being a lower priority amidst academic and project commitments. This realization stayed with me and motivated me to establish a platform where I could not only share the technical details but also convey the motivations, processes, challenges, and insights behind each project.\nIt is my belief that even a basic project, when fully understood and properly documented, holds more educational and developmental value than a great one with only a surface-level understanding.\nTaking the time to work through each part of a project helps me learn more and feel more connected to the work.\nYou can find more about this inspiration and my reflections on the event in this post.\nWhy I Built This Blog This site serves as a central platform to organize, document, and share my technical work and experiments.\nSince many of my GitHub repositories are private, this blog acts as an open and structured window into the projects I am pursuing, providing detailed insights, learnings, and future directions.\nSome of the projects are actively maintained, others are works in progress, and a few have reached functional milestones but are continually refined as my understanding evolves.\nAll projects featured here are either independently developed or draw upon external inspirations, in which case appropriate credits are acknowledged.\nFor Industry Professionals/Recruiters If you are an industry professional or recruiter visiting this site, I encourage you to explore the \u0026ldquo;Projects\u0026rdquo; and \u0026ldquo;About\u0026rdquo; sections for a clearer perspective on my work, interests, and approach.\nI am currently seeking internship opportunities in areas such as digital and mixed-signal VLSI design, particularly those that intersect with robotics and AI-enhanced systems.\nI am always eager to learn and explore new challenges, and I would love the opportunity to contribute to innovative projects in these areas. If you\u0026rsquo;d like to discuss a project in more detail, I\u0026rsquo;d be happy to do so.\nThank you for considering my work.\nFor Peers, Friends, and Fellow Explorers If you are a peer, a friend, or simply an interested visitor, I warmly welcome discussions, collaborations, and exchange of ideas.\nI believe that collaboration enriches learning for all parties involved, providing diverse perspectives and new ways of thinking.\nIf you have an idea to discuss, a project to collaborate on, or simply wish to connect, please feel free to reach out through the Contact Page or via direct messages on the platforms listed below.\nWhile I am active across multiple channels, my activity on Instagram is currently limited.\nWhy I Focus on Projects For those wondering why there is an emphasis on building and documenting projects, the reason is simple: I believe true learning comes through hands-on experience.\nAcademic projects often serve their purpose within fixed timelines, but rarely do they encourage continuous exploration beyond submission. When I initiate and manage a project entirely—starting from its original motivation through to real-world deployment—I develop a deeper sense of ownership and a stronger commitment to refine and improve it over time.\nMany of my projects span across disciplines, and this intersectional nature drives me to think critically and holistically. I am always open to collaborations that allow for exploration across varied fields.\nIn my view, engaging actively with projects provides a form of learning that cannot be replaced by passive consumption, whether through videos or multiple online courses.\nWhile these resources are important, I believe that their true value is realized when they are used as stepping stones to build something original and substantial.\nConnect With Me 🌐 Portfolio: mummanajagadeesh.github.io 💼 LinkedIn: Jagadeesh Mummana 🔧 GitHub: Mummanajagadeesh 📸 Instagram: @jagadeesh__97__ For additional social links and informal contact options, please visit the Contact Page.\nThank you for your time and interest. I look forward to continuing this journey of learning, building, and sharing, and I hope you find something here that resonates with you.\nPrivacy Policy\nWhy This Page Exists?\nNo one usually checks what’s in here — but if you’re reading this, you’re clearly more thorough than most.\nThis page was supposed to be for a newsletter — the idea was to collect emails and send updates. But that never really happened. Nothing is being collected, stored, or sent. There’s no newsletter, and no personal data involved.\nNot that you’re missing out. I post pretty randomly — sometimes at weird hours, sometimes not at all. A newsletter wouldn’t help much anyway. If you find any of this worth reading, that’s already more than I expect.\nMost of the posts here are just straight content — no polished tutorials, no fancy demos. It’s mostly me trying to make sense of things as I go. I do have some private repos with better docs (mostly for myself). General stuff can be found under the projects/ and blog/ sections if you’re curious.\nThis page also exists to cover the privacy basics. This site doesn’t collect personal data, but standard anonymous analytics or server logs might be used to understand general traffic — nothing identifiable is stored, sold, or shared.\nIt’s a quiet page, but it’s here for the sake of transparency — and because it’s simply the right thing to do.\nOpen to feedback if you’ve got any. And if you’re still reading this — thanks. You’ve got patience I clearly don’t :(\n","date":"January 3, 2025","image":"/images/post/hw/image_hu9092715905389529254.png","permalink":"/blog/hello-world/","title":"Hello World!! (Why I Built This Blog)"}]